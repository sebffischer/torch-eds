---
title: "Practical Use Case"
---

```{r, include = FALSE}
set.seed(123)
torch::torch_manual_seed(123)
```

In this notebook, we will go through a full practical example of training a neural network on a tabular dataset.
First, we will implement the model using only `torch`, then we will see how `mlr3torch` can be used to make the process easier.


# Using only `torch`

## Defining the Dataset

For computational reasons, we will use a small, tabular dataset, i.e. the *german credit* dataset.
In order to access it, make sure that the `mlr3` package is installed.

```{r}
tbl = readRDS(system.file("extdata", "spam.rds", package = "mlr3"))
head(tbl)
```

The dataset contains 4601 rows and 58 columns, of which 57 are features and 1 is the target.
The goal is to predict whether an email is spam or not.
All features are numeric, only the target is a factor.

```{r}
head(tbl)
```

In order to train a neural network, we need to convert this data into a `dataset`.
For this tabular problem, we can simply convert the features and the target to `torch_tensor` objects and then use the `tensor_dataset()` function to create the dataset.

```{r}
features = torch_tensor(as.matrix(tbl[, -58]))
target = torch_tensor(as.integer(tbl$type) - 1)
ds = tensor_dataset(features, target)
```

## Defining the Model

We will use a simple feedforward neural network with 2 hidden layers.

```{r}
model = nn_sequential(
  nn_linear(57, 100),
  nn_relu(),
  nn_linear(100, 100),
  nn_relu(),
  nn_linear(100, 2)
)
```


## Defining the Optimizer

```{r}
optimizer = optim_adam(model$parameters, lr = 0.001)
```


## Training the Model

We want to do validation during training and therefore split the data into a training and a validation set.

```{r}
train_idx = sample(nrow(tbl), 0.8 * nrow(tbl))
```

We can then use the `dataset_subset` function to create the training and validation sets.

```{r}
train_ds = dataset_subset(ds, train_idx)
val_ds = dataset_subset(ds, -train_idx)
```

Then, we define the trainings loop.

```{r}
```


## Training the Model

```{r}
train = function(model, optimizer, train_ds, val_ds, epochs, batch_size) {
  train_dl = dataloader(train_ds, batch_size = batch_size, shuffle = TRUE)
  val_dl = dataloader(val_ds, batch_size = batch_size, shuffle = FALSE)

  history = list()

  for (epoch in seq_len(epochs)) {
    model$train()
    coro::loop({for (batch in train_dl) {
      optimizer$zero_grad()
      output = model(batch[[1]])
      loss = nll_loss(output, batch[[2]])
      loss$backward()
      optimizer$step()
    }})

    model$eval()
    coro::loop({for (batch in val_dl) {
      output = model(batch[[1]])
      loss = nll_loss(output, batch[[2]])
      history$append(list(loss = loss$item()))
    }})
  }
}
```

Finally, we can also plot the results using ggplot2.

```{r}
library(ggplot2)
ggplot(history, aes(x = epoch, y = loss)) +
  geom_line() +
  labs(title = "Training and Validation Loss", x = "Epoch", y = "Loss")
```


Writing these training loops is repetitive, error-prone and unnecessary.
For this reason one can often resort to higher level libraries that abstract away the details.
In `R`, one such library is `mlr3torch`, but there is also e.g. also `luz`.

# Using `mlr3torch`

Install it first:
```{r, eval = FALSE}
install.packages("mlr3torch")
```

# Defining the Task

In order to use `mlr3torch`, we first need to define a task.

```{r}
library(mlr3torch)
tsk_spam = as_task_classif(tbl, target = "type")
```

```{r}
```
