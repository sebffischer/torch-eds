<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.3.450">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Building Neural Networks with torch</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="module-data-optim_files/libs/clipboard/clipboard.min.js"></script>
<script src="module-data-optim_files/libs/quarto-html/quarto.js"></script>
<script src="module-data-optim_files/libs/quarto-html/popper.min.js"></script>
<script src="module-data-optim_files/libs/quarto-html/tippy.umd.min.js"></script>
<script src="module-data-optim_files/libs/quarto-html/anchor.min.js"></script>
<link href="module-data-optim_files/libs/quarto-html/tippy.css" rel="stylesheet">
<link href="module-data-optim_files/libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="module-data-optim_files/libs/bootstrap/bootstrap.min.js"></script>
<link href="module-data-optim_files/libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="module-data-optim_files/libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">

  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

</head>

<body class="fullcontent">

<div id="quarto-content" class="page-columns page-rows-contents page-layout-article">

<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">Building Neural Networks with torch</h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  

</header>

<section id="from-linear-models-to-neural-networks" class="level1">
<h1>From Linear Models to Neural Networks</h1>
<p>In the previous notebook, we learned how to use torch’s autograd system to fit a simple linear model of the form <span class="math inline">\(\hat{y} = ax + b\)</span> using gradient descent. We manually:</p>
<ol type="1">
<li>Created parameters <code>a</code> and <code>b</code> with <code>requires_grad = TRUE</code></li>
<li>Implemented forward pass using basic operations: <code>y = a * x + b</code></li>
<li>Computed gradients and updated parameters with a simple update rule: <code>a$sub_(lr * a$grad)</code></li>
</ol>
<p>For more complex models, this approach becomes unwieldy. <code>torch</code> provides several high-level abstractions that make it easier to build and train neural networks:</p>
<ul>
<li><code>nn_module</code>: A class to organize model parameters and define the forward pass.</li>
<li><code>optim</code>: Classes that implement various optimization algorithms (replaces our manual gradient updates)</li>
<li><code>dataset</code> and <code>dataloader</code>: Classes to handle data loading and batching (replaces our manual data handling)</li>
</ul>
<p>Let’s see how these components work together by building a neural network to classify spiral data.</p>
</section>
<section id="neural-network-architecture-with-nn_module" class="level1">
<h1>Neural Network Architecture with <code>nn_module</code></h1>
<p>The <code>nn_module</code> class serves several purposes:</p>
<ol type="1">
<li>Acts as a container for learnable parameters</li>
<li>Provides train/eval modes (important for layers like dropout and batch normalization)</li>
<li>Defines the forward pass of the model</li>
</ol>
<p>Torch provides many common neural network modules out of the box. For example, the simple linear model we created earlier (<span class="math inline">\(\hat{y} = ax + b\)</span>) could be implemented using the built-in <code>nn_linear</code> module:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(torch)</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a>linear_model <span class="ot">&lt;-</span> <span class="fu">nn_linear</span>(<span class="at">in_features =</span> <span class="dv">1</span>, <span class="at">out_features =</span> <span class="dv">1</span>, <span class="at">bias =</span> <span class="cn">TRUE</span>)</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="fu">print</span>(linear_model<span class="sc">$</span>parameters)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>$weight
torch_tensor
-0.6658
[ CPUFloatType{1,1} ][ requires_grad = TRUE ]

$bias
torch_tensor
-0.8826
[ CPUFloatType{1} ][ requires_grad = TRUE ]</code></pre>
</div>
</div>
<p>We can condcut a forward pass by simply calling the function on some inputs.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="fu">linear_model</span>(<span class="fu">torch_randn</span>(<span class="dv">1</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>torch_tensor
-0.7433
[ CPUFloatType{1} ][ grad_fn = &lt;AddBackward0&gt; ]</code></pre>
</div>
</div>
<p>Note that while <code>nn_module</code>s act like functions, but also have a <em>state</em>, most importantly their parameter weights.</p>
<p>It is straightforward to implement a custom <code>nn_module</code>, which requires implementing two key methods:</p>
<ol type="1">
<li><p><code>initialize</code>: This method is the constructor that runs when the model is created. It defines the layers and their dimensions.</p></li>
<li><p><code>forward</code>: This method defines how data flows through your network - it specifies the actual computation path from input to output.</p></li>
</ol>
<p>We can implement a simple linear regression module ourselves.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a>nn_simple_linear <span class="ot">=</span> <span class="fu">nn_module</span>(<span class="st">"nn_simple_linear"</span>,</span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a>  <span class="at">initialize =</span> <span class="cf">function</span>() {</span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a>    <span class="co"># requires_grad is TRUE by default</span></span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a>    self<span class="sc">$</span>a <span class="ot">=</span> <span class="fu">nn_parameter</span>(<span class="fu">torch_randn</span>(<span class="dv">1</span>), <span class="at">requires_grad =</span> <span class="cn">TRUE</span>)</span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a>    self<span class="sc">$</span>b <span class="ot">=</span> <span class="fu">nn_parameter</span>(<span class="fu">torch_randn</span>(<span class="dv">1</span>), <span class="at">requires_grad =</span> <span class="cn">TRUE</span>)</span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a>  },</span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"></a>  <span class="at">forward =</span> <span class="cf">function</span>(x) {</span>
<span id="cb5-8"><a href="#cb5-8" aria-hidden="true" tabindex="-1"></a>    self<span class="sc">$</span>a <span class="sc">*</span> x <span class="sc">+</span> self<span class="sc">$</span>b</span>
<span id="cb5-9"><a href="#cb5-9" aria-hidden="true" tabindex="-1"></a>  }</span>
<span id="cb5-10"><a href="#cb5-10" aria-hidden="true" tabindex="-1"></a>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Note that <code>nn_spiral_net</code> is not an <code>nn_module</code> itself, but an <code>nn_module_generator</code>. To create the <code>nn_module</code>, we call it and pass construction arguments to its <code>$initialize()</code> method:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a>simple_linear <span class="ot">=</span> <span class="fu">nn_simple_linear</span>()</span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a>simple_linear</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>An `nn_module` containing 2 parameters.

── Parameters ──────────────────────────────────────────────────────────────────
• a: Float [1:1]
• b: Float [1:1]</code></pre>
</div>
</div>
<p>Further, note that we need to wrap the trainable tensors in <code>nn_parameter()</code>, otherwise they will not be included in the <code>$parameters</code>. Only those weights that are part of the networks parameters and for which <code>$requires_grad</code> is <code>TRUE</code> will later be updated by the optimizer.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb8"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a>simple_linear<span class="sc">$</span>parameters</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>$a
torch_tensor
 0.2896
[ CPUFloatType{1} ][ requires_grad = TRUE ]

$b
torch_tensor
-1.4755
[ CPUFloatType{1} ][ requires_grad = TRUE ]</code></pre>
</div>
</div>
<p>Besides parameters, neural networks can also have <strong>buffers</strong> (<code>nn_buffer</code>). Buffers are tensors that are part of the model’s state but don’t receive gradients during backpropagation. They’re commonly used for:</p>
<ul>
<li>Running statistics in batch normalization (mean/variance)</li>
<li>Pre-computed constants</li>
</ul>
<p>Another important method of a network is <code>$state_dict()</code>, which returns the network’s parameters and buffers.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb10"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a>simple_linear<span class="sc">$</span><span class="fu">state_dict</span>()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>$a
torch_tensor
 0.2896
[ CPUFloatType{1} ][ requires_grad = TRUE ]

$b
torch_tensor
-1.4755
[ CPUFloatType{1} ][ requires_grad = TRUE ]</code></pre>
</div>
</div>
<p>You can also load new parameters into a network usign <code>$load_state_dict()</code>:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb12"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a>simple_linear<span class="sc">$</span><span class="fu">load_state_dict</span>(<span class="fu">list</span>(</span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a>  <span class="at">a =</span> <span class="fu">nn_parameter</span>(<span class="fu">torch_tensor</span>(<span class="dv">1</span>)),</span>
<span id="cb12-3"><a href="#cb12-3" aria-hidden="true" tabindex="-1"></a>  <span class="at">b =</span> <span class="fu">nn_parameter</span>(<span class="fu">torch_tensor</span>(<span class="dv">0</span>))</span>
<span id="cb12-4"><a href="#cb12-4" aria-hidden="true" tabindex="-1"></a>))</span>
<span id="cb12-5"><a href="#cb12-5" aria-hidden="true" tabindex="-1"></a>simple_linear<span class="sc">$</span><span class="fu">state_dict</span>()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>$a
torch_tensor
 1
[ CPUFloatType{1} ][ requires_grad = TRUE ]

$b
torch_tensor
 0
[ CPUFloatType{1} ][ requires_grad = TRUE ]</code></pre>
</div>
</div>
<p>The state dict can for example be used to save the weights of a network. Note that in general, you cannot simply save and load <code>torch</code> objects using <code>saveRDS</code> and <code>readRDS</code>:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb14"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a>pth <span class="ot">=</span> <span class="fu">tempfile</span>()</span>
<span id="cb14-2"><a href="#cb14-2" aria-hidden="true" tabindex="-1"></a><span class="fu">saveRDS</span>(simple_linear<span class="sc">$</span><span class="fu">state_dict</span>(), pth)</span>
<span id="cb14-3"><a href="#cb14-3" aria-hidden="true" tabindex="-1"></a><span class="fu">readRDS</span>(pth)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>$a
torch_tensor</code></pre>
</div>
<div class="cell-output cell-output-error">
<pre><code>Error in (function (self) : external pointer is not valid</code></pre>
</div>
</div>
<p>Instead, you can use <code>torch_save</code> and <code>torch_load</code>:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb17"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a><span class="fu">torch_save</span>(simple_linear<span class="sc">$</span>parameters, pth)</span>
<span id="cb17-2"><a href="#cb17-2" aria-hidden="true" tabindex="-1"></a><span class="fu">torch_load</span>(pth)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>$a
torch_tensor
 1
[ CPUFloatType{1} ][ requires_grad = TRUE ]

$b
torch_tensor
 0
[ CPUFloatType{1} ][ requires_grad = TRUE ]</code></pre>
</div>
</div>
<p>Besides adding parameters and buffers to the network’s state dict by registering <code>nn_parameter</code>s and <code>nn_buffer</code>s in the module’s <code>$initialize()</code> method, you can also register other <code>nn_module</code>s.</p>
<p>Below, we showcast this by defining a neural network with two hidden layers and relu activation. We call it <em>spiral net</em> as we will later use it to classify two spirals.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb19"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb19-1"><a href="#cb19-1" aria-hidden="true" tabindex="-1"></a>nn_spiral_net <span class="ot">&lt;-</span> <span class="fu">nn_module</span>(<span class="st">"nn_spiral_net"</span>,</span>
<span id="cb19-2"><a href="#cb19-2" aria-hidden="true" tabindex="-1"></a>  <span class="at">initialize =</span> <span class="cf">function</span>(input_size, hidden_size, output_size) {</span>
<span id="cb19-3"><a href="#cb19-3" aria-hidden="true" tabindex="-1"></a>    self<span class="sc">$</span>fc1 <span class="ot">&lt;-</span> <span class="fu">nn_linear</span>(input_size, hidden_size)</span>
<span id="cb19-4"><a href="#cb19-4" aria-hidden="true" tabindex="-1"></a>    self<span class="sc">$</span>fc2 <span class="ot">&lt;-</span> <span class="fu">nn_linear</span>(hidden_size, hidden_size)</span>
<span id="cb19-5"><a href="#cb19-5" aria-hidden="true" tabindex="-1"></a>    self<span class="sc">$</span>fc3 <span class="ot">&lt;-</span> <span class="fu">nn_linear</span>(hidden_size, output_size)</span>
<span id="cb19-6"><a href="#cb19-6" aria-hidden="true" tabindex="-1"></a>    self<span class="sc">$</span>relu <span class="ot">=</span> <span class="fu">nn_relu</span>()</span>
<span id="cb19-7"><a href="#cb19-7" aria-hidden="true" tabindex="-1"></a>  },</span>
<span id="cb19-8"><a href="#cb19-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-9"><a href="#cb19-9" aria-hidden="true" tabindex="-1"></a>  <span class="at">forward =</span> <span class="cf">function</span>(x) {</span>
<span id="cb19-10"><a href="#cb19-10" aria-hidden="true" tabindex="-1"></a>    x <span class="sc">|&gt;</span></span>
<span id="cb19-11"><a href="#cb19-11" aria-hidden="true" tabindex="-1"></a>      self<span class="sc">$</span><span class="fu">fc1</span>() <span class="sc">|&gt;</span></span>
<span id="cb19-12"><a href="#cb19-12" aria-hidden="true" tabindex="-1"></a>      self<span class="sc">$</span><span class="fu">relu</span>() <span class="sc">|&gt;</span></span>
<span id="cb19-13"><a href="#cb19-13" aria-hidden="true" tabindex="-1"></a>      self<span class="sc">$</span><span class="fu">fc2</span>() <span class="sc">|&gt;</span></span>
<span id="cb19-14"><a href="#cb19-14" aria-hidden="true" tabindex="-1"></a>      self<span class="sc">$</span><span class="fu">relu</span>() <span class="sc">|&gt;</span></span>
<span id="cb19-15"><a href="#cb19-15" aria-hidden="true" tabindex="-1"></a>      self<span class="sc">$</span><span class="fu">fc3</span>()</span>
<span id="cb19-16"><a href="#cb19-16" aria-hidden="true" tabindex="-1"></a>  }</span>
<span id="cb19-17"><a href="#cb19-17" aria-hidden="true" tabindex="-1"></a>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Tip
</div>
</div>
<div class="callout-body-container callout-body">
<p>Instead of creating an <code>nn_relu()</code> during network initialization, we could instead also have used the <code>nnf_relu</code> function directly in the forward pass. This is possible for the activation functions as it has no trainable weights.</p>
<p>In torch in general, <code>nn_</code> functions create module instances that can maintain state (like trainable weights or running statistics), while <code>nnf_</code> functions provide the same operations as pure functions without any state.</p>
<p>Furthermore, for simple sequential networks, we could have also used <code>nn_sequential</code> to defined it instead of <code>nn_module</code>. This allows you to chain layers together in a linear fashion without explicitly defining the forward pass.</p>
</div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb20"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb20-1"><a href="#cb20-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Create model instance</span></span>
<span id="cb20-2"><a href="#cb20-2" aria-hidden="true" tabindex="-1"></a>model <span class="ot">&lt;-</span> <span class="fu">nn_spiral_net</span>(</span>
<span id="cb20-3"><a href="#cb20-3" aria-hidden="true" tabindex="-1"></a>  <span class="at">input_size =</span> <span class="dv">2</span>,</span>
<span id="cb20-4"><a href="#cb20-4" aria-hidden="true" tabindex="-1"></a>  <span class="at">hidden_size =</span> <span class="dv">64</span>,</span>
<span id="cb20-5"><a href="#cb20-5" aria-hidden="true" tabindex="-1"></a>  <span class="at">output_size =</span> <span class="dv">2</span></span>
<span id="cb20-6"><a href="#cb20-6" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb20-7"><a href="#cb20-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-8"><a href="#cb20-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Print model architecture</span></span>
<span id="cb20-9"><a href="#cb20-9" aria-hidden="true" tabindex="-1"></a><span class="fu">print</span>(model)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>An `nn_module` containing 4,482 parameters.

── Modules ─────────────────────────────────────────────────────────────────────
• fc1: &lt;nn_linear&gt; #192 parameters
• fc2: &lt;nn_linear&gt; #4,160 parameters
• fc3: &lt;nn_linear&gt; #130 parameters
• relu: &lt;nn_relu&gt; #0 parameters</code></pre>
</div>
</div>
<p>The model outputs logits (unnormalized probabilities) because when training a classification network with cross-entropy, using logits is more numerically stable. If we wanted to obtain class probabilities we would have to apply the softmax function:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb22"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb22-1"><a href="#cb22-1" aria-hidden="true" tabindex="-1"></a>logits <span class="ot">=</span> <span class="fu">model</span>(<span class="fu">torch_randn</span>(<span class="dv">1</span>, <span class="dv">2</span>))</span>
<span id="cb22-2"><a href="#cb22-2" aria-hidden="true" tabindex="-1"></a><span class="fu">print</span>(logits)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>torch_tensor
 0.1018  0.0443
[ CPUFloatType{1,2} ][ grad_fn = &lt;AddmmBackward0&gt; ]</code></pre>
</div>
<div class="sourceCode cell-code" id="cb24"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb24-1"><a href="#cb24-1" aria-hidden="true" tabindex="-1"></a><span class="co"># dim = 2 applies softmax along the class dimension (columns)</span></span>
<span id="cb24-2"><a href="#cb24-2" aria-hidden="true" tabindex="-1"></a><span class="fu">nnf_softmax</span>(logits, <span class="at">dim =</span> <span class="dv">2</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>torch_tensor
 0.5144  0.4856
[ CPUFloatType{1,2} ][ grad_fn = &lt;SoftmaxBackward0&gt; ]</code></pre>
</div>
</div>
<p>Next, let’s create a synthetic spiral dataset for binary classification:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb26"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb26-1"><a href="#cb26-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(torch)</span>
<span id="cb26-2"><a href="#cb26-2" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(ggplot2)</span>
<span id="cb26-3"><a href="#cb26-3" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(mlbench)</span>
<span id="cb26-4"><a href="#cb26-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-5"><a href="#cb26-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Generate spiral data</span></span>
<span id="cb26-6"><a href="#cb26-6" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">123</span>)</span>
<span id="cb26-7"><a href="#cb26-7" aria-hidden="true" tabindex="-1"></a>n <span class="ot">&lt;-</span> <span class="dv">500</span></span>
<span id="cb26-8"><a href="#cb26-8" aria-hidden="true" tabindex="-1"></a>spiral <span class="ot">&lt;-</span> <span class="fu">mlbench.spirals</span>(n, <span class="at">sd =</span> <span class="fl">0.1</span>)</span>
<span id="cb26-9"><a href="#cb26-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-10"><a href="#cb26-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Convert to data frame</span></span>
<span id="cb26-11"><a href="#cb26-11" aria-hidden="true" tabindex="-1"></a>spiral_data <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(</span>
<span id="cb26-12"><a href="#cb26-12" aria-hidden="true" tabindex="-1"></a>  <span class="at">x =</span> spiral<span class="sc">$</span>x[,<span class="dv">1</span>],</span>
<span id="cb26-13"><a href="#cb26-13" aria-hidden="true" tabindex="-1"></a>  <span class="at">y =</span> spiral<span class="sc">$</span>x[,<span class="dv">2</span>],</span>
<span id="cb26-14"><a href="#cb26-14" aria-hidden="true" tabindex="-1"></a>  <span class="at">label =</span> <span class="fu">as.factor</span>(spiral<span class="sc">$</span>classes)</span>
<span id="cb26-15"><a href="#cb26-15" aria-hidden="true" tabindex="-1"></a>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>The data looks like this:</p>
<div class="cell">
<div class="cell-output-display">
<p><img src="module-data-optim_files/figure-html/unnamed-chunk-14-1.png" class="img-fluid" width="672"></p>
</div>
</div>
</section>
<section id="dataset-and-dataloader" class="level1">
<h1>Dataset and DataLoader</h1>
<p>The <code>dataset</code> and <code>dataloader</code> classes separate concerns:</p>
<ul>
<li><code>dataset</code>: Handles data storage and access to individual samples
<ul>
<li><code>.getitem()</code>: Returns a single sample. It does not matter <em>how</em>: An image could be read from disk or a record could be retrieved from a database.</li>
<li><code>.getbatch()</code> (optional): returns a full batch</li>
<li><code>.length()</code>: Returns dataset size</li>
</ul></li>
<li><code>dataloader</code>: Handles batching, shuffling, and parallel loading
<ul>
<li>Creates mini-batches</li>
<li>Optionally shuffles data</li>
<li>Can load data in parallel</li>
</ul></li>
</ul>
<p>We will start by implementing the dataset. In its <code>$initialize()</code> method it expects a <code>data.frame</code> with columns <code>"x"</code>, <code>"y"</code>, and <code>"label"</code>. We then convert these two tensors and store it in the object.</p>
<p>Below, we chose to implement <code>.getitem()</code>, but we could have also implemented <code>.getbatch()</code>.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb27"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb27-1"><a href="#cb27-1" aria-hidden="true" tabindex="-1"></a>spiral_dataset <span class="ot">&lt;-</span> <span class="fu">dataset</span>(</span>
<span id="cb27-2"><a href="#cb27-2" aria-hidden="true" tabindex="-1"></a>  <span class="at">name =</span> <span class="st">"spiral_dataset"</span>,</span>
<span id="cb27-3"><a href="#cb27-3" aria-hidden="true" tabindex="-1"></a>  <span class="at">initialize =</span> <span class="cf">function</span>(data) {</span>
<span id="cb27-4"><a href="#cb27-4" aria-hidden="true" tabindex="-1"></a>    self<span class="sc">$</span>x <span class="ot">&lt;-</span> <span class="fu">torch_tensor</span>(<span class="fu">as.matrix</span>(data[, <span class="fu">c</span>(<span class="st">"x"</span>, <span class="st">"y"</span>)]))</span>
<span id="cb27-5"><a href="#cb27-5" aria-hidden="true" tabindex="-1"></a>    self<span class="sc">$</span>y <span class="ot">&lt;-</span> <span class="fu">torch_tensor</span>(<span class="fu">as.integer</span>(data<span class="sc">$</span>label))</span>
<span id="cb27-6"><a href="#cb27-6" aria-hidden="true" tabindex="-1"></a>  },</span>
<span id="cb27-7"><a href="#cb27-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-8"><a href="#cb27-8" aria-hidden="true" tabindex="-1"></a>  <span class="at">.getitem =</span> <span class="cf">function</span>(i) {</span>
<span id="cb27-9"><a href="#cb27-9" aria-hidden="true" tabindex="-1"></a>    <span class="fu">list</span>(</span>
<span id="cb27-10"><a href="#cb27-10" aria-hidden="true" tabindex="-1"></a>      <span class="at">x =</span> self<span class="sc">$</span>x[i,],</span>
<span id="cb27-11"><a href="#cb27-11" aria-hidden="true" tabindex="-1"></a>      <span class="at">y =</span> self<span class="sc">$</span>y[i]</span>
<span id="cb27-12"><a href="#cb27-12" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb27-13"><a href="#cb27-13" aria-hidden="true" tabindex="-1"></a>  },</span>
<span id="cb27-14"><a href="#cb27-14" aria-hidden="true" tabindex="-1"></a>  <span class="at">.length =</span> <span class="cf">function</span>() {</span>
<span id="cb27-15"><a href="#cb27-15" aria-hidden="true" tabindex="-1"></a>    self<span class="sc">$</span>y<span class="sc">$</span><span class="fu">size</span>()[[<span class="dv">1</span>]]</span>
<span id="cb27-16"><a href="#cb27-16" aria-hidden="true" tabindex="-1"></a>  }</span>
<span id="cb27-17"><a href="#cb27-17" aria-hidden="true" tabindex="-1"></a>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Now that we have defined the dataset class, we will create a train and validation dataset:</p>
<p>Training and validation datasets serve different purposes: - Training data is used to update the model’s parameters and learn patterns - Validation data helps evaluate how well the model generalizes to unseen data, detect overfitting, and guide model selection decisions</p>
<p>Validation in deep learning is crucial for: 1. Detecting overfitting: If training loss decreases but validation loss increases, the model is likely overfitting to the training data 2. Model selection: We can use validation performance to choose the best model architecture and hyperparameters 3. Early stopping: We can stop training when validation performance stops improving to prevent overfitting</p>
<p>The validation set acts as a proxy for unseen data, giving us an estimate of how well our model will generalize to new examples. It’s important that we don’t use this data for training, keeping it completely separate to get an unbiased evaluation of model performance.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb28"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb28-1"><a href="#cb28-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Split data into train and validation sets</span></span>
<span id="cb28-2"><a href="#cb28-2" aria-hidden="true" tabindex="-1"></a>train_ids <span class="ot">=</span> <span class="fu">sample</span>(<span class="dv">1</span><span class="sc">:</span><span class="dv">500</span>, <span class="dv">400</span>)</span>
<span id="cb28-3"><a href="#cb28-3" aria-hidden="true" tabindex="-1"></a>train_data <span class="ot">&lt;-</span> spiral_data[train_ids,]</span>
<span id="cb28-4"><a href="#cb28-4" aria-hidden="true" tabindex="-1"></a>valid_data <span class="ot">&lt;-</span> spiral_data[<span class="sc">-</span>train_ids,]</span>
<span id="cb28-5"><a href="#cb28-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-6"><a href="#cb28-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Create datasets</span></span>
<span id="cb28-7"><a href="#cb28-7" aria-hidden="true" tabindex="-1"></a>train_dataset <span class="ot">&lt;-</span> <span class="fu">spiral_dataset</span>(train_data)</span>
<span id="cb28-8"><a href="#cb28-8" aria-hidden="true" tabindex="-1"></a>valid_dataset <span class="ot">&lt;-</span> <span class="fu">spiral_dataset</span>(valid_data)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>We can access the individual elements via the <code>$.getitem()</code> method:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb29"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb29-1"><a href="#cb29-1" aria-hidden="true" tabindex="-1"></a>train_dataset<span class="sc">$</span><span class="fu">.getitem</span>(<span class="dv">1</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>$x
torch_tensor
-0.2233
-0.8093
[ CPUFloatType{2} ]

$y
torch_tensor
1
[ CPULongType{} ]</code></pre>
</div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb31"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb31-1"><a href="#cb31-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Create dataloaders</span></span>
<span id="cb31-2"><a href="#cb31-2" aria-hidden="true" tabindex="-1"></a>train_loader <span class="ot">&lt;-</span> <span class="fu">dataloader</span>(</span>
<span id="cb31-3"><a href="#cb31-3" aria-hidden="true" tabindex="-1"></a>  train_dataset,</span>
<span id="cb31-4"><a href="#cb31-4" aria-hidden="true" tabindex="-1"></a>  <span class="at">batch_size =</span> <span class="dv">64</span>,</span>
<span id="cb31-5"><a href="#cb31-5" aria-hidden="true" tabindex="-1"></a>  <span class="at">shuffle =</span> <span class="cn">TRUE</span></span>
<span id="cb31-6"><a href="#cb31-6" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb31-7"><a href="#cb31-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb31-8"><a href="#cb31-8" aria-hidden="true" tabindex="-1"></a>valid_loader <span class="ot">&lt;-</span> <span class="fu">dataloader</span>(</span>
<span id="cb31-9"><a href="#cb31-9" aria-hidden="true" tabindex="-1"></a>  valid_dataset,</span>
<span id="cb31-10"><a href="#cb31-10" aria-hidden="true" tabindex="-1"></a>  <span class="at">batch_size =</span> <span class="dv">64</span>,</span>
<span id="cb31-11"><a href="#cb31-11" aria-hidden="true" tabindex="-1"></a>  <span class="at">shuffle =</span> <span class="cn">FALSE</span></span>
<span id="cb31-12"><a href="#cb31-12" aria-hidden="true" tabindex="-1"></a>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>The most common way to iterate over the batches of a <code>dataloader</code> is to use the <code>coro::loop</code> function which almost looks like a for loop:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb32"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb32-1"><a href="#cb32-1" aria-hidden="true" tabindex="-1"></a>n_batches <span class="ot">&lt;-</span> <span class="dv">0</span></span>
<span id="cb32-2"><a href="#cb32-2" aria-hidden="true" tabindex="-1"></a>coro<span class="sc">::</span><span class="fu">loop</span>(<span class="cf">for</span> (batch <span class="cf">in</span> train_loader) {</span>
<span id="cb32-3"><a href="#cb32-3" aria-hidden="true" tabindex="-1"></a>  n_batches <span class="ot">&lt;-</span> n_batches <span class="sc">+</span> <span class="dv">1</span></span>
<span id="cb32-4"><a href="#cb32-4" aria-hidden="true" tabindex="-1"></a>})</span>
<span id="cb32-5"><a href="#cb32-5" aria-hidden="true" tabindex="-1"></a><span class="fu">print</span>(<span class="fu">head</span>(batch<span class="sc">$</span>x))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>torch_tensor
 0.2497  0.0796
-0.3869 -0.6818
-0.6957  0.1147
-0.0103  0.8366
 0.7821 -0.4226
-0.3766 -0.4060
[ CPUFloatType{6,2} ]</code></pre>
</div>
<div class="sourceCode cell-code" id="cb34"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb34-1"><a href="#cb34-1" aria-hidden="true" tabindex="-1"></a><span class="fu">print</span>(<span class="fu">head</span>(batch<span class="sc">$</span>y))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>torch_tensor
 1
 1
 1
 2
 1
 2
[ CPULongType{6} ]</code></pre>
</div>
<div class="sourceCode cell-code" id="cb36"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb36-1"><a href="#cb36-1" aria-hidden="true" tabindex="-1"></a><span class="fu">print</span>(n_batches)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 7</code></pre>
</div>
</div>
<p>It is also possible to manually do this iteration by first creating an iterator using <code>torch::dataloader_make_iter()</code> and then calling <code>dataloader_next()</code> until <code>NULL</code> is returned and the iterator is exhausted:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb38"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb38-1"><a href="#cb38-1" aria-hidden="true" tabindex="-1"></a>iter <span class="ot">=</span> <span class="fu">dataloader_make_iter</span>(train_loader)</span>
<span id="cb38-2"><a href="#cb38-2" aria-hidden="true" tabindex="-1"></a>batch <span class="ot">=</span> <span class="fu">dataloader_next</span>(iter)</span>
<span id="cb38-3"><a href="#cb38-3" aria-hidden="true" tabindex="-1"></a><span class="fu">print</span>(<span class="fu">head</span>(batch<span class="sc">$</span>x))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>torch_tensor
 0.9425 -0.3511
-0.4098  0.2399
-0.9531  0.4458
 0.1429  0.7114
 0.4824 -0.3720
 0.5232 -0.1947
[ CPUFloatType{6,2} ]</code></pre>
</div>
<div class="sourceCode cell-code" id="cb40"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb40-1"><a href="#cb40-1" aria-hidden="true" tabindex="-1"></a><span class="fu">print</span>(<span class="fu">head</span>(batch<span class="sc">$</span>y))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>torch_tensor
 1
 1
 2
 2
 2
 2
[ CPULongType{6} ]</code></pre>
</div>
</div>
</section>
<section id="optimizers-in-torch" class="level1">
<h1>Optimizers in torch</h1>
<p>torch provides several optimizers, with Adam being one of the most popular choices. The main optimizer API consists of:</p>
<ol type="1">
<li>Initializing the optimizer, which requires passing the parameters of the module to be optimized and setting the optimizer’s hyperparameters such as the learning rate.</li>
<li><code>step()</code>: Update parameters using current gradients</li>
<li><code>zero_grad()</code>: Reset gradients of all the parameters to zero before each backward pass</li>
<li>Just like <code>nn_module</code>s they have a <code>$state_dict()</code> which can e.g.&nbsp;be saved to later resume training.</li>
</ol>
<p>Available optimizers include:</p>
<ul>
<li>SGD (<code>optim_sgd</code>): Basic stochastic gradient descent</li>
<li>Adam (<code>optim_adam</code>): Adaptive moment estimation, combines RMSprop and momentum</li>
<li>RMSprop (<code>optim_rmsprop</code>): Adapts learning rates based on moving average of squared gradients</li>
<li>Adagrad (<code>optim_adagrad</code>): Adapts learning rates based on parameter-specific history</li>
</ul>
<div class="cell">
<div class="sourceCode cell-code" id="cb42"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb42-1"><a href="#cb42-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Create optimizer</span></span>
<span id="cb42-2"><a href="#cb42-2" aria-hidden="true" tabindex="-1"></a>optimizer <span class="ot">&lt;-</span> <span class="fu">optim_adam</span>(model<span class="sc">$</span>parameters, <span class="at">lr =</span> <span class="fl">0.2</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>We will see the optimizer in action in the next section!</p>
</section>
<section id="training-loop" class="level1">
<h1>Training Loop</h1>
<p>Now we can put everything together:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb43"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb43-1"><a href="#cb43-1" aria-hidden="true" tabindex="-1"></a><span class="fu">debug</span>(optimizer<span class="sc">$</span>step)</span>
<span id="cb43-2"><a href="#cb43-2" aria-hidden="true" tabindex="-1"></a><span class="co"># Training settings</span></span>
<span id="cb43-3"><a href="#cb43-3" aria-hidden="true" tabindex="-1"></a>n_epochs <span class="ot">&lt;-</span> <span class="dv">2</span></span>
<span id="cb43-4"><a href="#cb43-4" aria-hidden="true" tabindex="-1"></a>device <span class="ot">&lt;-</span> <span class="st">"mps"</span></span>
<span id="cb43-5"><a href="#cb43-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb43-6"><a href="#cb43-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Move model to device</span></span>
<span id="cb43-7"><a href="#cb43-7" aria-hidden="true" tabindex="-1"></a>model<span class="sc">$</span><span class="fu">to</span>(<span class="at">device =</span> device)</span>
<span id="cb43-8"><a href="#cb43-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb43-9"><a href="#cb43-9" aria-hidden="true" tabindex="-1"></a><span class="co"># Training loop</span></span>
<span id="cb43-10"><a href="#cb43-10" aria-hidden="true" tabindex="-1"></a>history <span class="ot">=</span> <span class="fu">list</span>(<span class="at">loss =</span> <span class="fu">numeric</span>(), <span class="at">train_acc =</span> <span class="fu">numeric</span>(), <span class="at">valid_acc =</span> <span class="fu">numeric</span>())</span>
<span id="cb43-11"><a href="#cb43-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb43-12"><a href="#cb43-12" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span>(epoch <span class="cf">in</span> <span class="fu">seq_len</span>(n_epochs)) {</span>
<span id="cb43-13"><a href="#cb43-13" aria-hidden="true" tabindex="-1"></a>  model<span class="sc">$</span><span class="fu">train</span>()  <span class="co"># Set to training mode</span></span>
<span id="cb43-14"><a href="#cb43-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb43-15"><a href="#cb43-15" aria-hidden="true" tabindex="-1"></a>  <span class="co"># training loop</span></span>
<span id="cb43-16"><a href="#cb43-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb43-17"><a href="#cb43-17" aria-hidden="true" tabindex="-1"></a>  train_losses <span class="ot">&lt;-</span> <span class="fu">numeric</span>()</span>
<span id="cb43-18"><a href="#cb43-18" aria-hidden="true" tabindex="-1"></a>  train_accs <span class="ot">&lt;-</span> <span class="fu">numeric</span>()</span>
<span id="cb43-19"><a href="#cb43-19" aria-hidden="true" tabindex="-1"></a>  coro<span class="sc">::</span><span class="fu">loop</span>(<span class="cf">for</span>(batch <span class="cf">in</span> train_loader) {</span>
<span id="cb43-20"><a href="#cb43-20" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Move batch to device</span></span>
<span id="cb43-21"><a href="#cb43-21" aria-hidden="true" tabindex="-1"></a>    x <span class="ot">&lt;-</span> batch<span class="sc">$</span>x<span class="sc">$</span><span class="fu">to</span>(<span class="at">device =</span> device)</span>
<span id="cb43-22"><a href="#cb43-22" aria-hidden="true" tabindex="-1"></a>    y <span class="ot">&lt;-</span> batch<span class="sc">$</span>y<span class="sc">$</span><span class="fu">to</span>(<span class="at">device =</span> device)</span>
<span id="cb43-23"><a href="#cb43-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb43-24"><a href="#cb43-24" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Forward pass</span></span>
<span id="cb43-25"><a href="#cb43-25" aria-hidden="true" tabindex="-1"></a>    output <span class="ot">&lt;-</span> <span class="fu">model</span>(x)</span>
<span id="cb43-26"><a href="#cb43-26" aria-hidden="true" tabindex="-1"></a>    loss <span class="ot">&lt;-</span> <span class="fu">nnf_cross_entropy</span>(output, y)</span>
<span id="cb43-27"><a href="#cb43-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb43-28"><a href="#cb43-28" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Backward pass</span></span>
<span id="cb43-29"><a href="#cb43-29" aria-hidden="true" tabindex="-1"></a>    optimizer<span class="sc">$</span><span class="fu">zero_grad</span>()</span>
<span id="cb43-30"><a href="#cb43-30" aria-hidden="true" tabindex="-1"></a>    loss<span class="sc">$</span><span class="fu">backward</span>()</span>
<span id="cb43-31"><a href="#cb43-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb43-32"><a href="#cb43-32" aria-hidden="true" tabindex="-1"></a>    <span class="fu">browser</span>()</span>
<span id="cb43-33"><a href="#cb43-33" aria-hidden="true" tabindex="-1"></a>    <span class="fu">print</span>(model<span class="sc">$</span>parameters[[<span class="dv">1</span>]]<span class="sc">$</span>grad)</span>
<span id="cb43-34"><a href="#cb43-34" aria-hidden="true" tabindex="-1"></a>    param <span class="ot">=</span> <span class="fu">as_array</span>(model<span class="sc">$</span>parameters[[<span class="dv">1</span>]])</span>
<span id="cb43-35"><a href="#cb43-35" aria-hidden="true" tabindex="-1"></a>    optimizer<span class="sc">$</span><span class="fu">step</span>()</span>
<span id="cb43-36"><a href="#cb43-36" aria-hidden="true" tabindex="-1"></a>    <span class="fu">print</span>(<span class="fu">mean</span>(<span class="fu">abs</span>(param <span class="sc">-</span> <span class="fu">as_array</span>(model<span class="sc">$</span>parameters[[<span class="dv">1</span>]]))))</span>
<span id="cb43-37"><a href="#cb43-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb43-38"><a href="#cb43-38" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Store training losses</span></span>
<span id="cb43-39"><a href="#cb43-39" aria-hidden="true" tabindex="-1"></a>    train_losses <span class="ot">&lt;-</span> <span class="fu">c</span>(train_losses, loss<span class="sc">$</span><span class="fu">item</span>())</span>
<span id="cb43-40"><a href="#cb43-40" aria-hidden="true" tabindex="-1"></a>    train_accs <span class="ot">&lt;-</span> <span class="fu">c</span>(train_accs, <span class="fu">mean</span>(<span class="fu">as_array</span>(output<span class="sc">$</span><span class="fu">argmax</span>(<span class="at">dim =</span> <span class="dv">2</span>) <span class="sc">==</span> y)))</span>
<span id="cb43-41"><a href="#cb43-41" aria-hidden="true" tabindex="-1"></a>  })</span>
<span id="cb43-42"><a href="#cb43-42" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb43-43"><a href="#cb43-43" aria-hidden="true" tabindex="-1"></a>  history<span class="sc">$</span>loss <span class="ot">&lt;-</span> <span class="fu">c</span>(history<span class="sc">$</span>loss, <span class="fu">mean</span>(train_losses))</span>
<span id="cb43-44"><a href="#cb43-44" aria-hidden="true" tabindex="-1"></a>  history<span class="sc">$</span>train_acc <span class="ot">&lt;-</span> <span class="fu">c</span>(history<span class="sc">$</span>train_acc, <span class="fu">mean</span>(train_accs))</span>
<span id="cb43-45"><a href="#cb43-45" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb43-46"><a href="#cb43-46" aria-hidden="true" tabindex="-1"></a>  <span class="co"># validation loop</span></span>
<span id="cb43-47"><a href="#cb43-47" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb43-48"><a href="#cb43-48" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Set model into evaluation mode</span></span>
<span id="cb43-49"><a href="#cb43-49" aria-hidden="true" tabindex="-1"></a>  model<span class="sc">$</span><span class="fu">eval</span>()</span>
<span id="cb43-50"><a href="#cb43-50" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb43-51"><a href="#cb43-51" aria-hidden="true" tabindex="-1"></a>  valid_accs <span class="ot">&lt;-</span> <span class="fu">numeric</span>()</span>
<span id="cb43-52"><a href="#cb43-52" aria-hidden="true" tabindex="-1"></a>  coro<span class="sc">::</span><span class="fu">loop</span>(<span class="cf">for</span>(batch <span class="cf">in</span> valid_loader) {</span>
<span id="cb43-53"><a href="#cb43-53" aria-hidden="true" tabindex="-1"></a>    x <span class="ot">&lt;-</span> batch<span class="sc">$</span>x<span class="sc">$</span><span class="fu">to</span>(<span class="at">device =</span> device)</span>
<span id="cb43-54"><a href="#cb43-54" aria-hidden="true" tabindex="-1"></a>    y <span class="ot">&lt;-</span> batch<span class="sc">$</span>y<span class="sc">$</span><span class="fu">to</span>(<span class="at">device =</span> device)</span>
<span id="cb43-55"><a href="#cb43-55" aria-hidden="true" tabindex="-1"></a>    output <span class="ot">&lt;-</span> <span class="fu">with_no_grad</span>(<span class="fu">model</span>(x))</span>
<span id="cb43-56"><a href="#cb43-56" aria-hidden="true" tabindex="-1"></a>    valid_acc <span class="ot">&lt;-</span> <span class="fu">as_array</span>(output<span class="sc">$</span><span class="fu">argmax</span>(<span class="at">dim =</span> <span class="dv">2</span>) <span class="sc">==</span> y)</span>
<span id="cb43-57"><a href="#cb43-57" aria-hidden="true" tabindex="-1"></a>    valid_accs <span class="ot">=</span> <span class="fu">c</span>(valid_accs, <span class="fu">mean</span>(valid_acc))</span>
<span id="cb43-58"><a href="#cb43-58" aria-hidden="true" tabindex="-1"></a>  })</span>
<span id="cb43-59"><a href="#cb43-59" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb43-60"><a href="#cb43-60" aria-hidden="true" tabindex="-1"></a>  history<span class="sc">$</span>valid_acc <span class="ot">&lt;-</span> <span class="fu">c</span>(history<span class="sc">$</span>valid_acc, <span class="fu">mean</span>(valid_accs))</span>
<span id="cb43-61"><a href="#cb43-61" aria-hidden="true" tabindex="-1"></a>}</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Called from: eval_bare(loop, env)
torch_tensor
0.001 *
 3.1720  1.0426
 -1.1880  0.5351
  0.5167 -0.2328
  0.0000  0.0000
 -0.0788  0.6213
  1.4944  0.0494
  1.4955  0.6167
 -0.0694 -0.2371
  0.2413  1.0063
  1.2513  1.1381
 -1.1691 -0.9084
 -0.2082  0.2204
  0.0998  0.1785
  0.3030 -3.3863
 -0.7519 -0.5013
  0.5504  1.2651
 -1.7415 -0.3828
 -0.4231  0.4604
  1.6828  0.7078
  0.9722  1.0542
  0.0948 -0.5846
  0.4701  0.9037
  0.0000  0.0000
 -0.8350  1.2751
  0.0420  0.1648
  3.3269  1.5966
 -0.8878 -1.5316
  0.9655  0.5289
  1.4799 -0.1212
... [the output was truncated (use n=-1 to disable)]
[ MPSFloatType{64,2} ]
debugging in: optimizer$step()
debug: {
    loop_fun &lt;- function(group, param, g, p) {
        grad &lt;- param$grad
        amsgrad &lt;- group$amsgrad
        if (length(state(param)) == 0) {
            state(param) &lt;- list()
            state(param)[["step"]] &lt;- 0
            state(param)[["exp_avg"]] &lt;- torch_zeros_like(param, 
                memory_format = torch_preserve_format())
            state(param)[["exp_avg_sq"]] &lt;- torch_zeros_like(param, 
                memory_format = torch_preserve_format())
            if (amsgrad) {
                state(param)[["max_exp_avg_sq"]] &lt;- torch_zeros_like(param, 
                  memory_format = torch_preserve_format())
            }
        }
        exp_avg &lt;- state(param)[["exp_avg"]]
        exp_avg_sq &lt;- state(param)[["exp_avg_sq"]]
        if (amsgrad) {
            max_exp_avg_sq &lt;- state(param)[["max_exp_avg_sq"]]
        }
        beta1 &lt;- group$betas[[1]]
        beta2 &lt;- group$betas[[2]]
        state(param)[["step"]] &lt;- state(param)[["step"]] + 1
        bias_correction1 &lt;- 1 - beta1^state(param)[["step"]]
        bias_correction2 &lt;- 1 - beta2^state(param)[["step"]]
        if (group$weight_decay != 0) {
            grad$add_(param, alpha = group$weight_decay)
        }
        exp_avg$mul_(beta1)$add_(grad, alpha = 1 - beta1)
        exp_avg_sq$mul_(beta2)$addcmul_(grad, grad, value = 1 - 
            beta2)
        if (amsgrad) {
            max_exp_avg_sq$set_data(max_exp_avg_sq$max(other = exp_avg_sq))
            denom &lt;- (max_exp_avg_sq$sqrt()/sqrt(bias_correction2))$add_(group$eps)
        }
        else {
            denom &lt;- (exp_avg_sq$sqrt()/sqrt(bias_correction2))$add_(group$eps)
        }
        step_size &lt;- group$lr/bias_correction1
        param$addcdiv_(exp_avg, denom, value = -step_size)
    }
    private$step_helper(closure, loop_fun)
}
debug: loop_fun &lt;- function(group, param, g, p) {
    grad &lt;- param$grad
    amsgrad &lt;- group$amsgrad
    if (length(state(param)) == 0) {
        state(param) &lt;- list()
        state(param)[["step"]] &lt;- 0
        state(param)[["exp_avg"]] &lt;- torch_zeros_like(param, 
            memory_format = torch_preserve_format())
        state(param)[["exp_avg_sq"]] &lt;- torch_zeros_like(param, 
            memory_format = torch_preserve_format())
        if (amsgrad) {
            state(param)[["max_exp_avg_sq"]] &lt;- torch_zeros_like(param, 
                memory_format = torch_preserve_format())
        }
    }
    exp_avg &lt;- state(param)[["exp_avg"]]
    exp_avg_sq &lt;- state(param)[["exp_avg_sq"]]
    if (amsgrad) {
        max_exp_avg_sq &lt;- state(param)[["max_exp_avg_sq"]]
    }
    beta1 &lt;- group$betas[[1]]
    beta2 &lt;- group$betas[[2]]
    state(param)[["step"]] &lt;- state(param)[["step"]] + 1
    bias_correction1 &lt;- 1 - beta1^state(param)[["step"]]
    bias_correction2 &lt;- 1 - beta2^state(param)[["step"]]
    if (group$weight_decay != 0) {
        grad$add_(param, alpha = group$weight_decay)
    }
    exp_avg$mul_(beta1)$add_(grad, alpha = 1 - beta1)
    exp_avg_sq$mul_(beta2)$addcmul_(grad, grad, value = 1 - beta2)
    if (amsgrad) {
        max_exp_avg_sq$set_data(max_exp_avg_sq$max(other = exp_avg_sq))
        denom &lt;- (max_exp_avg_sq$sqrt()/sqrt(bias_correction2))$add_(group$eps)
    }
    else {
        denom &lt;- (exp_avg_sq$sqrt()/sqrt(bias_correction2))$add_(group$eps)
    }
    step_size &lt;- group$lr/bias_correction1
    param$addcdiv_(exp_avg, denom, value = -step_size)
}
debug: private$step_helper(closure, loop_fun)
exiting from: optimizer$step()
[1] 0
Called from: eval_bare(loop, env)
torch_tensor
0.001 *
 3.6895  3.5698
 -1.9571  1.0062
  0.5299 -0.3602
  0.0000  0.0000
 -0.6988  1.0039
  2.3897  0.8647
  2.2381  1.1014
 -0.1287 -0.9544
  2.0036  4.3875
 -0.1539  2.7790
 -1.5024 -2.4373
 -0.3445  0.5600
  0.1992  0.4389
 -0.9370 -5.2462
 -1.3976 -1.5034
 -1.2241  3.5261
 -1.4333 -1.3061
 -0.4693  0.7899
  2.4172  1.6054
  1.5433  2.2229
  0.2811 -1.3983
  0.6555  1.7179
  0.0000  0.0000
 -1.5030  2.9083
  0.1061  0.6333
  4.1282  3.7002
 -1.2000 -3.5315
  1.2218  1.2493
  1.3131  0.5708
... [the output was truncated (use n=-1 to disable)]
[ MPSFloatType{64,2} ]
debugging in: optimizer$step()
debug: {
    loop_fun &lt;- function(group, param, g, p) {
        grad &lt;- param$grad
        amsgrad &lt;- group$amsgrad
        if (length(state(param)) == 0) {
            state(param) &lt;- list()
            state(param)[["step"]] &lt;- 0
            state(param)[["exp_avg"]] &lt;- torch_zeros_like(param, 
                memory_format = torch_preserve_format())
            state(param)[["exp_avg_sq"]] &lt;- torch_zeros_like(param, 
                memory_format = torch_preserve_format())
            if (amsgrad) {
                state(param)[["max_exp_avg_sq"]] &lt;- torch_zeros_like(param, 
                  memory_format = torch_preserve_format())
            }
        }
        exp_avg &lt;- state(param)[["exp_avg"]]
        exp_avg_sq &lt;- state(param)[["exp_avg_sq"]]
        if (amsgrad) {
            max_exp_avg_sq &lt;- state(param)[["max_exp_avg_sq"]]
        }
        beta1 &lt;- group$betas[[1]]
        beta2 &lt;- group$betas[[2]]
        state(param)[["step"]] &lt;- state(param)[["step"]] + 1
        bias_correction1 &lt;- 1 - beta1^state(param)[["step"]]
        bias_correction2 &lt;- 1 - beta2^state(param)[["step"]]
        if (group$weight_decay != 0) {
            grad$add_(param, alpha = group$weight_decay)
        }
        exp_avg$mul_(beta1)$add_(grad, alpha = 1 - beta1)
        exp_avg_sq$mul_(beta2)$addcmul_(grad, grad, value = 1 - 
            beta2)
        if (amsgrad) {
            max_exp_avg_sq$set_data(max_exp_avg_sq$max(other = exp_avg_sq))
            denom &lt;- (max_exp_avg_sq$sqrt()/sqrt(bias_correction2))$add_(group$eps)
        }
        else {
            denom &lt;- (exp_avg_sq$sqrt()/sqrt(bias_correction2))$add_(group$eps)
        }
        step_size &lt;- group$lr/bias_correction1
        param$addcdiv_(exp_avg, denom, value = -step_size)
    }
    private$step_helper(closure, loop_fun)
}
debug: loop_fun &lt;- function(group, param, g, p) {
    grad &lt;- param$grad
    amsgrad &lt;- group$amsgrad
    if (length(state(param)) == 0) {
        state(param) &lt;- list()
        state(param)[["step"]] &lt;- 0
        state(param)[["exp_avg"]] &lt;- torch_zeros_like(param, 
            memory_format = torch_preserve_format())
        state(param)[["exp_avg_sq"]] &lt;- torch_zeros_like(param, 
            memory_format = torch_preserve_format())
        if (amsgrad) {
            state(param)[["max_exp_avg_sq"]] &lt;- torch_zeros_like(param, 
                memory_format = torch_preserve_format())
        }
    }
    exp_avg &lt;- state(param)[["exp_avg"]]
    exp_avg_sq &lt;- state(param)[["exp_avg_sq"]]
    if (amsgrad) {
        max_exp_avg_sq &lt;- state(param)[["max_exp_avg_sq"]]
    }
    beta1 &lt;- group$betas[[1]]
    beta2 &lt;- group$betas[[2]]
    state(param)[["step"]] &lt;- state(param)[["step"]] + 1
    bias_correction1 &lt;- 1 - beta1^state(param)[["step"]]
    bias_correction2 &lt;- 1 - beta2^state(param)[["step"]]
    if (group$weight_decay != 0) {
        grad$add_(param, alpha = group$weight_decay)
    }
    exp_avg$mul_(beta1)$add_(grad, alpha = 1 - beta1)
    exp_avg_sq$mul_(beta2)$addcmul_(grad, grad, value = 1 - beta2)
    if (amsgrad) {
        max_exp_avg_sq$set_data(max_exp_avg_sq$max(other = exp_avg_sq))
        denom &lt;- (max_exp_avg_sq$sqrt()/sqrt(bias_correction2))$add_(group$eps)
    }
    else {
        denom &lt;- (exp_avg_sq$sqrt()/sqrt(bias_correction2))$add_(group$eps)
    }
    step_size &lt;- group$lr/bias_correction1
    param$addcdiv_(exp_avg, denom, value = -step_size)
}
debug: private$step_helper(closure, loop_fun)
exiting from: optimizer$step()
[1] 0
Called from: eval_bare(loop, env)
torch_tensor
0.001 *
 7.6335  3.2814
 -3.1826  1.8105
  0.5771 -0.2683
  0.0000  0.0000
 -0.1915  1.3030
  2.9352  0.5986
  2.7796  0.9927
  0.0085 -1.3123
  3.1084  5.4236
  2.2134  1.2912
 -2.9079 -2.5513
 -0.2977  0.8158
  0.6347  0.5375
 -2.9889 -5.7847
 -1.9102 -2.2254
 -0.2905  3.5054
 -3.0376 -1.3215
 -0.5217  0.9006
  2.7559  0.3892
  2.1120  2.6613
  0.1611 -1.5787
  0.8631  2.0701
  0.0000  0.0000
 -1.4514  3.8345
  0.3880  0.8601
  7.2558  3.9470
 -2.8621 -4.0903
  1.9825  1.4318
  2.8435  0.8583
... [the output was truncated (use n=-1 to disable)]
[ MPSFloatType{64,2} ]
debugging in: optimizer$step()
debug: {
    loop_fun &lt;- function(group, param, g, p) {
        grad &lt;- param$grad
        amsgrad &lt;- group$amsgrad
        if (length(state(param)) == 0) {
            state(param) &lt;- list()
            state(param)[["step"]] &lt;- 0
            state(param)[["exp_avg"]] &lt;- torch_zeros_like(param, 
                memory_format = torch_preserve_format())
            state(param)[["exp_avg_sq"]] &lt;- torch_zeros_like(param, 
                memory_format = torch_preserve_format())
            if (amsgrad) {
                state(param)[["max_exp_avg_sq"]] &lt;- torch_zeros_like(param, 
                  memory_format = torch_preserve_format())
            }
        }
        exp_avg &lt;- state(param)[["exp_avg"]]
        exp_avg_sq &lt;- state(param)[["exp_avg_sq"]]
        if (amsgrad) {
            max_exp_avg_sq &lt;- state(param)[["max_exp_avg_sq"]]
        }
        beta1 &lt;- group$betas[[1]]
        beta2 &lt;- group$betas[[2]]
        state(param)[["step"]] &lt;- state(param)[["step"]] + 1
        bias_correction1 &lt;- 1 - beta1^state(param)[["step"]]
        bias_correction2 &lt;- 1 - beta2^state(param)[["step"]]
        if (group$weight_decay != 0) {
            grad$add_(param, alpha = group$weight_decay)
        }
        exp_avg$mul_(beta1)$add_(grad, alpha = 1 - beta1)
        exp_avg_sq$mul_(beta2)$addcmul_(grad, grad, value = 1 - 
            beta2)
        if (amsgrad) {
            max_exp_avg_sq$set_data(max_exp_avg_sq$max(other = exp_avg_sq))
            denom &lt;- (max_exp_avg_sq$sqrt()/sqrt(bias_correction2))$add_(group$eps)
        }
        else {
            denom &lt;- (exp_avg_sq$sqrt()/sqrt(bias_correction2))$add_(group$eps)
        }
        step_size &lt;- group$lr/bias_correction1
        param$addcdiv_(exp_avg, denom, value = -step_size)
    }
    private$step_helper(closure, loop_fun)
}
debug: loop_fun &lt;- function(group, param, g, p) {
    grad &lt;- param$grad
    amsgrad &lt;- group$amsgrad
    if (length(state(param)) == 0) {
        state(param) &lt;- list()
        state(param)[["step"]] &lt;- 0
        state(param)[["exp_avg"]] &lt;- torch_zeros_like(param, 
            memory_format = torch_preserve_format())
        state(param)[["exp_avg_sq"]] &lt;- torch_zeros_like(param, 
            memory_format = torch_preserve_format())
        if (amsgrad) {
            state(param)[["max_exp_avg_sq"]] &lt;- torch_zeros_like(param, 
                memory_format = torch_preserve_format())
        }
    }
    exp_avg &lt;- state(param)[["exp_avg"]]
    exp_avg_sq &lt;- state(param)[["exp_avg_sq"]]
    if (amsgrad) {
        max_exp_avg_sq &lt;- state(param)[["max_exp_avg_sq"]]
    }
    beta1 &lt;- group$betas[[1]]
    beta2 &lt;- group$betas[[2]]
    state(param)[["step"]] &lt;- state(param)[["step"]] + 1
    bias_correction1 &lt;- 1 - beta1^state(param)[["step"]]
    bias_correction2 &lt;- 1 - beta2^state(param)[["step"]]
    if (group$weight_decay != 0) {
        grad$add_(param, alpha = group$weight_decay)
    }
    exp_avg$mul_(beta1)$add_(grad, alpha = 1 - beta1)
    exp_avg_sq$mul_(beta2)$addcmul_(grad, grad, value = 1 - beta2)
    if (amsgrad) {
        max_exp_avg_sq$set_data(max_exp_avg_sq$max(other = exp_avg_sq))
        denom &lt;- (max_exp_avg_sq$sqrt()/sqrt(bias_correction2))$add_(group$eps)
    }
    else {
        denom &lt;- (exp_avg_sq$sqrt()/sqrt(bias_correction2))$add_(group$eps)
    }
    step_size &lt;- group$lr/bias_correction1
    param$addcdiv_(exp_avg, denom, value = -step_size)
}
debug: private$step_helper(closure, loop_fun)
exiting from: optimizer$step()
[1] 0
Called from: eval_bare(loop, env)
torch_tensor
0.001 *
 7.6572  4.5671
 -5.6761  3.3167
  1.1684 -0.9163
  0.0000  0.0000
 -0.3978  1.4034
  3.4052  1.0734
  3.8394  1.7872
  0.6065 -2.8035
  3.2274  7.1838
  0.2024  3.3991
 -3.2338 -3.6642
  0.4444  1.2070
 -0.0718  1.0063
 -1.5411 -8.8180
 -1.7016 -3.2791
 -1.3160  4.5854
 -3.0127 -1.7380
 -1.9018  1.7512
  2.6954  0.9014
  1.7892  3.6768
  0.2611 -2.3596
  0.8667  3.3865
  0.0000  0.0000
 -4.9197  7.6128
  0.6075  1.2747
  7.9148  5.7791
 -3.1475 -4.9424
  2.9103  2.5736
  3.4407  0.9957
... [the output was truncated (use n=-1 to disable)]
[ MPSFloatType{64,2} ]
debugging in: optimizer$step()
debug: {
    loop_fun &lt;- function(group, param, g, p) {
        grad &lt;- param$grad
        amsgrad &lt;- group$amsgrad
        if (length(state(param)) == 0) {
            state(param) &lt;- list()
            state(param)[["step"]] &lt;- 0
            state(param)[["exp_avg"]] &lt;- torch_zeros_like(param, 
                memory_format = torch_preserve_format())
            state(param)[["exp_avg_sq"]] &lt;- torch_zeros_like(param, 
                memory_format = torch_preserve_format())
            if (amsgrad) {
                state(param)[["max_exp_avg_sq"]] &lt;- torch_zeros_like(param, 
                  memory_format = torch_preserve_format())
            }
        }
        exp_avg &lt;- state(param)[["exp_avg"]]
        exp_avg_sq &lt;- state(param)[["exp_avg_sq"]]
        if (amsgrad) {
            max_exp_avg_sq &lt;- state(param)[["max_exp_avg_sq"]]
        }
        beta1 &lt;- group$betas[[1]]
        beta2 &lt;- group$betas[[2]]
        state(param)[["step"]] &lt;- state(param)[["step"]] + 1
        bias_correction1 &lt;- 1 - beta1^state(param)[["step"]]
        bias_correction2 &lt;- 1 - beta2^state(param)[["step"]]
        if (group$weight_decay != 0) {
            grad$add_(param, alpha = group$weight_decay)
        }
        exp_avg$mul_(beta1)$add_(grad, alpha = 1 - beta1)
        exp_avg_sq$mul_(beta2)$addcmul_(grad, grad, value = 1 - 
            beta2)
        if (amsgrad) {
            max_exp_avg_sq$set_data(max_exp_avg_sq$max(other = exp_avg_sq))
            denom &lt;- (max_exp_avg_sq$sqrt()/sqrt(bias_correction2))$add_(group$eps)
        }
        else {
            denom &lt;- (exp_avg_sq$sqrt()/sqrt(bias_correction2))$add_(group$eps)
        }
        step_size &lt;- group$lr/bias_correction1
        param$addcdiv_(exp_avg, denom, value = -step_size)
    }
    private$step_helper(closure, loop_fun)
}
debug: loop_fun &lt;- function(group, param, g, p) {
    grad &lt;- param$grad
    amsgrad &lt;- group$amsgrad
    if (length(state(param)) == 0) {
        state(param) &lt;- list()
        state(param)[["step"]] &lt;- 0
        state(param)[["exp_avg"]] &lt;- torch_zeros_like(param, 
            memory_format = torch_preserve_format())
        state(param)[["exp_avg_sq"]] &lt;- torch_zeros_like(param, 
            memory_format = torch_preserve_format())
        if (amsgrad) {
            state(param)[["max_exp_avg_sq"]] &lt;- torch_zeros_like(param, 
                memory_format = torch_preserve_format())
        }
    }
    exp_avg &lt;- state(param)[["exp_avg"]]
    exp_avg_sq &lt;- state(param)[["exp_avg_sq"]]
    if (amsgrad) {
        max_exp_avg_sq &lt;- state(param)[["max_exp_avg_sq"]]
    }
    beta1 &lt;- group$betas[[1]]
    beta2 &lt;- group$betas[[2]]
    state(param)[["step"]] &lt;- state(param)[["step"]] + 1
    bias_correction1 &lt;- 1 - beta1^state(param)[["step"]]
    bias_correction2 &lt;- 1 - beta2^state(param)[["step"]]
    if (group$weight_decay != 0) {
        grad$add_(param, alpha = group$weight_decay)
    }
    exp_avg$mul_(beta1)$add_(grad, alpha = 1 - beta1)
    exp_avg_sq$mul_(beta2)$addcmul_(grad, grad, value = 1 - beta2)
    if (amsgrad) {
        max_exp_avg_sq$set_data(max_exp_avg_sq$max(other = exp_avg_sq))
        denom &lt;- (max_exp_avg_sq$sqrt()/sqrt(bias_correction2))$add_(group$eps)
    }
    else {
        denom &lt;- (exp_avg_sq$sqrt()/sqrt(bias_correction2))$add_(group$eps)
    }
    step_size &lt;- group$lr/bias_correction1
    param$addcdiv_(exp_avg, denom, value = -step_size)
}
debug: private$step_helper(closure, loop_fun)
exiting from: optimizer$step()
[1] 0
Called from: eval_bare(loop, env)
torch_tensor
0.01 *
 0.7367  0.5055
 -0.6869  0.4205
  0.1194 -0.1136
  0.0000  0.0000
 -0.0919  0.2191
  0.2482  0.1123
  0.3390  0.2084
  0.0893 -0.3331
  0.3432  0.8153
 -0.2834  0.4606
 -0.2115 -0.4125
  0.0446  0.1280
 -0.0189  0.1163
 -0.1302 -1.0646
 -0.2541 -0.3337
 -0.4005  0.7775
 -0.1704 -0.3006
 -0.1798  0.1863
  0.1532  0.1090
  0.0797  0.4584
  0.0653 -0.3469
  0.0802  0.4061
  0.0000  0.0000
 -0.4289  0.9034
  0.0683  0.1620
  0.6000  0.7230
 -0.2771 -0.6479
  0.2307  0.2981
  0.2332  0.1625
... [the output was truncated (use n=-1 to disable)]
[ MPSFloatType{64,2} ]
debugging in: optimizer$step()
debug: {
    loop_fun &lt;- function(group, param, g, p) {
        grad &lt;- param$grad
        amsgrad &lt;- group$amsgrad
        if (length(state(param)) == 0) {
            state(param) &lt;- list()
            state(param)[["step"]] &lt;- 0
            state(param)[["exp_avg"]] &lt;- torch_zeros_like(param, 
                memory_format = torch_preserve_format())
            state(param)[["exp_avg_sq"]] &lt;- torch_zeros_like(param, 
                memory_format = torch_preserve_format())
            if (amsgrad) {
                state(param)[["max_exp_avg_sq"]] &lt;- torch_zeros_like(param, 
                  memory_format = torch_preserve_format())
            }
        }
        exp_avg &lt;- state(param)[["exp_avg"]]
        exp_avg_sq &lt;- state(param)[["exp_avg_sq"]]
        if (amsgrad) {
            max_exp_avg_sq &lt;- state(param)[["max_exp_avg_sq"]]
        }
        beta1 &lt;- group$betas[[1]]
        beta2 &lt;- group$betas[[2]]
        state(param)[["step"]] &lt;- state(param)[["step"]] + 1
        bias_correction1 &lt;- 1 - beta1^state(param)[["step"]]
        bias_correction2 &lt;- 1 - beta2^state(param)[["step"]]
        if (group$weight_decay != 0) {
            grad$add_(param, alpha = group$weight_decay)
        }
        exp_avg$mul_(beta1)$add_(grad, alpha = 1 - beta1)
        exp_avg_sq$mul_(beta2)$addcmul_(grad, grad, value = 1 - 
            beta2)
        if (amsgrad) {
            max_exp_avg_sq$set_data(max_exp_avg_sq$max(other = exp_avg_sq))
            denom &lt;- (max_exp_avg_sq$sqrt()/sqrt(bias_correction2))$add_(group$eps)
        }
        else {
            denom &lt;- (exp_avg_sq$sqrt()/sqrt(bias_correction2))$add_(group$eps)
        }
        step_size &lt;- group$lr/bias_correction1
        param$addcdiv_(exp_avg, denom, value = -step_size)
    }
    private$step_helper(closure, loop_fun)
}
debug: loop_fun &lt;- function(group, param, g, p) {
    grad &lt;- param$grad
    amsgrad &lt;- group$amsgrad
    if (length(state(param)) == 0) {
        state(param) &lt;- list()
        state(param)[["step"]] &lt;- 0
        state(param)[["exp_avg"]] &lt;- torch_zeros_like(param, 
            memory_format = torch_preserve_format())
        state(param)[["exp_avg_sq"]] &lt;- torch_zeros_like(param, 
            memory_format = torch_preserve_format())
        if (amsgrad) {
            state(param)[["max_exp_avg_sq"]] &lt;- torch_zeros_like(param, 
                memory_format = torch_preserve_format())
        }
    }
    exp_avg &lt;- state(param)[["exp_avg"]]
    exp_avg_sq &lt;- state(param)[["exp_avg_sq"]]
    if (amsgrad) {
        max_exp_avg_sq &lt;- state(param)[["max_exp_avg_sq"]]
    }
    beta1 &lt;- group$betas[[1]]
    beta2 &lt;- group$betas[[2]]
    state(param)[["step"]] &lt;- state(param)[["step"]] + 1
    bias_correction1 &lt;- 1 - beta1^state(param)[["step"]]
    bias_correction2 &lt;- 1 - beta2^state(param)[["step"]]
    if (group$weight_decay != 0) {
        grad$add_(param, alpha = group$weight_decay)
    }
    exp_avg$mul_(beta1)$add_(grad, alpha = 1 - beta1)
    exp_avg_sq$mul_(beta2)$addcmul_(grad, grad, value = 1 - beta2)
    if (amsgrad) {
        max_exp_avg_sq$set_data(max_exp_avg_sq$max(other = exp_avg_sq))
        denom &lt;- (max_exp_avg_sq$sqrt()/sqrt(bias_correction2))$add_(group$eps)
    }
    else {
        denom &lt;- (exp_avg_sq$sqrt()/sqrt(bias_correction2))$add_(group$eps)
    }
    step_size &lt;- group$lr/bias_correction1
    param$addcdiv_(exp_avg, denom, value = -step_size)
}
debug: private$step_helper(closure, loop_fun)
exiting from: optimizer$step()
[1] 0
Called from: eval_bare(loop, env)
torch_tensor
0.01 *
 0.7228  0.8878
 -0.8451  0.4993
  0.1571 -0.1824
  0.0000  0.0000
 -0.1176  0.3507
  0.3087  0.1430
  0.3844  0.2551
  0.1389 -0.4308
  0.3859  0.9863
 -0.3945  0.7429
 -0.1605 -0.5927
  0.0231  0.1537
 -0.0520  0.1372
 -0.0564 -1.5334
 -0.2958 -0.3684
 -0.5619  1.1233
 -0.1538 -0.4335
 -0.2671  0.2543
  0.1213  0.2550
  0.0560  0.5666
  0.0906 -0.4896
  0.0769  0.4837
  0.0000  0.0000
 -0.4994  1.2010
  0.0774  0.1867
  0.6260  1.0118
 -0.2090 -0.9458
  0.2682  0.3464
  0.2711  0.2107
... [the output was truncated (use n=-1 to disable)]
[ MPSFloatType{64,2} ]
debugging in: optimizer$step()
debug: {
    loop_fun &lt;- function(group, param, g, p) {
        grad &lt;- param$grad
        amsgrad &lt;- group$amsgrad
        if (length(state(param)) == 0) {
            state(param) &lt;- list()
            state(param)[["step"]] &lt;- 0
            state(param)[["exp_avg"]] &lt;- torch_zeros_like(param, 
                memory_format = torch_preserve_format())
            state(param)[["exp_avg_sq"]] &lt;- torch_zeros_like(param, 
                memory_format = torch_preserve_format())
            if (amsgrad) {
                state(param)[["max_exp_avg_sq"]] &lt;- torch_zeros_like(param, 
                  memory_format = torch_preserve_format())
            }
        }
        exp_avg &lt;- state(param)[["exp_avg"]]
        exp_avg_sq &lt;- state(param)[["exp_avg_sq"]]
        if (amsgrad) {
            max_exp_avg_sq &lt;- state(param)[["max_exp_avg_sq"]]
        }
        beta1 &lt;- group$betas[[1]]
        beta2 &lt;- group$betas[[2]]
        state(param)[["step"]] &lt;- state(param)[["step"]] + 1
        bias_correction1 &lt;- 1 - beta1^state(param)[["step"]]
        bias_correction2 &lt;- 1 - beta2^state(param)[["step"]]
        if (group$weight_decay != 0) {
            grad$add_(param, alpha = group$weight_decay)
        }
        exp_avg$mul_(beta1)$add_(grad, alpha = 1 - beta1)
        exp_avg_sq$mul_(beta2)$addcmul_(grad, grad, value = 1 - 
            beta2)
        if (amsgrad) {
            max_exp_avg_sq$set_data(max_exp_avg_sq$max(other = exp_avg_sq))
            denom &lt;- (max_exp_avg_sq$sqrt()/sqrt(bias_correction2))$add_(group$eps)
        }
        else {
            denom &lt;- (exp_avg_sq$sqrt()/sqrt(bias_correction2))$add_(group$eps)
        }
        step_size &lt;- group$lr/bias_correction1
        param$addcdiv_(exp_avg, denom, value = -step_size)
    }
    private$step_helper(closure, loop_fun)
}
debug: loop_fun &lt;- function(group, param, g, p) {
    grad &lt;- param$grad
    amsgrad &lt;- group$amsgrad
    if (length(state(param)) == 0) {
        state(param) &lt;- list()
        state(param)[["step"]] &lt;- 0
        state(param)[["exp_avg"]] &lt;- torch_zeros_like(param, 
            memory_format = torch_preserve_format())
        state(param)[["exp_avg_sq"]] &lt;- torch_zeros_like(param, 
            memory_format = torch_preserve_format())
        if (amsgrad) {
            state(param)[["max_exp_avg_sq"]] &lt;- torch_zeros_like(param, 
                memory_format = torch_preserve_format())
        }
    }
    exp_avg &lt;- state(param)[["exp_avg"]]
    exp_avg_sq &lt;- state(param)[["exp_avg_sq"]]
    if (amsgrad) {
        max_exp_avg_sq &lt;- state(param)[["max_exp_avg_sq"]]
    }
    beta1 &lt;- group$betas[[1]]
    beta2 &lt;- group$betas[[2]]
    state(param)[["step"]] &lt;- state(param)[["step"]] + 1
    bias_correction1 &lt;- 1 - beta1^state(param)[["step"]]
    bias_correction2 &lt;- 1 - beta2^state(param)[["step"]]
    if (group$weight_decay != 0) {
        grad$add_(param, alpha = group$weight_decay)
    }
    exp_avg$mul_(beta1)$add_(grad, alpha = 1 - beta1)
    exp_avg_sq$mul_(beta2)$addcmul_(grad, grad, value = 1 - beta2)
    if (amsgrad) {
        max_exp_avg_sq$set_data(max_exp_avg_sq$max(other = exp_avg_sq))
        denom &lt;- (max_exp_avg_sq$sqrt()/sqrt(bias_correction2))$add_(group$eps)
    }
    else {
        denom &lt;- (exp_avg_sq$sqrt()/sqrt(bias_correction2))$add_(group$eps)
    }
    step_size &lt;- group$lr/bias_correction1
    param$addcdiv_(exp_avg, denom, value = -step_size)
}
debug: private$step_helper(closure, loop_fun)
exiting from: optimizer$step()
[1] 0
Called from: eval_bare(loop, env)
torch_tensor
0.01 *
 1.0249  1.0966
 -0.9078  0.6011
  0.1142 -0.2097
  0.0000  0.0000
 -0.1299  0.3597
  0.2689  0.1004
  0.4925  0.2092
  0.2067 -0.5980
  0.6801  1.3135
 -0.1481  1.0710
 -0.2856 -0.7144
  0.1388  0.1723
  0.0631  0.1131
 -0.5003 -1.8495
 -0.1797 -0.4837
 -0.8430  1.1849
 -0.1447 -0.4633
 -0.2626  0.2389
  0.0761  0.2852
  0.0760  0.7190
  0.1178 -0.5108
  0.0966  0.5071
  0.0000  0.0000
 -0.3382  1.4659
  0.1477  0.2602
  0.6589  1.1771
 -0.5268 -1.1823
  0.3218  0.3584
  0.1675  0.2087
... [the output was truncated (use n=-1 to disable)]
[ MPSFloatType{64,2} ]
debugging in: optimizer$step()
debug: {
    loop_fun &lt;- function(group, param, g, p) {
        grad &lt;- param$grad
        amsgrad &lt;- group$amsgrad
        if (length(state(param)) == 0) {
            state(param) &lt;- list()
            state(param)[["step"]] &lt;- 0
            state(param)[["exp_avg"]] &lt;- torch_zeros_like(param, 
                memory_format = torch_preserve_format())
            state(param)[["exp_avg_sq"]] &lt;- torch_zeros_like(param, 
                memory_format = torch_preserve_format())
            if (amsgrad) {
                state(param)[["max_exp_avg_sq"]] &lt;- torch_zeros_like(param, 
                  memory_format = torch_preserve_format())
            }
        }
        exp_avg &lt;- state(param)[["exp_avg"]]
        exp_avg_sq &lt;- state(param)[["exp_avg_sq"]]
        if (amsgrad) {
            max_exp_avg_sq &lt;- state(param)[["max_exp_avg_sq"]]
        }
        beta1 &lt;- group$betas[[1]]
        beta2 &lt;- group$betas[[2]]
        state(param)[["step"]] &lt;- state(param)[["step"]] + 1
        bias_correction1 &lt;- 1 - beta1^state(param)[["step"]]
        bias_correction2 &lt;- 1 - beta2^state(param)[["step"]]
        if (group$weight_decay != 0) {
            grad$add_(param, alpha = group$weight_decay)
        }
        exp_avg$mul_(beta1)$add_(grad, alpha = 1 - beta1)
        exp_avg_sq$mul_(beta2)$addcmul_(grad, grad, value = 1 - 
            beta2)
        if (amsgrad) {
            max_exp_avg_sq$set_data(max_exp_avg_sq$max(other = exp_avg_sq))
            denom &lt;- (max_exp_avg_sq$sqrt()/sqrt(bias_correction2))$add_(group$eps)
        }
        else {
            denom &lt;- (exp_avg_sq$sqrt()/sqrt(bias_correction2))$add_(group$eps)
        }
        step_size &lt;- group$lr/bias_correction1
        param$addcdiv_(exp_avg, denom, value = -step_size)
    }
    private$step_helper(closure, loop_fun)
}
debug: loop_fun &lt;- function(group, param, g, p) {
    grad &lt;- param$grad
    amsgrad &lt;- group$amsgrad
    if (length(state(param)) == 0) {
        state(param) &lt;- list()
        state(param)[["step"]] &lt;- 0
        state(param)[["exp_avg"]] &lt;- torch_zeros_like(param, 
            memory_format = torch_preserve_format())
        state(param)[["exp_avg_sq"]] &lt;- torch_zeros_like(param, 
            memory_format = torch_preserve_format())
        if (amsgrad) {
            state(param)[["max_exp_avg_sq"]] &lt;- torch_zeros_like(param, 
                memory_format = torch_preserve_format())
        }
    }
    exp_avg &lt;- state(param)[["exp_avg"]]
    exp_avg_sq &lt;- state(param)[["exp_avg_sq"]]
    if (amsgrad) {
        max_exp_avg_sq &lt;- state(param)[["max_exp_avg_sq"]]
    }
    beta1 &lt;- group$betas[[1]]
    beta2 &lt;- group$betas[[2]]
    state(param)[["step"]] &lt;- state(param)[["step"]] + 1
    bias_correction1 &lt;- 1 - beta1^state(param)[["step"]]
    bias_correction2 &lt;- 1 - beta2^state(param)[["step"]]
    if (group$weight_decay != 0) {
        grad$add_(param, alpha = group$weight_decay)
    }
    exp_avg$mul_(beta1)$add_(grad, alpha = 1 - beta1)
    exp_avg_sq$mul_(beta2)$addcmul_(grad, grad, value = 1 - beta2)
    if (amsgrad) {
        max_exp_avg_sq$set_data(max_exp_avg_sq$max(other = exp_avg_sq))
        denom &lt;- (max_exp_avg_sq$sqrt()/sqrt(bias_correction2))$add_(group$eps)
    }
    else {
        denom &lt;- (exp_avg_sq$sqrt()/sqrt(bias_correction2))$add_(group$eps)
    }
    step_size &lt;- group$lr/bias_correction1
    param$addcdiv_(exp_avg, denom, value = -step_size)
}
debug: private$step_helper(closure, loop_fun)
exiting from: optimizer$step()
[1] 0
Called from: eval_bare(loop, env)
torch_tensor
0.01 *
 1.2359  1.1979
 -1.0042  0.6736
  0.1378 -0.2461
  0.0000  0.0000
 -0.1174  0.3951
  0.3383  0.1236
  0.5778  0.2426
  0.2780 -0.7661
  0.7482  1.4606
 -0.0739  1.1673
 -0.3545 -0.8116
  0.1435  0.1780
  0.0419  0.1369
 -0.6191 -2.0360
 -0.2141 -0.5633
 -0.7620  1.3090
 -0.3063 -0.5193
 -0.2956  0.2740
  0.1289  0.3270
  0.1190  0.7963
  0.1104 -0.5795
  0.1140  0.6017
  0.0000  0.0000
 -0.3564  1.7728
  0.1541  0.2951
  0.8675  1.3147
 -0.5209 -1.2984
  0.3917  0.3854
  0.2896  0.1952
... [the output was truncated (use n=-1 to disable)]
[ MPSFloatType{64,2} ]
debugging in: optimizer$step()
debug: {
    loop_fun &lt;- function(group, param, g, p) {
        grad &lt;- param$grad
        amsgrad &lt;- group$amsgrad
        if (length(state(param)) == 0) {
            state(param) &lt;- list()
            state(param)[["step"]] &lt;- 0
            state(param)[["exp_avg"]] &lt;- torch_zeros_like(param, 
                memory_format = torch_preserve_format())
            state(param)[["exp_avg_sq"]] &lt;- torch_zeros_like(param, 
                memory_format = torch_preserve_format())
            if (amsgrad) {
                state(param)[["max_exp_avg_sq"]] &lt;- torch_zeros_like(param, 
                  memory_format = torch_preserve_format())
            }
        }
        exp_avg &lt;- state(param)[["exp_avg"]]
        exp_avg_sq &lt;- state(param)[["exp_avg_sq"]]
        if (amsgrad) {
            max_exp_avg_sq &lt;- state(param)[["max_exp_avg_sq"]]
        }
        beta1 &lt;- group$betas[[1]]
        beta2 &lt;- group$betas[[2]]
        state(param)[["step"]] &lt;- state(param)[["step"]] + 1
        bias_correction1 &lt;- 1 - beta1^state(param)[["step"]]
        bias_correction2 &lt;- 1 - beta2^state(param)[["step"]]
        if (group$weight_decay != 0) {
            grad$add_(param, alpha = group$weight_decay)
        }
        exp_avg$mul_(beta1)$add_(grad, alpha = 1 - beta1)
        exp_avg_sq$mul_(beta2)$addcmul_(grad, grad, value = 1 - 
            beta2)
        if (amsgrad) {
            max_exp_avg_sq$set_data(max_exp_avg_sq$max(other = exp_avg_sq))
            denom &lt;- (max_exp_avg_sq$sqrt()/sqrt(bias_correction2))$add_(group$eps)
        }
        else {
            denom &lt;- (exp_avg_sq$sqrt()/sqrt(bias_correction2))$add_(group$eps)
        }
        step_size &lt;- group$lr/bias_correction1
        param$addcdiv_(exp_avg, denom, value = -step_size)
    }
    private$step_helper(closure, loop_fun)
}
debug: loop_fun &lt;- function(group, param, g, p) {
    grad &lt;- param$grad
    amsgrad &lt;- group$amsgrad
    if (length(state(param)) == 0) {
        state(param) &lt;- list()
        state(param)[["step"]] &lt;- 0
        state(param)[["exp_avg"]] &lt;- torch_zeros_like(param, 
            memory_format = torch_preserve_format())
        state(param)[["exp_avg_sq"]] &lt;- torch_zeros_like(param, 
            memory_format = torch_preserve_format())
        if (amsgrad) {
            state(param)[["max_exp_avg_sq"]] &lt;- torch_zeros_like(param, 
                memory_format = torch_preserve_format())
        }
    }
    exp_avg &lt;- state(param)[["exp_avg"]]
    exp_avg_sq &lt;- state(param)[["exp_avg_sq"]]
    if (amsgrad) {
        max_exp_avg_sq &lt;- state(param)[["max_exp_avg_sq"]]
    }
    beta1 &lt;- group$betas[[1]]
    beta2 &lt;- group$betas[[2]]
    state(param)[["step"]] &lt;- state(param)[["step"]] + 1
    bias_correction1 &lt;- 1 - beta1^state(param)[["step"]]
    bias_correction2 &lt;- 1 - beta2^state(param)[["step"]]
    if (group$weight_decay != 0) {
        grad$add_(param, alpha = group$weight_decay)
    }
    exp_avg$mul_(beta1)$add_(grad, alpha = 1 - beta1)
    exp_avg_sq$mul_(beta2)$addcmul_(grad, grad, value = 1 - beta2)
    if (amsgrad) {
        max_exp_avg_sq$set_data(max_exp_avg_sq$max(other = exp_avg_sq))
        denom &lt;- (max_exp_avg_sq$sqrt()/sqrt(bias_correction2))$add_(group$eps)
    }
    else {
        denom &lt;- (exp_avg_sq$sqrt()/sqrt(bias_correction2))$add_(group$eps)
    }
    step_size &lt;- group$lr/bias_correction1
    param$addcdiv_(exp_avg, denom, value = -step_size)
}
debug: private$step_helper(closure, loop_fun)
exiting from: optimizer$step()
[1] 0
Called from: eval_bare(loop, env)
torch_tensor
0.01 *
 1.5789  1.2355
 -1.2269  0.7779
  0.1534 -0.2345
  0.0000  0.0000
 -0.1334  0.4223
  0.5483  0.1755
  0.7929  0.2941
  0.2842 -0.8034
  0.7470  1.5445
 -0.0405  1.2257
 -0.5101 -0.9099
  0.1314  0.1989
  0.0524  0.1550
 -0.5138 -2.1640
 -0.3051 -0.6460
 -0.7483  1.3355
 -0.4743 -0.5339
 -0.3782  0.3030
  0.3192  0.3407
  0.2414  0.9023
  0.1292 -0.6315
  0.1150  0.6581
  0.0000  0.0000
 -0.5054  1.8473
  0.1541  0.2951
  1.2434  1.4602
 -0.4960 -1.4431
  0.5533  0.4722
  0.4520  0.2442
... [the output was truncated (use n=-1 to disable)]
[ MPSFloatType{64,2} ]
debugging in: optimizer$step()
debug: {
    loop_fun &lt;- function(group, param, g, p) {
        grad &lt;- param$grad
        amsgrad &lt;- group$amsgrad
        if (length(state(param)) == 0) {
            state(param) &lt;- list()
            state(param)[["step"]] &lt;- 0
            state(param)[["exp_avg"]] &lt;- torch_zeros_like(param, 
                memory_format = torch_preserve_format())
            state(param)[["exp_avg_sq"]] &lt;- torch_zeros_like(param, 
                memory_format = torch_preserve_format())
            if (amsgrad) {
                state(param)[["max_exp_avg_sq"]] &lt;- torch_zeros_like(param, 
                  memory_format = torch_preserve_format())
            }
        }
        exp_avg &lt;- state(param)[["exp_avg"]]
        exp_avg_sq &lt;- state(param)[["exp_avg_sq"]]
        if (amsgrad) {
            max_exp_avg_sq &lt;- state(param)[["max_exp_avg_sq"]]
        }
        beta1 &lt;- group$betas[[1]]
        beta2 &lt;- group$betas[[2]]
        state(param)[["step"]] &lt;- state(param)[["step"]] + 1
        bias_correction1 &lt;- 1 - beta1^state(param)[["step"]]
        bias_correction2 &lt;- 1 - beta2^state(param)[["step"]]
        if (group$weight_decay != 0) {
            grad$add_(param, alpha = group$weight_decay)
        }
        exp_avg$mul_(beta1)$add_(grad, alpha = 1 - beta1)
        exp_avg_sq$mul_(beta2)$addcmul_(grad, grad, value = 1 - 
            beta2)
        if (amsgrad) {
            max_exp_avg_sq$set_data(max_exp_avg_sq$max(other = exp_avg_sq))
            denom &lt;- (max_exp_avg_sq$sqrt()/sqrt(bias_correction2))$add_(group$eps)
        }
        else {
            denom &lt;- (exp_avg_sq$sqrt()/sqrt(bias_correction2))$add_(group$eps)
        }
        step_size &lt;- group$lr/bias_correction1
        param$addcdiv_(exp_avg, denom, value = -step_size)
    }
    private$step_helper(closure, loop_fun)
}
debug: loop_fun &lt;- function(group, param, g, p) {
    grad &lt;- param$grad
    amsgrad &lt;- group$amsgrad
    if (length(state(param)) == 0) {
        state(param) &lt;- list()
        state(param)[["step"]] &lt;- 0
        state(param)[["exp_avg"]] &lt;- torch_zeros_like(param, 
            memory_format = torch_preserve_format())
        state(param)[["exp_avg_sq"]] &lt;- torch_zeros_like(param, 
            memory_format = torch_preserve_format())
        if (amsgrad) {
            state(param)[["max_exp_avg_sq"]] &lt;- torch_zeros_like(param, 
                memory_format = torch_preserve_format())
        }
    }
    exp_avg &lt;- state(param)[["exp_avg"]]
    exp_avg_sq &lt;- state(param)[["exp_avg_sq"]]
    if (amsgrad) {
        max_exp_avg_sq &lt;- state(param)[["max_exp_avg_sq"]]
    }
    beta1 &lt;- group$betas[[1]]
    beta2 &lt;- group$betas[[2]]
    state(param)[["step"]] &lt;- state(param)[["step"]] + 1
    bias_correction1 &lt;- 1 - beta1^state(param)[["step"]]
    bias_correction2 &lt;- 1 - beta2^state(param)[["step"]]
    if (group$weight_decay != 0) {
        grad$add_(param, alpha = group$weight_decay)
    }
    exp_avg$mul_(beta1)$add_(grad, alpha = 1 - beta1)
    exp_avg_sq$mul_(beta2)$addcmul_(grad, grad, value = 1 - beta2)
    if (amsgrad) {
        max_exp_avg_sq$set_data(max_exp_avg_sq$max(other = exp_avg_sq))
        denom &lt;- (max_exp_avg_sq$sqrt()/sqrt(bias_correction2))$add_(group$eps)
    }
    else {
        denom &lt;- (exp_avg_sq$sqrt()/sqrt(bias_correction2))$add_(group$eps)
    }
    step_size &lt;- group$lr/bias_correction1
    param$addcdiv_(exp_avg, denom, value = -step_size)
}
debug: private$step_helper(closure, loop_fun)
exiting from: optimizer$step()
[1] 0
Called from: eval_bare(loop, env)
torch_tensor
0.01 *
 1.4522  1.7402
 -1.2900  0.8091
  0.1737 -0.2523
  0.0000  0.0000
 -0.1690  0.5618
  0.5000  0.2312
  0.7861  0.3060
  0.2725 -0.8664
  0.8899  1.7402
 -0.2922  1.5686
 -0.4524 -1.1516
  0.1701  0.2372
  0.0323  0.1581
 -0.5486 -2.6558
 -0.3058 -0.7177
 -0.9901  1.7241
 -0.3860 -0.6842
 -0.4072  0.3147
  0.1952  0.4717
  0.1464  1.0666
  0.1842 -0.7839
  0.1393  0.7531
  0.0000  0.0000
 -0.4374  1.9410
  0.1917  0.3339
  1.0768  1.8439
 -0.4925 -1.8044
  0.5827  0.5173
  0.4059  0.3062
... [the output was truncated (use n=-1 to disable)]
[ MPSFloatType{64,2} ]
debugging in: optimizer$step()
debug: {
    loop_fun &lt;- function(group, param, g, p) {
        grad &lt;- param$grad
        amsgrad &lt;- group$amsgrad
        if (length(state(param)) == 0) {
            state(param) &lt;- list()
            state(param)[["step"]] &lt;- 0
            state(param)[["exp_avg"]] &lt;- torch_zeros_like(param, 
                memory_format = torch_preserve_format())
            state(param)[["exp_avg_sq"]] &lt;- torch_zeros_like(param, 
                memory_format = torch_preserve_format())
            if (amsgrad) {
                state(param)[["max_exp_avg_sq"]] &lt;- torch_zeros_like(param, 
                  memory_format = torch_preserve_format())
            }
        }
        exp_avg &lt;- state(param)[["exp_avg"]]
        exp_avg_sq &lt;- state(param)[["exp_avg_sq"]]
        if (amsgrad) {
            max_exp_avg_sq &lt;- state(param)[["max_exp_avg_sq"]]
        }
        beta1 &lt;- group$betas[[1]]
        beta2 &lt;- group$betas[[2]]
        state(param)[["step"]] &lt;- state(param)[["step"]] + 1
        bias_correction1 &lt;- 1 - beta1^state(param)[["step"]]
        bias_correction2 &lt;- 1 - beta2^state(param)[["step"]]
        if (group$weight_decay != 0) {
            grad$add_(param, alpha = group$weight_decay)
        }
        exp_avg$mul_(beta1)$add_(grad, alpha = 1 - beta1)
        exp_avg_sq$mul_(beta2)$addcmul_(grad, grad, value = 1 - 
            beta2)
        if (amsgrad) {
            max_exp_avg_sq$set_data(max_exp_avg_sq$max(other = exp_avg_sq))
            denom &lt;- (max_exp_avg_sq$sqrt()/sqrt(bias_correction2))$add_(group$eps)
        }
        else {
            denom &lt;- (exp_avg_sq$sqrt()/sqrt(bias_correction2))$add_(group$eps)
        }
        step_size &lt;- group$lr/bias_correction1
        param$addcdiv_(exp_avg, denom, value = -step_size)
    }
    private$step_helper(closure, loop_fun)
}
debug: loop_fun &lt;- function(group, param, g, p) {
    grad &lt;- param$grad
    amsgrad &lt;- group$amsgrad
    if (length(state(param)) == 0) {
        state(param) &lt;- list()
        state(param)[["step"]] &lt;- 0
        state(param)[["exp_avg"]] &lt;- torch_zeros_like(param, 
            memory_format = torch_preserve_format())
        state(param)[["exp_avg_sq"]] &lt;- torch_zeros_like(param, 
            memory_format = torch_preserve_format())
        if (amsgrad) {
            state(param)[["max_exp_avg_sq"]] &lt;- torch_zeros_like(param, 
                memory_format = torch_preserve_format())
        }
    }
    exp_avg &lt;- state(param)[["exp_avg"]]
    exp_avg_sq &lt;- state(param)[["exp_avg_sq"]]
    if (amsgrad) {
        max_exp_avg_sq &lt;- state(param)[["max_exp_avg_sq"]]
    }
    beta1 &lt;- group$betas[[1]]
    beta2 &lt;- group$betas[[2]]
    state(param)[["step"]] &lt;- state(param)[["step"]] + 1
    bias_correction1 &lt;- 1 - beta1^state(param)[["step"]]
    bias_correction2 &lt;- 1 - beta2^state(param)[["step"]]
    if (group$weight_decay != 0) {
        grad$add_(param, alpha = group$weight_decay)
    }
    exp_avg$mul_(beta1)$add_(grad, alpha = 1 - beta1)
    exp_avg_sq$mul_(beta2)$addcmul_(grad, grad, value = 1 - beta2)
    if (amsgrad) {
        max_exp_avg_sq$set_data(max_exp_avg_sq$max(other = exp_avg_sq))
        denom &lt;- (max_exp_avg_sq$sqrt()/sqrt(bias_correction2))$add_(group$eps)
    }
    else {
        denom &lt;- (exp_avg_sq$sqrt()/sqrt(bias_correction2))$add_(group$eps)
    }
    step_size &lt;- group$lr/bias_correction1
    param$addcdiv_(exp_avg, denom, value = -step_size)
}
debug: private$step_helper(closure, loop_fun)
exiting from: optimizer$step()
[1] 0
Called from: eval_bare(loop, env)
torch_tensor
0.01 *
 1.3962  1.8206
 -1.4540  0.9316
  0.1900 -0.3377
  0.0000  0.0000
 -0.1964  0.6665
  0.4943  0.2170
  0.8010  0.3850
  0.3079 -0.9110
  0.9774  1.8744
 -0.4762  1.6913
 -0.3953 -1.2106
  0.1760  0.2672
  0.0037  0.1809
 -0.5066 -2.9850
 -0.2764 -0.7447
 -1.2279  1.9900
 -0.3104 -0.7795
 -0.3987  0.3913
  0.1358  0.5522
  0.1088  1.1367
  0.2127 -0.8618
  0.1406  0.8492
  0.0000  0.0000
 -0.5798  2.2823
  0.1936  0.3387
  1.0183  1.9908
 -0.5141 -1.9170
  0.5715  0.5624
  0.3847  0.3408
... [the output was truncated (use n=-1 to disable)]
[ MPSFloatType{64,2} ]
debugging in: optimizer$step()
debug: {
    loop_fun &lt;- function(group, param, g, p) {
        grad &lt;- param$grad
        amsgrad &lt;- group$amsgrad
        if (length(state(param)) == 0) {
            state(param) &lt;- list()
            state(param)[["step"]] &lt;- 0
            state(param)[["exp_avg"]] &lt;- torch_zeros_like(param, 
                memory_format = torch_preserve_format())
            state(param)[["exp_avg_sq"]] &lt;- torch_zeros_like(param, 
                memory_format = torch_preserve_format())
            if (amsgrad) {
                state(param)[["max_exp_avg_sq"]] &lt;- torch_zeros_like(param, 
                  memory_format = torch_preserve_format())
            }
        }
        exp_avg &lt;- state(param)[["exp_avg"]]
        exp_avg_sq &lt;- state(param)[["exp_avg_sq"]]
        if (amsgrad) {
            max_exp_avg_sq &lt;- state(param)[["max_exp_avg_sq"]]
        }
        beta1 &lt;- group$betas[[1]]
        beta2 &lt;- group$betas[[2]]
        state(param)[["step"]] &lt;- state(param)[["step"]] + 1
        bias_correction1 &lt;- 1 - beta1^state(param)[["step"]]
        bias_correction2 &lt;- 1 - beta2^state(param)[["step"]]
        if (group$weight_decay != 0) {
            grad$add_(param, alpha = group$weight_decay)
        }
        exp_avg$mul_(beta1)$add_(grad, alpha = 1 - beta1)
        exp_avg_sq$mul_(beta2)$addcmul_(grad, grad, value = 1 - 
            beta2)
        if (amsgrad) {
            max_exp_avg_sq$set_data(max_exp_avg_sq$max(other = exp_avg_sq))
            denom &lt;- (max_exp_avg_sq$sqrt()/sqrt(bias_correction2))$add_(group$eps)
        }
        else {
            denom &lt;- (exp_avg_sq$sqrt()/sqrt(bias_correction2))$add_(group$eps)
        }
        step_size &lt;- group$lr/bias_correction1
        param$addcdiv_(exp_avg, denom, value = -step_size)
    }
    private$step_helper(closure, loop_fun)
}
debug: loop_fun &lt;- function(group, param, g, p) {
    grad &lt;- param$grad
    amsgrad &lt;- group$amsgrad
    if (length(state(param)) == 0) {
        state(param) &lt;- list()
        state(param)[["step"]] &lt;- 0
        state(param)[["exp_avg"]] &lt;- torch_zeros_like(param, 
            memory_format = torch_preserve_format())
        state(param)[["exp_avg_sq"]] &lt;- torch_zeros_like(param, 
            memory_format = torch_preserve_format())
        if (amsgrad) {
            state(param)[["max_exp_avg_sq"]] &lt;- torch_zeros_like(param, 
                memory_format = torch_preserve_format())
        }
    }
    exp_avg &lt;- state(param)[["exp_avg"]]
    exp_avg_sq &lt;- state(param)[["exp_avg_sq"]]
    if (amsgrad) {
        max_exp_avg_sq &lt;- state(param)[["max_exp_avg_sq"]]
    }
    beta1 &lt;- group$betas[[1]]
    beta2 &lt;- group$betas[[2]]
    state(param)[["step"]] &lt;- state(param)[["step"]] + 1
    bias_correction1 &lt;- 1 - beta1^state(param)[["step"]]
    bias_correction2 &lt;- 1 - beta2^state(param)[["step"]]
    if (group$weight_decay != 0) {
        grad$add_(param, alpha = group$weight_decay)
    }
    exp_avg$mul_(beta1)$add_(grad, alpha = 1 - beta1)
    exp_avg_sq$mul_(beta2)$addcmul_(grad, grad, value = 1 - beta2)
    if (amsgrad) {
        max_exp_avg_sq$set_data(max_exp_avg_sq$max(other = exp_avg_sq))
        denom &lt;- (max_exp_avg_sq$sqrt()/sqrt(bias_correction2))$add_(group$eps)
    }
    else {
        denom &lt;- (exp_avg_sq$sqrt()/sqrt(bias_correction2))$add_(group$eps)
    }
    step_size &lt;- group$lr/bias_correction1
    param$addcdiv_(exp_avg, denom, value = -step_size)
}
debug: private$step_helper(closure, loop_fun)
exiting from: optimizer$step()
[1] 0
Called from: eval_bare(loop, env)
torch_tensor
0.01 *
 1.6362  1.7156
 -1.5766  0.9762
  0.2442 -0.3926
  0.0000  0.0000
 -0.2445  0.6066
  0.5236  0.2422
  0.8402  0.4441
  0.3150 -0.9635
  1.0477  2.1557
 -0.4820  1.6111
 -0.4765 -1.1737
  0.1735  0.3124
 -0.0140  0.2036
 -0.5598 -3.0680
 -0.4103 -0.8137
 -1.2799  2.0207
 -0.3425 -0.7684
 -0.4796  0.4287
  0.2011  0.5173
  0.2079  1.1767
  0.1828 -0.8804
  0.1575  0.9402
  0.0000  0.0000
 -0.7831  2.4580
  0.2183  0.3966
  1.2335  1.9544
 -0.6270 -1.9515
  0.5764  0.6217
  0.3977  0.3389
... [the output was truncated (use n=-1 to disable)]
[ MPSFloatType{64,2} ]
debugging in: optimizer$step()
debug: {
    loop_fun &lt;- function(group, param, g, p) {
        grad &lt;- param$grad
        amsgrad &lt;- group$amsgrad
        if (length(state(param)) == 0) {
            state(param) &lt;- list()
            state(param)[["step"]] &lt;- 0
            state(param)[["exp_avg"]] &lt;- torch_zeros_like(param, 
                memory_format = torch_preserve_format())
            state(param)[["exp_avg_sq"]] &lt;- torch_zeros_like(param, 
                memory_format = torch_preserve_format())
            if (amsgrad) {
                state(param)[["max_exp_avg_sq"]] &lt;- torch_zeros_like(param, 
                  memory_format = torch_preserve_format())
            }
        }
        exp_avg &lt;- state(param)[["exp_avg"]]
        exp_avg_sq &lt;- state(param)[["exp_avg_sq"]]
        if (amsgrad) {
            max_exp_avg_sq &lt;- state(param)[["max_exp_avg_sq"]]
        }
        beta1 &lt;- group$betas[[1]]
        beta2 &lt;- group$betas[[2]]
        state(param)[["step"]] &lt;- state(param)[["step"]] + 1
        bias_correction1 &lt;- 1 - beta1^state(param)[["step"]]
        bias_correction2 &lt;- 1 - beta2^state(param)[["step"]]
        if (group$weight_decay != 0) {
            grad$add_(param, alpha = group$weight_decay)
        }
        exp_avg$mul_(beta1)$add_(grad, alpha = 1 - beta1)
        exp_avg_sq$mul_(beta2)$addcmul_(grad, grad, value = 1 - 
            beta2)
        if (amsgrad) {
            max_exp_avg_sq$set_data(max_exp_avg_sq$max(other = exp_avg_sq))
            denom &lt;- (max_exp_avg_sq$sqrt()/sqrt(bias_correction2))$add_(group$eps)
        }
        else {
            denom &lt;- (exp_avg_sq$sqrt()/sqrt(bias_correction2))$add_(group$eps)
        }
        step_size &lt;- group$lr/bias_correction1
        param$addcdiv_(exp_avg, denom, value = -step_size)
    }
    private$step_helper(closure, loop_fun)
}
debug: loop_fun &lt;- function(group, param, g, p) {
    grad &lt;- param$grad
    amsgrad &lt;- group$amsgrad
    if (length(state(param)) == 0) {
        state(param) &lt;- list()
        state(param)[["step"]] &lt;- 0
        state(param)[["exp_avg"]] &lt;- torch_zeros_like(param, 
            memory_format = torch_preserve_format())
        state(param)[["exp_avg_sq"]] &lt;- torch_zeros_like(param, 
            memory_format = torch_preserve_format())
        if (amsgrad) {
            state(param)[["max_exp_avg_sq"]] &lt;- torch_zeros_like(param, 
                memory_format = torch_preserve_format())
        }
    }
    exp_avg &lt;- state(param)[["exp_avg"]]
    exp_avg_sq &lt;- state(param)[["exp_avg_sq"]]
    if (amsgrad) {
        max_exp_avg_sq &lt;- state(param)[["max_exp_avg_sq"]]
    }
    beta1 &lt;- group$betas[[1]]
    beta2 &lt;- group$betas[[2]]
    state(param)[["step"]] &lt;- state(param)[["step"]] + 1
    bias_correction1 &lt;- 1 - beta1^state(param)[["step"]]
    bias_correction2 &lt;- 1 - beta2^state(param)[["step"]]
    if (group$weight_decay != 0) {
        grad$add_(param, alpha = group$weight_decay)
    }
    exp_avg$mul_(beta1)$add_(grad, alpha = 1 - beta1)
    exp_avg_sq$mul_(beta2)$addcmul_(grad, grad, value = 1 - beta2)
    if (amsgrad) {
        max_exp_avg_sq$set_data(max_exp_avg_sq$max(other = exp_avg_sq))
        denom &lt;- (max_exp_avg_sq$sqrt()/sqrt(bias_correction2))$add_(group$eps)
    }
    else {
        denom &lt;- (exp_avg_sq$sqrt()/sqrt(bias_correction2))$add_(group$eps)
    }
    step_size &lt;- group$lr/bias_correction1
    param$addcdiv_(exp_avg, denom, value = -step_size)
}
debug: private$step_helper(closure, loop_fun)
exiting from: optimizer$step()
[1] 0
Called from: eval_bare(loop, env)
torch_tensor
0.01 *
 1.8948  1.8682
 -1.7295  1.1113
  0.2501 -0.3950
  0.0000  0.0000
 -0.2362  0.6716
  0.5686  0.2025
  0.8958  0.4425
  0.3740 -1.0596
  1.1291  2.2717
 -0.3882  1.7557
 -0.5040 -1.2657
  0.1817  0.3205
  0.0490  0.2408
 -0.7034 -3.3032
 -0.4323 -0.8663
 -1.4026  2.1537
 -0.3204 -0.8131
 -0.4967  0.4790
  0.1988  0.4930
  0.1764  1.2756
  0.1988 -0.9529
  0.1708  0.9682
  0.0000  0.0000
 -0.7607  2.7099
  0.2397  0.4313
  1.3290  2.0851
 -0.8361 -2.0914
  0.5815  0.6623
  0.4182  0.3956
... [the output was truncated (use n=-1 to disable)]
[ MPSFloatType{64,2} ]
debugging in: optimizer$step()
debug: {
    loop_fun &lt;- function(group, param, g, p) {
        grad &lt;- param$grad
        amsgrad &lt;- group$amsgrad
        if (length(state(param)) == 0) {
            state(param) &lt;- list()
            state(param)[["step"]] &lt;- 0
            state(param)[["exp_avg"]] &lt;- torch_zeros_like(param, 
                memory_format = torch_preserve_format())
            state(param)[["exp_avg_sq"]] &lt;- torch_zeros_like(param, 
                memory_format = torch_preserve_format())
            if (amsgrad) {
                state(param)[["max_exp_avg_sq"]] &lt;- torch_zeros_like(param, 
                  memory_format = torch_preserve_format())
            }
        }
        exp_avg &lt;- state(param)[["exp_avg"]]
        exp_avg_sq &lt;- state(param)[["exp_avg_sq"]]
        if (amsgrad) {
            max_exp_avg_sq &lt;- state(param)[["max_exp_avg_sq"]]
        }
        beta1 &lt;- group$betas[[1]]
        beta2 &lt;- group$betas[[2]]
        state(param)[["step"]] &lt;- state(param)[["step"]] + 1
        bias_correction1 &lt;- 1 - beta1^state(param)[["step"]]
        bias_correction2 &lt;- 1 - beta2^state(param)[["step"]]
        if (group$weight_decay != 0) {
            grad$add_(param, alpha = group$weight_decay)
        }
        exp_avg$mul_(beta1)$add_(grad, alpha = 1 - beta1)
        exp_avg_sq$mul_(beta2)$addcmul_(grad, grad, value = 1 - 
            beta2)
        if (amsgrad) {
            max_exp_avg_sq$set_data(max_exp_avg_sq$max(other = exp_avg_sq))
            denom &lt;- (max_exp_avg_sq$sqrt()/sqrt(bias_correction2))$add_(group$eps)
        }
        else {
            denom &lt;- (exp_avg_sq$sqrt()/sqrt(bias_correction2))$add_(group$eps)
        }
        step_size &lt;- group$lr/bias_correction1
        param$addcdiv_(exp_avg, denom, value = -step_size)
    }
    private$step_helper(closure, loop_fun)
}
debug: loop_fun &lt;- function(group, param, g, p) {
    grad &lt;- param$grad
    amsgrad &lt;- group$amsgrad
    if (length(state(param)) == 0) {
        state(param) &lt;- list()
        state(param)[["step"]] &lt;- 0
        state(param)[["exp_avg"]] &lt;- torch_zeros_like(param, 
            memory_format = torch_preserve_format())
        state(param)[["exp_avg_sq"]] &lt;- torch_zeros_like(param, 
            memory_format = torch_preserve_format())
        if (amsgrad) {
            state(param)[["max_exp_avg_sq"]] &lt;- torch_zeros_like(param, 
                memory_format = torch_preserve_format())
        }
    }
    exp_avg &lt;- state(param)[["exp_avg"]]
    exp_avg_sq &lt;- state(param)[["exp_avg_sq"]]
    if (amsgrad) {
        max_exp_avg_sq &lt;- state(param)[["max_exp_avg_sq"]]
    }
    beta1 &lt;- group$betas[[1]]
    beta2 &lt;- group$betas[[2]]
    state(param)[["step"]] &lt;- state(param)[["step"]] + 1
    bias_correction1 &lt;- 1 - beta1^state(param)[["step"]]
    bias_correction2 &lt;- 1 - beta2^state(param)[["step"]]
    if (group$weight_decay != 0) {
        grad$add_(param, alpha = group$weight_decay)
    }
    exp_avg$mul_(beta1)$add_(grad, alpha = 1 - beta1)
    exp_avg_sq$mul_(beta2)$addcmul_(grad, grad, value = 1 - beta2)
    if (amsgrad) {
        max_exp_avg_sq$set_data(max_exp_avg_sq$max(other = exp_avg_sq))
        denom &lt;- (max_exp_avg_sq$sqrt()/sqrt(bias_correction2))$add_(group$eps)
    }
    else {
        denom &lt;- (exp_avg_sq$sqrt()/sqrt(bias_correction2))$add_(group$eps)
    }
    step_size &lt;- group$lr/bias_correction1
    param$addcdiv_(exp_avg, denom, value = -step_size)
}
debug: private$step_helper(closure, loop_fun)
exiting from: optimizer$step()
[1] 0
Called from: eval_bare(loop, env)
torch_tensor
0.01 *
 1.6087  2.5419
 -1.8857  1.1695
  0.2923 -0.4107
  0.0000  0.0000
 -0.2940  0.8360
  0.5649  0.3237
  0.9283  0.4841
  0.3286 -1.1038
  1.1712  2.7114
 -0.7593  2.3166
 -0.3976 -1.5530
  0.2184  0.3612
  0.0124  0.2544
 -0.5604 -3.9385
 -0.4889 -0.9250
 -1.6929  2.8330
 -0.2238 -1.1779
 -0.6245  0.5207
  0.1481  0.7119
  0.0187  1.4680
  0.2647 -1.1644
  0.2016  1.0821
  0.0000  0.0000
 -0.9069  2.8027
  0.2520  0.5669
  1.1851  2.6657
 -0.7526 -2.4745
  0.6693  0.8445
  0.3965  0.4890
... [the output was truncated (use n=-1 to disable)]
[ MPSFloatType{64,2} ]
debugging in: optimizer$step()
debug: {
    loop_fun &lt;- function(group, param, g, p) {
        grad &lt;- param$grad
        amsgrad &lt;- group$amsgrad
        if (length(state(param)) == 0) {
            state(param) &lt;- list()
            state(param)[["step"]] &lt;- 0
            state(param)[["exp_avg"]] &lt;- torch_zeros_like(param, 
                memory_format = torch_preserve_format())
            state(param)[["exp_avg_sq"]] &lt;- torch_zeros_like(param, 
                memory_format = torch_preserve_format())
            if (amsgrad) {
                state(param)[["max_exp_avg_sq"]] &lt;- torch_zeros_like(param, 
                  memory_format = torch_preserve_format())
            }
        }
        exp_avg &lt;- state(param)[["exp_avg"]]
        exp_avg_sq &lt;- state(param)[["exp_avg_sq"]]
        if (amsgrad) {
            max_exp_avg_sq &lt;- state(param)[["max_exp_avg_sq"]]
        }
        beta1 &lt;- group$betas[[1]]
        beta2 &lt;- group$betas[[2]]
        state(param)[["step"]] &lt;- state(param)[["step"]] + 1
        bias_correction1 &lt;- 1 - beta1^state(param)[["step"]]
        bias_correction2 &lt;- 1 - beta2^state(param)[["step"]]
        if (group$weight_decay != 0) {
            grad$add_(param, alpha = group$weight_decay)
        }
        exp_avg$mul_(beta1)$add_(grad, alpha = 1 - beta1)
        exp_avg_sq$mul_(beta2)$addcmul_(grad, grad, value = 1 - 
            beta2)
        if (amsgrad) {
            max_exp_avg_sq$set_data(max_exp_avg_sq$max(other = exp_avg_sq))
            denom &lt;- (max_exp_avg_sq$sqrt()/sqrt(bias_correction2))$add_(group$eps)
        }
        else {
            denom &lt;- (exp_avg_sq$sqrt()/sqrt(bias_correction2))$add_(group$eps)
        }
        step_size &lt;- group$lr/bias_correction1
        param$addcdiv_(exp_avg, denom, value = -step_size)
    }
    private$step_helper(closure, loop_fun)
}
debug: loop_fun &lt;- function(group, param, g, p) {
    grad &lt;- param$grad
    amsgrad &lt;- group$amsgrad
    if (length(state(param)) == 0) {
        state(param) &lt;- list()
        state(param)[["step"]] &lt;- 0
        state(param)[["exp_avg"]] &lt;- torch_zeros_like(param, 
            memory_format = torch_preserve_format())
        state(param)[["exp_avg_sq"]] &lt;- torch_zeros_like(param, 
            memory_format = torch_preserve_format())
        if (amsgrad) {
            state(param)[["max_exp_avg_sq"]] &lt;- torch_zeros_like(param, 
                memory_format = torch_preserve_format())
        }
    }
    exp_avg &lt;- state(param)[["exp_avg"]]
    exp_avg_sq &lt;- state(param)[["exp_avg_sq"]]
    if (amsgrad) {
        max_exp_avg_sq &lt;- state(param)[["max_exp_avg_sq"]]
    }
    beta1 &lt;- group$betas[[1]]
    beta2 &lt;- group$betas[[2]]
    state(param)[["step"]] &lt;- state(param)[["step"]] + 1
    bias_correction1 &lt;- 1 - beta1^state(param)[["step"]]
    bias_correction2 &lt;- 1 - beta2^state(param)[["step"]]
    if (group$weight_decay != 0) {
        grad$add_(param, alpha = group$weight_decay)
    }
    exp_avg$mul_(beta1)$add_(grad, alpha = 1 - beta1)
    exp_avg_sq$mul_(beta2)$addcmul_(grad, grad, value = 1 - beta2)
    if (amsgrad) {
        max_exp_avg_sq$set_data(max_exp_avg_sq$max(other = exp_avg_sq))
        denom &lt;- (max_exp_avg_sq$sqrt()/sqrt(bias_correction2))$add_(group$eps)
    }
    else {
        denom &lt;- (exp_avg_sq$sqrt()/sqrt(bias_correction2))$add_(group$eps)
    }
    step_size &lt;- group$lr/bias_correction1
    param$addcdiv_(exp_avg, denom, value = -step_size)
}
debug: private$step_helper(closure, loop_fun)
exiting from: optimizer$step()
[1] 0</code></pre>
</div>
</div>
<p>The decision boundary plot shows how our neural network learned to separate the spiral classes, demonstrating its ability to learn non-linear patterns that would be impossible with a simple linear model.</p>
<div class="cell">
<div class="cell-output-display">
<p><img src="module-data-optim_files/figure-html/unnamed-chunk-23-1.png" class="img-fluid" width="672"></p>
</div>
</div>
<p>We can also visualize the predictions of our final network:</p>
<div class="cell">
<div class="cell-output-display">
<p><img src="module-data-optim_files/figure-html/unnamed-chunk-24-1.png" class="img-fluid" width="672"></p>
</div>
</div>
<p>This example demonstrates how torch’s high-level components work together to build and train neural networks:</p>
<ul>
<li><code>nn_module</code> manages our parameters and network architecture</li>
<li>The optimizer handles parameter updates</li>
<li>Dataset/dataloader classes efficiently feed data to our model</li>
<li>The training loop brings it all together</li>
</ul>
</section>

</main>
<!-- /main column -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->



</body></html>