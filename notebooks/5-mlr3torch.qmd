---
title: "Training Neural Networks with mlr3torch"
---

{{< include _setup.qmd >}}

## Why Use `mlr3torch`?

`mlr3torch` is a package that extends the `mlr3` framework with deep learning capabilities, allowing the application of deep learning techniques to both tabular and non-tabular data. The package implements many routines common in deep learning and allows users to focus on the actual problem at hand. Some advantages of using `mlr3torch` over 'only' `torch` are:

- **Less Code**: Avoid writing repetitive boilerplate code by utilizing predefined network architectures or easily building custom ones tailored to your specific needs.

- **mlr3 Integration**: Especially for users with experience in the `mlr3` framework, working with `mlr3torch` should feel familiar. Due to the integration into the `mlr3` framework, many `mlr3` features like hyperparameter tuning, preprocessing, and resampling are readily available for `mlr3torch`.

However, as `mlr3torch` is a framework, it is less flexible than `torch` itself, so knowing both is recommended. Another helpful R package that provides many useful functions to train neural networks is [`luz`](https://mlverse.github.io/luz/).

## `mlr3` Recap

Before diving into `mlr3torch`, we will briefly review the core building blocks of the `mlr3` machine learning framework. For reference, we recommend the [mlr3 book](https://mlr3book.mlr-org.com/) that explains the `mlr3` framework in more detail. Additionally, the [mlr3 website](https://mlr-org.com/) contains more tutorials and overviews.

### Task

A task is a machine learning problem on a dataset. It consists of the data itself and some metadata such as the features or the target variable. To create an example task that comes with `mlr3`, we can use the `tsk()` function:

```{r}
library(mlr3)
tsk("iris")
```

To create a custom `Task` from a `data.frame`, we can use the `as_task_<type>` converters:
```{r}
head(iris)
tsk_iris <- as_task_classif(iris, id = "iris", target = "Species")
tsk_iris
```

::: {.callout-tip}
To get the help page for an `mlr3` object, you can call `tsk_iris$help()`.
:::

You can access the data of a task using the `$data()` method, which accepts arguments `rows` and `cols` to select specific rows and columns.

```{r}
tsk_iris$data(rows = 1:5, cols = c("Sepal.Length", "Sepal.Width"))
```

Using the `mlr3viz` extension, we can get an overview of the task:

```{r}
library(mlr3viz)
autoplot(tsk_iris)
```

### Learner

A learner is a machine learning algorithm that can be `$train()`ed on a `Task` and `$predict()`ed on a `Task`. An overview of all learners is shown on our [website](https://mlr-org.com/learners.html). We can construct one by passing the name of the learner to the `lrn()` function.

```{r}
lrn_tree <- lrn("classif.rpart")
```

Next, we need to split the data into a training and test set and apply the learner on the former.

```{r}
split <- partition(tsk_iris, ratio = 0.8)
lrn_tree$train(tsk_iris, row_ids = split$train)
```

The trained model can be accessed via the `$model` slot of the learner:
```{r}
print(lrn_tree$model)
```

To make predictions on the test set, we can use the `$predict()` method of the learner:
```{r}
predictions <- lrn_tree$predict(tsk_iris, row_ids = split$test)
```

To make predictions on `data.frame`s, we can use the `$predict_newdata()` method of the learner:
```{r}
new_data <- iris[1:2, ]
lrn_tree$predict_newdata(new_data)
```

### Performance Evaluation

To assess the quality of the predictions, we can use a `Measure`. `mlr3` comes with many predefined measures, and we can construct them by passing the name of the measure to the `msr()` function. Below, we construct the mean classification accuracy measure -- which can only be applied to classification tasks -- and use it to evaluate the predictions.

```{r}
acc <- msr("classif.acc")
predictions$score(acc)
```

For more elaborate evaluation strategies, we can use `rsmp()` to define a `Resampling` strategy that can be executed using `resample()`.
```{r}
rsmp_cv <- rsmp("cv", folds = 3)

rr <- resample(
  task       = tsk_iris,
  learner    = lrn_tree,
  resampling = rsmp_cv
)

# Aggregate the results
rr$aggregate(msr("classif.acc"))
```

### Hyperparameter Tuning

Hyperparameter tuning is an essential process in machine learning to optimize the performance of models by selecting the best combination of hyperparameters. In the `mlr3` framework, hyperparameter tuning is facilitated by the `mlr3tuning` extension, which provides a flexible and powerful interface for defining, searching, and evaluating hyperparameters.

#### Key Concepts

- **Hyperparameters**: Configurable settings provided to the learning algorithm before training begins, such as learning rate, number of trees in a random forest, or regularization parameters.

- **Search Space**: The range of values or distributions from which hyperparameters are sampled during the tuning process.

- **Resampling Strategy**: A method to evaluate the performance of a hyperparameter configuration, commonly using techniques like cross-validation or bootstrapping.

- **Tuner**: An algorithm that explores the search space to find the optimal hyperparameters. Common tuners include grid search, random search, and Bayesian optimization.

#### Workflow

1. **Define the Search Space**: Specify the range and distribution of hyperparameters to explore.

   ```{r}
   library(mlr3tuning)
   lrn_tree$configure(
     cp = to_tune(lower = 0.001, upper = 0.1),
     maxdepth = to_tune(lower = 1, upper = 30)
   )
   ```

2. **Choose a Resampling Strategy**: Determine how to evaluate each hyperparameter configuration's performance.

   ```{r}
   rsmp_tune <- rsmp("cv", folds = 3)
   ```

3. **Select a Tuner**: Decide on the algorithm that will search through the hyperparameter space.

   ```{r}
   tuner <- tnr("random_search")
   ```

4. **Select a Measure**: Define the metric to optimize during tuning.

   ```{r}
   msr_tune <- msr("classif.acc")
   ```

5. **Execute Tuning**: Run the tuning process to find the optimal hyperparameters. Here we also specify our budget of 10 evaluations.

   ```{r}
   tune_result <- tune(
     task = tsk_iris,
     learner = lrn_tree,
     resampling = rsmp_tune,
     measure = msr_tune,
     tuner = tuner,
     term_evals = 10L
   )
   ```

6. **Apply the Best Hyperparameters**: Update the learner with the best-found hyperparameters and retrain the model.

   ```{r}
   lrn_tree$param_set$values <- tune_result$result_learner_param_vals
   lrn_tree$train(tsk_iris)
   ```

::: {.callout-note}
## Quiz: Tuning Performance

Question 1: Estimating the performance of a tuned model:

Through the tuning archive, we can access the performance of the best-found hyperparameter configuration.

```{r}
tune_result$archive$data[order(classif.acc, decreasing = TRUE), ][1, classif.acc]
```

Do you think this is a good estimate for the performance of the final model? Explain your answer.

<details>
<summary>Click for answer</summary>
One reason why we would expect the performance of the final model to be worse than the performance of the best-found hyperparameter configuration is due to *optimization bias*:
We choose the model configuration with the highest validation performance.
This selection process biases the result since the chosen model is the best among several trials.
To illustrate this, imagine that we take the maximum of 10 random numbers drawn from a normal distribution with mean 0.
The maximum over those numbers is larger than $0$, even though this is the mean of the generating distribution.

```{r, echo = FALSE}
library(ggplot2)
x <- rnorm(100)
ggplot(data.frame(x = x), aes(y = x)) +
  geom_boxplot() +
  geom_hline(yintercept = max(x), color = "red") +
  theme_minimal()
```

</details>
:::

These two steps can also be encapsulated in the `AutoTuner` class, which first finds the best hyperparameters and then trains the model with them.

```{r}
at <- auto_tuner(
  learner = lrn_tree,
  resampling = rsmp_tune,
  measure = msr_tune,
  term_evals = 10L,
  tuner = tuner
)
```

The `AutoTuner` can be used just like any other `Learner`.
To get a valid performance estimate of the tuning process, we can `resample()` it on the task.
This is called *nested resampling*: the outer resampling is for evaluation and the inner resampling is for tuning.

```{r}
rr <- resample(tsk_iris, at, rsmp_tune)
rr$aggregate()
```

### Learning Pipelines

In many cases, we don't only fit a single learner but a whole learning pipeline.
Common use cases include the preprocessing of the data, e.g., for imputing missing values, scaling the data, or encoding categorical features, but many other operations are possible.
The `mlr3` extension `mlr3pipelines` is a toolbox for defining such learning pipelines.
Its core building block is the `PipeOp` that can be constructed using the `po()` function.

```{r}
library(mlr3pipelines)
pca <- po("pca")
```

Just like a learner, it has a `$train()` and `$predict()` method, and we can apply it to a `Task` using these methods.

```{r}
pca$train(list(tsk_iris))
pca$predict(list(tsk_iris))[[1L]]
```

Usually, such `PipeOp`s are combined with a `Learner` into a full learning `Graph`.
This is possible using the `%>>%` chain operator.

```{r}
library(mlr3pipelines)
graph <- po("pca") %>>% lrn("classif.rpart")
print(graph)
graph$plot(horizontal = TRUE)
```

The resulting `Graph` can be converted back into a `Learner` using the `as_learner()` function and used just like any other `Learner`.

```{r}
glrn <- as_learner(graph)
glrn$train(tsk_iris)
```

## Brief Introduction to `mlr3torch`

`mlr3torch` builds upon the same components as `mlr3`, only that we use Deep `Learner`s, and can also work on non-tabular data.
A simple example learner is the `lrn("classif.mlp")` learner, which is a Multi-Layer Perceptron (MLP) for classification tasks.

### Using a Predefined Torch Learner

```{r, message = FALSE}
library(mlr3torch)
lrn_mlp <- lrn("classif.mlp",
  neurons = c(50, 50), # Two hidden layers with 50 neurons each
  batch_size = 256, # Number of samples per gradient update
  epochs = 30, # Number of training epochs
  device = "auto", # Uses GPU if available, otherwise CPU
  shuffle = TRUE, # because iris is sorted
  optimizer = t_opt("adam") # Adam optimizer
)
```

This multi-layer perceptron can be used just like the classification tree above.

```{r}
lrn_mlp$train(tsk_iris, row_ids = split$train)
```

The trained `nn_module` can be accessed via the `$model` slot of the learner:

```{r}
lrn_mlp$model$network
```

Besides the trained network, the `$model` of the learner also contains the `$state_dict()` of the optimizer and other information.

Having trained the neural network on the training set, we can now make predictions on the test set:

```{r}
predictions <- lrn_mlp$predict(tsk_iris, row_ids = split$test)
predictions$score(msr("classif.acc"))
```

Using the benchmarking facilities of `mlr3`, we can also easily compare the classification tree with our deep learning learner:

```{r}
# Define the resampling strategy
rsmp_cv <- rsmp("cv", folds = 3)

# Create a benchmark grid to compare both learners
benchmark_grid <- benchmark_grid(
  tasks = tsk_iris,
  learners = list(lrn_tree, lrn_mlp),
  resampling = rsmp_cv
)

# Run the benchmark
rr_benchmark <- benchmark(benchmark_grid)

# Aggregate the results
results_benchmark <- rr_benchmark$aggregate(msr("classif.acc"))

# Print the results
print(results_benchmark)
```

### Validation Performance

Tracking validation performance is crucial for understanding how well your neural network is learning and to detect issues such as overfitting.
In the `mlr3` machine learning framework, this can be easily done by specifying the `$validate` field of a `Learner`.
Note that this is not possible for all `Learner`s, but only for those with the `"validation"` property.
This includes boosting algorithms such as XGBoost or CatBoost, and also the `mlr3torch` learners.

Below, we set the validation ratio to 30% of the training data, specify the measures to record, and set the callbacks of the learner to record the history of the training process.

```{r}
lrn_mlp$configure(
  validate = 0.3,
  callbacks = t_clbk("history"),
  predict_type = "prob",
  measures_valid = msr("classif.logloss"),
  measures_train = msr("classif.logloss")
)
```

:::{.callout-tip}
The `$configure()` method of a `Learner` allows you to simultaneously set fields and hyperparameters of a learner.
:::

When we now train the learner, 30% of the training data is used for validation, and the loss is recorded in the history of the learner.

```{r}
lrn_mlp$train(tsk_iris)
```

After training, the results of the callback can be accessed via the model.

```{r}
head(lrn_mlp$model$callbacks$history)
```

Additionally, the final validation scores can be accessed via the `$internal_valid_scores` field of the learner.

```{r}
lrn_mlp$internal_valid_scores
```

### Defining a Custom Torch Learner

`mlr3torch` also allows defining custom architectures by assembling special `PipeOp`s in a `Graph`.
As a starting point in the graph, we need to mark the entry of the Neural Network using an ingress pipeop.
Because we are working with a task with only one numeric feature, we can use `po("torch_ingress_num")`.
There also exist inputs for categorical features (`po("torch_ingress_cat")`) and generic tensors (`po("torch_ingress_ltnsr")`).

```{r}
ingress <- po("torch_ingress_num")
```

The next steps in the graph are the actual layers of the neural network.

```{r}
architecture <- po("nn_linear_1", out_features = 100) %>>%
  po("nn_relu_1") %>>%
  po("nn_linear_2", out_features = 100) %>>%
  po("nn_relu_2") %>>%
  po("nn_head")

architecture$plot(horizontal = TRUE)
```

After specifying the architecture, we need to set the remaining parts for the learner, which are the loss, optimizer, and the remaining training configuration such as the epochs, device, or the batch size.

```{r}
graph <- ingress %>>% architecture %>>%
  po("torch_loss", loss = "cross_entropy") %>>%
  po("torch_optimizer", optimizer = t_opt("adam")) %>>%
  po("torch_model_classif", epochs = 10, batch_size = 256)
```

Just like before, we can convert the graph into a `Learner` using `as_learner()` and train it on the task:

```{r}
glrn <- as_learner(graph)
glrn$train(tsk_iris, row_ids = split$train)
```

### Working with Non-Tabular Data

In the `mlr3` ecosystem, the data of a task is always stored in a `data.frame` or `data.table`.
To work with non-tabular data, the `mlr3torch` package offers a custom datatype, the `lazy_tensor`, which can be stored in a `data.table`.

As an example to showcase this, we can use the CIFAR-10 dataset, which is a dataset of 60,000 32x32 color images in 10 classes, with 6,000 images per class.

```{r}
tsk_cifar <- tsk("cifar10")
tsk_cifar
```

The image below shows some examples from the dataset:

![](images/cifar10.jpg)

:::{.callout-tip}
To avoid having to re-download the dataset every time, you can set the `mlr3torch.cache` option to `TRUE`.
```{r, eval = FALSE}
options(mlr3torch.cache = TRUE)
```
:::

When accessing the data, only the images are represented as `lazy_tensor`s, the labels are still stored as a factor column:

```{r}
tsk_cifar$head()
```

A `lazy_tensor` column can be compared to the `torch::dataset` class that we have seen earlier.
This means it does not necessarily store the data in memory, but only stores the information on how to load the data.

```{r}
image_vec <- tsk_cifar$data(cols = "image")[[1L]]
head(image_vec)
```

To access the data as `torch_tensor`s, we can call the `materialize()` function:

```{r}
image_tensor <- materialize(image_vec)[[1L]]
```

To construct the CIFAR-10 task ourselves, we need to first:

1. Construct a `torch::dataset` that returns the images as `torch_tensor`s.
2. Create a `factor()` vector that contains the labels.

We can access the dataset via the `torchvision` package.
For simplicity, we will only load the training data.

```{r}
cifar10_original <- torchvision::cifar10_dataset(root = "data/cifar10", train = TRUE, download = TRUE)
image_array <- cifar10_original$x
str(image_array)
```

The array contains 50,000 images (rows) of shape `32x32x3`.
The last dimension contains the channels, i.e., the RGB values of the pixels.
We reshape this so the channel dimension is the first dimension, which is the standard format for images in `torch`.

```{r}
image_array <- aperm(image_array, c(1, 4, 2, 3))
dim(image_array)
```

Next, we create a `torch::dataset()` that loads individual images as a `torch_tensor`.
To convert this to a `lazy_tensor` later, the `$.getitem()` method needs to return a named list.

```{r}
dataset_cifar10 <- dataset("cifar10",
  initialize = function(x) {
    self$x <- x
  },
  .getitem = function(i) {
    list(image = torch_tensor(self$x[i, , , ]))
  },
  .length = function() {
    nrow(self$x)
  }
)
```

The above object is not yet a dataset, but a dataset constructor, so we create the actual dataset by calling it with the image array.

```{r}
cifar10 <- dataset_cifar10(image_array)
```

We can check that this works correctly by accessing the first image.

```{r}
str(cifar10$.getitem(1))
```

To convert this to a `lazy_tensor`, we can use the `as_lazy_tensor()` function.
The only thing we need to specify is the output shapes of the tensors, which we set to `c(NA, 3, 32, 32)`.
The `NA` is used to indicate that the first dimension (batch dimension) can be of any size.

```{r}
cifar10_lt <- as_lazy_tensor(cifar10, dataset_shapes = list(image = c(NA, 3, 32, 32)))
head(cifar10_lt)
```

:::{.callout-tip}
To check that transformations on images were applied correctly, it can be useful to inspect the images, e.g., using `torchvision::tensor_image_browse()`.
:::

Next, we create the `factor` vector that contains the labels.
For that, we use the data stored in the original `torchvision::cifar10_dataset()` object.

```{r}
labels <- factor(cifar10_original$y, labels = cifar10_original$classes[1:10])
str(labels)
```

Next, we create a `data.table` that contains the images and labels.

```{r}
cifar10_dt <- data.table(image = cifar10_lt, label = labels)
head(cifar10_dt)
```

This table can now be converted to an `mlr3::Task` using the `as_task_<type>` converters.

```{r}
tsk_cifar <- as_task_classif(cifar10_dt, id = "cifar10", target = "label")
tsk_cifar
```

We will now try to train a simple multi-layer perceptron -- the one we have defined above -- on the images.
One problem that we have is that the images are of shape `32x32x3`, but the `nn_linear` layer expects a flat input of size `3072` (32 * 32 * 3).

This is where the `lazy_tensor` datatype comes in handy.
We can use the `PipeOp`s to transform the data before it is loaded.
Here, the `-1` in the shape `c(-1, 3072)` indicates that the first dimension (batch dimension) can be of any size.

```{r}
reshaper <- po("trafo_reshape", shape = c(-1, 3072))
tsk_cifar_flat <- reshaper$train(list(tsk_cifar))[[1L]]
tsk_cifar_flat$head()
```

Note that this transformation is not applied eagerly, but only when the data is actually loaded.

:::{.callout-note}
In this case, as all the images are stored in memory, we could have also applied the transformation directly to the `array` representing the images, but decided not to do this for demonstration purposes.
:::

We can now use almost the same graph as before on the flattened task.
We only need to exchange the ingress, as the new task has as data a `lazy_tensor` instead of numeric vectors.

```{r}
graph <- po("torch_ingress_ltnsr") %>>% architecture %>>%
  po("torch_loss", loss = t_loss("cross_entropy")) %>>%
  po("torch_optimizer", optimizer = t_opt("adam")) %>>%
  po("torch_model_classif", epochs = 10, batch_size = 256)
glrn <- as_learner(graph)
```

```{r, include = FALSE}
glrn$configure(
  torch_model_classif.epochs = 0
)
```

```{r}
glrn$train(tsk_cifar_flat)
```

Alternatively, we can integrate the flattening step into the graph from which the `GraphLearner` was created.

```{r}
graph_with_flattening <- reshaper %>>% graph
glrn_with_flattening <- as_learner(graph_with_flattening)
```

This learner can now be applied directly to the (unflattened) task.

```{r}
glrn_with_flattening$param_set$set_values(torch_model_classif.epochs = 0)
```

```{r}
glrn_with_flattening$train(tsk_cifar)
```

### Saving an `mlr3torch` Learner

We have seen earlier that torch tensors cannot simply be saved using `saveRDS()`.
The same also applies to the `mlr3torch` learners.
To save an `mlr3torch` learner, we need to call the `$marshal()` method first.

```{r}
pth <- tempfile(fileext = ".rds")
glrn_with_flattening$marshal()
saveRDS(glrn_with_flattening, pth)
```

Afterward, we can load the learner again using `readRDS()` and the `$unmarshal()` method.

```{r}
glrn_with_flattening <- readRDS(pth)
glrn_with_flattening$unmarshal()
```

