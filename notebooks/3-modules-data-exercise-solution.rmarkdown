---
title: "Modules and Data"
solutions: true
---

---
title: "It's a Sin(us)"
solutions: false
---

```{r, include = FALSE}
lgr::get_logger("mlr3")$set_threshold("warn")
lgr::get_logger("bbotk")$set_threshold("warn")
set.seed(123)
torch::torch_manual_seed(123)
options(mlr3torch.cache = TRUE)
```




**Question 1**: Create a `torch::dataset` that takes in arguments `n`, `min`, and `max` where:

* `n` is the total number of samples
* `min` is the lower bound of the data
* `max` is the upper bound of the data

In the `initialize` method, generate and store:

* a 2D tensor `x` of `n` values drawn from a uniform distribution between `min` and `max`
* a 2D tensor `y` that is defined as $sin(x) + \epsilon$ where $\epsilon$ is drawn from a normal distribution with mean 0 and standard deviation 0.1

The `dataset` should return a named list with values `x` and `y`.

Then, create an instance of the dataset with `n = 1000`, `min = 0`, and `max = 10`.

Make sure that the dataset is working by either calling its `$.getitem()` or `$.getbatch()` method depending on what you implemented. Also, check that the shapes of both tensors returned by the dataset are `(n_batch, 1)`.

::: {.content-visible when-meta=solutions}
**Solution**


```{r}
library(torch)
sin_dataset <- dataset(
  initialize = function(n, min, max) {
    self$x <- torch_rand(n, 1) * (max - min) + min
    self$y <- torch_sin(self$x) + torch_randn(n, 1) * 0.1^1
  },
  .getbatch = function(i) {
    list(x = self$x[i, drop = FALSE], y = self$y[i, drop = FALSE])
  },
  .length = function() {
    length(self$x)
  }
)
ds <- sin_dataset(n = 1000, min = 0, max = 10)
batch <- ds$.getbatch(1:10)
batch$x$shape
batch$y$shape
```

:::

**Question 2**: Create a `torch::dataloader` that takes in the dataset and returns batches of size 10. Create one tensor `X` and one tensor `Y` that contains the concatenated batches of `x` and `y`.

<details>
<summary>Hint</summary>
The functions `coro::loop()` and `torch_cat()` might be helpful.
</details>

::: {.content-visible when-meta=solutions}
**Solution**


```{r}
dl <- dataloader(ds, batch_size = 10)
batches <- list()
coro::loop(for (batch in dl) {
  batches <- c(batches, list(batch))
})
X <- torch_cat(lapply(batches, function(batch) batch$x), dim = 1)
Y <- torch_cat(lapply(batches, function(batch) batch$y), dim = 1)
```

:::

**Question 3**: Create a custom torch module that allows modeling the sinus data we have created. To test it, apply it to the tensor `X` we have created above and calculate its mean squared error with the tensor `Y`.

<details>
<summary>Hint</summary>
You can either use `nn_module` to create a custom module generically, or you can use `nn_sequential()` to create a custom module that is a sequence of layers.
</details>

::: {.content-visible when-meta=solutions}
**Solution**


```{r}
nn_sin <- nn_module("nn_sin",
  initialize = function(latent = 200) {
    self$lin1 <- nn_linear(1, latent)
    self$lin2 <- nn_linear(latent, latent)
    self$lin3 <- nn_linear(latent, 1)
  },
  forward = function(x) {
    x |>
      self$lin1() |>
      nnf_relu() |>
      self$lin2() |>
      nnf_relu() |>
      self$lin3()
  }
)
net <- nn_sin(200)
Y_pred <- with_no_grad(net(X))
nnf_mse_loss(Y_pred, Y)
```

:::

**Question 4**: Train the model on the task for different hyperparameters (`lr` or `epochs`) and visualize the results. Play around with the hyperparameters until you get a good fit. You can use the following code for that:


```{r}
library(ggplot2)
predict_network <- function(net, dataloader) {
  local_no_grad()
  xs <- list(x = numeric(), y = numeric(), pred = numeric())
  i <- 1
  net$eval()
  coro::loop(for (batch in dataloader) {
    xs$x <- c(xs$x, as.numeric(batch$x))
    xs$y <- c(xs$y, as.numeric(batch$y))
    xs$pred <- c(xs$pred, as.numeric(net(batch$x)))
  })
  as.data.frame(xs)
}
train_network <- function(net, dataloader, epochs, lr) {
  optimizer <- optim_ignite_adamw(net$parameters, lr = lr)
  net$train()
  for (i in seq_len(epochs)) {
    coro::loop(for (batch in dataloader) {
      optimizer$zero_grad()
      Y_pred <- net(batch$x)
      loss <- nnf_mse_loss(Y_pred, batch$y)
      loss$backward()
      optimizer$step()
    })
  }
  predict_network(net, dataloader)
}
plot_results <- function(df) {
  ggplot(data = df, aes(x = x)) +
    geom_point(aes(y = y, color = "true")) +
    geom_point(aes(y = pred, color = "pred")) +
    theme_minimal()
}
train_and_plot <- function(net, dataloader, epochs = 10, lr = 0.01) {
  result <- train_network(net, dataloader, epochs = epochs, lr = lr)
  plot_results(result)
}
```


:::{.callout-tip}
Beware of the reference semantics and make sure that you create a new instance of the network for each run.
:::

::: {.content-visible when-meta=solutions}
**Solution**


```{r}
net <- nn_sin(200)
train_and_plot(net, dl, epochs = 200, lr = 0.01)
```

:::

**Question 5**: Create a new instance from the sinus dataset class created earlier.
Now, set the `min` and `max` values to `10` and `20` respectively and visualize the results. What do you observe? Can you explain why this is happening and can you fix the network architecture to make it work?

<details>
<summary>Hint</summary>
The sinus function has a phase of $2 \pi$.
</details>

::: {.content-visible when-meta=solutions}
**Solution**


```{r}
dl_ood <- dataloader(sin_dataset(n = 1000, min = 0, max = 20), batch_size = 10)
plot_results(predict_network(net, dl_ood))
```


For values out of the range `[0, 10]`, the network fails to generalize. This is because the network only observed values in the range `[0, 10]` during training.

We can fix this by preprocessing the data


```{r}
nn_sin2 <- nn_module("nn_sin2",
  initialize = function(latent = 200) {
    self$lin1 <- nn_linear(1, latent)
    self$lin2 <- nn_linear(latent, latent)
    self$lin3 <- nn_linear(latent, 1)
  },
  forward = function(x) {
    (x %% (2 * pi)) |>
      self$lin1() |>
      nnf_relu() |>
      self$lin2() |>
      nnf_relu() |>
      self$lin3()
  }
)
net2 <- nn_sin2(200)
df <- train_network(net2, dl, epochs = 200, lr = 0.01)
plot_results(predict_network(net2, dl_ood))
```

:::


