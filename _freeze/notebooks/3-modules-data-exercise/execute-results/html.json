{
  "hash": "fa21a10d89599f2a4399820d32a10561",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"It's a Sin(us)\"\nsolutions: false\n---\n\n\n\n\n\n\n\n**Question 1**: Create a `torch::dataset` that takes in arguments `n`, `min`, and `max` where:\n\n* `n` is the total number of samples\n* `min` is the lower bound of the data\n* `max` is the upper bound of the data\n\nIn the `initialize` method, generate and store:\n\n* a 2D tensor `x` of `n` values drawn from a uniform distribution between `min` and `max`\n* a 2D tensor `y` that is defined as $sin(x) + \\epsilon$ where $\\epsilon$ is drawn from a normal distribution with mean 0 and standard deviation 0.1\n\nThe `dataset` should return a named list with values `x` and `y`.\n\nThen, create an instance of the dataset with `n = 1000`, `min = 0`, and `max = 10`.\n\nMake sure that the dataset is working by either calling its `$.getitem()` or `$.getbatch()` method depending on what you implemented. Also, check that the shapes of both tensors returned by the dataset are `(n_batch, 1)`.\n\n::: {.content-visible when-meta=solutions}\n**Solution**\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(torch)\nsin_dataset <- dataset(\n  initialize = function(n, min, max) {\n    self$x <- torch_rand(n, 1) * (max - min) + min\n    self$y <- torch_sin(self$x) + torch_randn(n, 1) * 0.1^1\n  },\n  .getbatch = function(i) {\n    list(x = self$x[i, drop = FALSE], y = self$y[i, drop = FALSE])\n  },\n  .length = function() {\n    length(self$x)\n  }\n)\nds <- sin_dataset(n = 1000, min = 0, max = 10)\nbatch <- ds$.getbatch(1:10)\nbatch$x$shape\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 10  1\n```\n\n\n:::\n\n```{.r .cell-code}\nbatch$y$shape\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 10  1\n```\n\n\n:::\n:::\n\n\n\n:::\n\n**Question 2**: Create a `torch::dataloader` that takes in the dataset and returns batches of size 10. Create one tensor `X` and one tensor `Y` that contains the concatenated batches of `x` and `y`.\n\n<details>\n<summary>Hint</summary>\nThe functions `coro::loop()` and `torch_cat()` might be helpful.\n</details>\n\n::: {.content-visible when-meta=solutions}\n**Solution**\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ndl <- dataloader(ds, batch_size = 10)\nbatches <- list()\ncoro::loop(for (batch in dl) {\n  batches <- c(batches, list(batch))\n})\nX <- torch_cat(lapply(batches, function(batch) batch$x), dim = 1)\nY <- torch_cat(lapply(batches, function(batch) batch$y), dim = 1)\n```\n:::\n\n\n\n:::\n\n**Question 3**: Create a custom torch module that allows modeling the sinus data we have created. To test it, apply it to the tensor `X` we have created above and calculate its mean squared error with the tensor `Y`.\n\n<details>\n<summary>Hint</summary>\nYou can either use `nn_module` to create a custom module generically, or you can use `nn_sequential()` to create a custom module that is a sequence of layers.\n</details>\n\n::: {.content-visible when-meta=solutions}\n**Solution**\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nnn_sin <- nn_module(\"nn_sin\",\n  initialize = function(latent = 200) {\n    self$lin1 <- nn_linear(1, latent)\n    self$lin2 <- nn_linear(latent, latent)\n    self$lin3 <- nn_linear(latent, 1)\n  },\n  forward = function(x) {\n    x |>\n      self$lin1() |>\n      nnf_relu() |>\n      self$lin2() |>\n      nnf_relu() |>\n      self$lin3()\n  }\n)\nnet <- nn_sin(200)\nY_pred <- with_no_grad(net(X))\nnnf_mse_loss(Y_pred, Y)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\ntorch_tensor\n1.08382\n[ CPUFloatType{} ]\n```\n\n\n:::\n:::\n\n\n\n:::\n\n**Question 4**: Train the model on the task for different hyperparameters (`lr` or `epochs`) and visualize the results. Play around with the hyperparameters until you get a good fit. You can use the following code for that:\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(ggplot2)\npredict_network <- function(net, dataloader) {\n  local_no_grad()\n  xs <- list(x = numeric(), y = numeric(), pred = numeric())\n  i <- 1\n  net$eval()\n  coro::loop(for (batch in dataloader) {\n    xs$x <- c(xs$x, as.numeric(batch$x))\n    xs$y <- c(xs$y, as.numeric(batch$y))\n    xs$pred <- c(xs$pred, as.numeric(net(batch$x)))\n  })\n  as.data.frame(xs)\n}\ntrain_network <- function(net, dataloader, epochs, lr) {\n  optimizer <- optim_ignite_adamw(net$parameters, lr = lr)\n  net$train()\n  for (i in seq_len(epochs)) {\n    coro::loop(for (batch in dataloader) {\n      optimizer$zero_grad()\n      Y_pred <- net(batch$x)\n      loss <- nnf_mse_loss(Y_pred, batch$y)\n      loss$backward()\n      optimizer$step()\n    })\n  }\n  predict_network(net, dataloader)\n}\nplot_results <- function(df) {\n  ggplot(data = df, aes(x = x)) +\n    geom_point(aes(y = y, color = \"true\")) +\n    geom_point(aes(y = pred, color = \"pred\")) +\n    theme_minimal()\n}\ntrain_and_plot <- function(net, dataloader, epochs = 10, lr = 0.01) {\n  result <- train_network(net, dataloader, epochs = epochs, lr = lr)\n  plot_results(result)\n}\n```\n:::\n\n\n\n\n:::{.callout-tip}\nBeware of the reference semantics and make sure that you create a new instance of the network for each run.\n:::\n\n::: {.content-visible when-meta=solutions}\n**Solution**\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nnet <- nn_sin(200)\ntrain_and_plot(net, dl, epochs = 200, lr = 0.01)\n```\n\n::: {.cell-output-display}\n![](3-modules-data-exercise_files/figure-html/unnamed-chunk-6-1.png){fig-align='center' width=672}\n:::\n:::\n\n\n\n:::\n\n**Question 5**: Create a new instance from the sinus dataset class created earlier.\nNow, set the `min` and `max` values to `10` and `20` respectively and visualize the results. What do you observe? Can you explain why this is happening and can you fix the network architecture to make it work?\n\n<details>\n<summary>Hint</summary>\nThe sinus function has a phase of $2 \\pi$.\n</details>\n\n::: {.content-visible when-meta=solutions}\n**Solution**\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ndl_ood <- dataloader(sin_dataset(n = 1000, min = 0, max = 20), batch_size = 10)\nplot_results(predict_network(net, dl_ood))\n```\n\n::: {.cell-output-display}\n![](3-modules-data-exercise_files/figure-html/unnamed-chunk-7-1.png){fig-align='center' width=672}\n:::\n:::\n\n\n\n\nFor values out of the range `[0, 10]`, the network fails to generalize. This is because the network only observed values in the range `[0, 10]` during training.\n\nWe can fix this by preprocessing the data\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nnn_sin2 <- nn_module(\"nn_sin2\",\n  initialize = function(latent = 200) {\n    self$lin1 <- nn_linear(1, latent)\n    self$lin2 <- nn_linear(latent, latent)\n    self$lin3 <- nn_linear(latent, 1)\n  },\n  forward = function(x) {\n    (x %% (2 * pi)) |>\n      self$lin1() |>\n      nnf_relu() |>\n      self$lin2() |>\n      nnf_relu() |>\n      self$lin3()\n  }\n)\nnet2 <- nn_sin2(200)\ndf <- train_network(net2, dl, epochs = 200, lr = 0.01)\nplot_results(predict_network(net2, dl_ood))\n```\n\n::: {.cell-output-display}\n![](3-modules-data-exercise_files/figure-html/unnamed-chunk-8-1.png){fig-align='center' width=672}\n:::\n:::\n\n\n\n:::\n",
    "supporting": [
      "3-modules-data-exercise_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}