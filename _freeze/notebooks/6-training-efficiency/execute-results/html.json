{
  "hash": "9e10fc19f004b9f389fb22b417272fb8",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Training Efficiency\"\n---\n\n\n\n\n\n\n\nMethods for increasing training efficiency can be roughly split into:\n\n1. Computational methods such as JIT compilation, using GPU, parallel data loading, etc., that allow doing the same thing **faster**.\n2. Methodological approaches that change how we approach modeling to achieve either better results or faster training.\n\n# Computational Approaches\n\n## Parallel Processing\n\n### Graphical Processing Unit (GPU)\n\nUsing a GPU is crucial when training relatively large neural networks because GPUs are specifically designed to handle the parallel processing required for complex computations.\nTo use a GPU in mlr3torch, we can set the device parameter to \"cuda\". By default, it is set to \"auto\", which will use a GPU if it is available and otherwise fall back to the CPU.\n\n:::{.callout-tip}\nTo check if a GPU is available, we can use the `torch::cuda_is_available()` function.\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(torch)\ncuda_is_available()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] FALSE\n```\n\n\n:::\n:::\n\n\n\n\nIf you have an M1 Mac (or later), you can also use the available graphics card by setting the `device` parameter to `\"mps\"`.\nYou can check this by running:\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nbackends_mps_is_available()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] TRUE\n```\n\n\n:::\n:::\n\n\n\n:::\n\nTo demonstrate the speed improvements obtained by using a GPU, we conduct a large matrix operation on a GPU and a CPU.\nWe start by randomly sampling a matrix of size 1000x1000.\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nx_cpu = torch_randn(1000, 1000, device = \"cpu\")\n```\n:::\n\n\n\n\nBelow, we perform a matrix multiplication on the CPU and the GPU and compare the timings.\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# this will only run if a GPU is available\nx_cuda = x_cpu$cuda()\n\nbench::mark(\n  cpu = x_cpu$matmul(x_cpu),\n  cuda = x_cuda$matmul(x_cuda)\n)\n```\n:::\n\n\n\n\n### CPU Threads\n\nTraining large networks on a CPU is not a recommended approach, but it can be useful for smaller networks or when you don't have a GPU.\nYou can still use multiple threads to speed up the execution of operations.\nNote that the code below will not run on macOS, as it is not possible to set the number of threads on macOS.\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# this will be skipped on macOS\nbench::mark(\n  {torch_set_num_threads(1L); x_cpu$matmul(x_cpu)},\n  {torch_set_num_threads(16L); x_cpu$matmul(x_cpu)}\n)\n```\n:::\n\n\n\n\n`torch` also allows for interop-parallelization, but this is more advanced and code needs to be written in a specific way.\n\n:::{.callout-note}\n## Quiz: Number of Threads\n\nQuestion 1: On a CPU with 4 cores, does it make sense to set the number of threads to values greater than 4? Explain your answer.\n\n<details>\n<summary>Click for answer</summary>\nOn a CPU with 4 cores, at most 4 threads can run in parallel.\nUsing more threads than the number of cores will not speed up the execution of operations.\n</details>\n\nQuestion 2: On a CPU with 64 cores, is it always the case that using 64 threads is better than using 32 threads?\n\n<details>\n<summary>Click for answer</summary>\nNot necessarily. Using more threads will mean that:\n\n1. The threads need to communicate and synchronize, which increases the runtime.\n2. More resources are used for the computation, which decreases the runtime.\n\nThe optimal number of threads is a trade-off between these two effects.\n</details>\n:::\n\n## Efficient Data Loading\n\nBesides speeding up the computation of operations in the forward and backward pass, another possible bottleneck is the loading of data.\nThere are various ways to improve data loading speed:\n\n1. Improve the implementation of the `dataset` class\n2. Parallelize the data loading process\n3. Move data to the GPU\n\nThese approaches will now be discussed.\n\n### Efficient Dataset Implementation\n\nWhen implementing a dataset, we need to define:\n\n1. How we store and load the data\n2. Whether implementing loading of a batch is beneficial\n\n:::{.callout-note}\n## Quiz: Data Loading\n\nThe *tiny imagenet* dataset is a dataset of 100,000 images of size 64x64x3.\nIt is a subset of the famous *imagenet* dataset.\nBelow, we show some examples from the dataset:\n\n![](../assets/tiny-imagenet.png)\n\n\n\n\n\n\n\n\n\nWe will now consider different ways to write a `torch::dataset` implementation for this data.\nAssume we have some image paths stored in a character vector as well as in an array where they are already loaded into memory.\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nstr(image_paths)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n chr [1:100] \"/Users/sebi/Library/Caches/org.R-project.R/R/mlr3torch/datasets/tiny_imagenet/raw/tiny-imagenet-200/train/n0144\"| __truncated__ ...\n```\n\n\n:::\n\n```{.r .cell-code}\nstr(image_array)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n num [1:100, 1:3, 1:64, 1:64] 1 0.0784 0.4706 0.5608 0.5647 ...\n```\n\n\n:::\n:::\n\n\n\n\nAn individual image can, for example, be loaded using the `torchvision::base_loader()` function:\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(torchvision)\nstr(base_loader(image_paths[1]))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n num [1:64, 1:64, 1:3] 1 1 1 1 1 ...\n```\n\n\n:::\n:::\n\n\n\n\n**Question 1:** Reading From Disk or RAM\n\nWhich of the following is the faster way to load the images? Explain why.\n\n1. Loading the images from disk:\n\n\n\n\n   ::: {.cell layout-align=\"center\"}\n   \n   ```{.r .cell-code}\n   ds_disk = dataset(\"image_paths\",\n     initialize = function(image_paths) {\n       self$image_paths = image_paths\n     },\n     .getitem = function(i) {\n       torch_tensor(torchvision::base_loader(self$image_paths[i]))\n     },\n     .length = function() {\n       length(self$image_paths)\n     }\n   )(image_paths)\n   ```\n   :::\n\n\n\n\n2. Loading the images from an array:\n\n\n\n\n   ::: {.cell layout-align=\"center\"}\n   \n   ```{.r .cell-code}\n   ds_ram = dataset(\"image_array\",\n     initialize = function(image_array) {\n       self$image_array = image_array\n     },\n     .getbatch = function(i) {\n       torch_tensor(self$image_array[i, , , ])\n     },\n     .length = function() {\n       nrow(self$image_array)\n     }\n   )(image_array)\n   ```\n   :::\n\n\n\n\n<details>\n<summary>Click for answer</summary>\n\nGenerally, loading images from RAM is significantly faster than loading them from disk.\nAlthough the benchmark presented below may seem somewhat 'unfair' since `ds_ram` has already loaded the images into memory, this difference is evident in practice.\nWhen iterating over the dataset for multiple epochs, the first method will need to reload the images from disk for each epoch, while the second method only requires a single loading of the images into memory.\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\niter = function(ds, ..., epochs = 1) {\n  dl = torch::dataloader(ds, batch_size = 16, ...)\n  for (epoch in seq_len(epochs)) {\n    coro::loop(for(batch in dl) {\n      batch\n    })\n  }\n}\nbench::mark(\n  disk = iter(ds_disk),\n  ram = iter(ds_ram),\n  check = FALSE\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 × 6\n  expression      min   median `itr/sec` mem_alloc `gc/sec`\n  <bch:expr> <bch:tm> <bch:tm>     <dbl> <bch:byt>    <dbl>\n1 disk        21.39ms   24.6ms      38.6      14MB     11.0\n2 ram          9.09ms   10.4ms      77.5     9.4MB     20.2\n```\n\n\n:::\n:::\n\n\n\n\n</details>\n\n**Question 2:** (Don't) Copy that\n\nConsider now the next dataset implementation:\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nds_tensor = dataset(\"tensor\",\n  initialize = function(image_array) {\n    self$tensor = torch_tensor(image_array)\n  },\n  .getitem = function(i) {\n    self$tensor[i, ..]\n  },\n  .length = function() {\n    nrow(self$tensor)\n  }\n)(image_array)\n```\n:::\n\n\n\n\nDo you think this implementation is faster or slower than the `ds_ram` implementation? Explain why.\n\n<details>\n<summary>Click for answer</summary>\nThis implementation is faster than the `ds_ram` implementation.\nThis is because the `ds_tensor` implementation copies the R array to a torch tensor only once, whereas the `ds_ram` implementation copies the R array to a torch tensor for each item.\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nbench::mark(\n  tensor = iter(ds_tensor),\n  array = iter(ds_ram),\n  check = FALSE\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 × 6\n  expression      min   median `itr/sec` mem_alloc `gc/sec`\n  <bch:expr> <bch:tm> <bch:tm>     <dbl> <bch:byt>    <dbl>\n1 tensor       4.99ms   5.72ms     150.    95.67KB     4.42\n2 array        9.36ms  11.24ms      72.1    9.38MB    18.8 \n```\n\n\n:::\n:::\n\n\n\n\n</details>\n\n**Question 3**: `$.getbatch()` vs `$.getitem()`\n\nWhich implementation is faster? Explain why.\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nds_tensor_batch = dataset(\"tensor_batch\",\n  initialize = function(image_array) {\n    self$tensor = torch_tensor(image_array)\n  },\n  .getbatch = function(i) {\n    self$tensor[i, ..]\n  },\n  .length = function() {\n    nrow(self$tensor)\n  }\n)(image_array)\n```\n:::\n\n\n\n\n<details>\n<summary>Click for answer</summary>\nThe `$.getbatch()` implementation is faster than the `$.getitem()` implementation.\nThis is because when using the `$.getitem()` method, the batch for indices `ids` is obtained by calling `$.getitem(id)` for each index in `ids` and then stacking them together, which requires a new tensor allocation.\nSlicing the tensor, however, avoids this allocation when `shuffle = TRUE` (which is also the default).\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nbench::mark(\n  getbatch = iter(ds_tensor_batch),\n  getitem = iter(ds_tensor),\n  check = FALSE\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 × 6\n  expression      min   median `itr/sec` mem_alloc `gc/sec`\n  <bch:expr> <bch:tm> <bch:tm>     <dbl> <bch:byt>    <dbl>\n1 getbatch     1.87ms   2.62ms      298.    3.83KB     2.24\n2 getitem      4.73ms   5.12ms      192.   54.69KB     7.29\n```\n\n\n:::\n:::\n\n\n\n</details>\n:::\n\n### Parallel Data Loading\n\nIn Deep Learning, datasets can be very large, and it might therefore be the case that the data is simply too large to fit into memory.\nIn this case, we can use parallel data loading to speed up the data loading process.\nInstead of loading the data sequentially in the main process, other R processes will be started that execute the data loading.\nFor example, if we set `num_workers = 4L`, 4 R processes will be started that load the data, while the main process is free to train the model.\nThese processes then send the batches to the main process.\nThe image below visualizes this process:\n\n![](../assets/parallel-dataloader.png)\n\nCreating such a parallel dataloader is as easy as setting the `num_workers` parameter to a value greater than 0.\n\n:::{.callout-note}\nNote that there is some communication overhead that results from sending the batches from the worker to the main process.\nThis will hopefully be reduced in the future, but is currently there.\nFor this reason, parallel data loading is therefore -- currently -- only beneficial when it is slow, e.g., because of loading the data from disk or because of expensive preprocessing.\n:::\n\n### Moving Data to the GPU\n\nOne thing we have ignored so far is that when training using a GPU, the data needs to be moved to the GPU.\nThis is because a GPU has its own memory (VRAM), and the data needs to be moved to this memory before it can be used for training.\nThe moving of the data to the GPU cannot be done on the processes that are loading the data but must be done in the main process, i.e., after the batch was received from (possibly parallelized) dataloader.\nOne way to speed up the data loading process is to pin the memory of the data to the GPU.\nBefore a tensor can be moved from RAM to VRAM, it needs to be in so-called page-locked memory, which can be done using the `pin_memory` parameter.\n\n![](../assets/pinned-memory.png)\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\niter_cuda = function(ds, pin_memory = TRUE) {\n  dl = torch::dataloader(ds, batch_size = 16, pin_memory = pin_memory)\n  coro::loop(for(batch in dl) {\n    batch$cuda()\n  })\n}\n\nbench::mark(\n  not_pinned = iter_cuda(ds_disk, pin_memory = FALSE),\n  pinned = iter_cuda(ds_disk, pin_memory = TRUE)\n)\n```\n:::\n\n\n\n\n:::{.callout-note}\n\nIn order to use parallel data loading or memory pinning with `mlr3torch`, these parameters can simply be specified in the learner:\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlrn(\"classif.mlp\", num_workers = 8L, pin_memory = TRUE, device = \"cuda\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n<LearnerTorchMLP[classif]:classif.mlp>: My Little Powny\n* Model: -\n* Parameters: device=cuda, num_threads=1, num_interop_threads=1, seed=random, jit_trace=FALSE, eval_freq=1,\n  measures_train=<list>, measures_valid=<list>, patience=0, min_delta=0, num_workers=8, pin_memory=TRUE,\n  neurons=integer(0), p=0.5, activation=<nn_relu>, activation_args=<list>\n* Validate: NULL\n* Packages: mlr3, mlr3torch, torch\n* Predict Types:  [response], prob\n* Feature Types: integer, numeric, lazy_tensor\n* Properties: internal_tuning, marshal, multiclass, twoclass, validation\n* Optimizer: adam\n* Loss: cross_entropy\n* Callbacks: -\n```\n\n\n:::\n:::\n\n\n\n:::\n\n## JIT Compilation & Ignite Optimizers\n\nSome special care needs to be taken when using `torch` (or `mlr3torch`) in order to get good performance.\nIn the future, this will hopefully not be necessary anymore, but is currently required.\n\n### 'Ignite' Optimizers\n\nIn `torch`, different versions of optimizers exist:\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\noptim_adamw\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n<optim_adamw> object generator\n  Inherits from: <inherit>\n  Public:\n    initialize: function (params, lr = 0.001, betas = c(0.9, 0.999), eps = 1e-08, \n    loop_fun: function (group, param, g, p) \n    step: function (closure = NULL) \n    clone: function (deep = FALSE) \n  Parent env: <environment: 0x1518c9580>\n  Locked objects: FALSE\n  Locked class: FALSE\n  Portable: TRUE\n```\n\n\n:::\n\n```{.r .cell-code}\noptim_ignite_adamw\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n<optim_ignite_adamw> object generator\n<optim_ignite> object generator\n  Inherits from: <inherit>\n  Public:\n    initialize: function (params, lr = 0.001, betas = c(0.9, 0.999), eps = 1e-08, \n    state_dict: function () \n    clone: function (deep = FALSE) \n  Private:\n    .optim: function (params, ...) \n    .step: function (opt) \n    .set_states: function (opt, params, states) \n    .add_param_group: function (opt, params, lr, betas, eps, weight_decay, amsgrad) \n    .assert_params: function (lr, betas, eps, weight_decay, amsgrad) \n    .set_param_group_options: function (opt, list) \n    .zero_grad: function (opt) \n    .get_param_groups: function (ptr) \n  Parent env: <environment: 0x151e3dae0>\n  Locked objects: FALSE\n  Locked class: FALSE\n  Portable: TRUE\n```\n\n\n:::\n:::\n\n\n\n\nThe 'ignite' indicates that the optimizer is a version that is optimized for performance.\nNot for all optimizers does an ignite version exist, but for the most common ones, it does.\n\nBelow, we compare the performance of the default optimizer and the ignite optimizer and see that the latter is considerably faster.\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nadamw = as_torch_optimizer(torch::optim_adamw)\nignite_adamw = as_torch_optimizer(torch::optim_ignite_adamw)\n\nlearner = lrn(\"classif.mlp\", epochs = 10, neurons = c(100, 100), batch_size = 32, optimizer = adamw)\n\nlearner_ignite = learner$clone(deep = TRUE)\nlearner_ignite$configure(\n  optimizer = ignite_adamw\n)\ntask_sonar = tsk(\"sonar\")\n\nbench::mark(\n  learner$train(task_sonar),\n  learner_ignite$train(task_sonar),\n  check = FALSE\n)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: Some expressions had a GC in every iteration; so filtering is disabled.\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 × 6\n  expression                            min   median `itr/sec` mem_alloc `gc/sec`\n  <bch:expr>                       <bch:tm> <bch:tm>     <dbl> <bch:byt>    <dbl>\n1 learner$train(task_sonar)           589ms    589ms      1.70    15.6MB     5.09\n2 learner_ignite$train(task_sonar)    238ms    259ms      3.86    10.8MB     5.79\n```\n\n\n:::\n:::\n\n\n\n\n### JIT Compilation\n\nJIT (Just-In-Time) compilation is a runtime optimization technique that compiles code into machine code during execution rather than beforehand.\nThis has different advantages:\n\n1. By JIT-compiling a model, some operations can be optimized for performance.\n2. A JIT-compiled model can be saved and executed without an R dependency for deployment (only LibTorch is required), e.g., in a C++ application.\n3. Running a JIT-compiled model in R is faster because the whole network is executed in C++ instead of R.\n\nIn `torch`, this can either be done using TorchScript or by tracing a model.\nWe will briefly discuss both approaches, but for more information, see the [torch documentation](https://torch.mlverse.org/docs/articles/torchscript).\n\n#### TorchScript\n\nTorchScript is a subset of Python -- i.e., its own programming language -- that can be used to define compiled functions.\nIn R, this is available via the [`jit_compile`](https://torch.mlverse.org/docs/reference/jit_compile.html) function.\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nf = jit_compile(\"\ndef f(x, w, bias):\n  return x @ w + bias\n\")$f\n\nx = torch_randn(10, 10)\nw = torch_randn(10, 1)\nbias = torch_randn(1)\n\nout = f(x, w, bias)\nstr(out)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nFloat [1:10, 1:1]\n```\n\n\n:::\n:::\n\n\n\n\nBesides syntax, there are some important differences between TorchScript and R:\n\n1. In TorchScript, indexing tensors is 0-based, and\n2. TorchScript is statically typed, so you need to specify the types of the arguments, unless they are tensors, which is the default.\n\nBelow, we define a function that takes a list of tensors and calculates their sum.\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nsum_jit = jit_compile(\"\ndef sum_jit(xs: List[Tensor]):\n  output = torch.zeros_like(xs[0])\n  for x in xs:\n    output = output + x\n  return output\n\")$sum_jit\n\nsum_jit(list(torch_randn(1), torch_randn(1)))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\ntorch_tensor\n-0.7121\n[ CPUFloatType{1} ]\n```\n\n\n:::\n:::\n\n\n\n\n#### Tracing\n\nThe alternative to writing TorchScript is to write your module in R and to use [`jit_trace`](https://torch.mlverse.org/docs/reference/jit_trace_module.html) to compile it.\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nf2 = function(x, w, bias) {\n  x$matmul(w) + bias\n}\n# need to provide some example input\n# arguments are passed by position\nf2 = jit_trace(f2, torch_randn(10, 10), torch_randn(10, 100), torch_randn(100))\nout2 = f2(x, w, bias)\ntorch_equal(out, out2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] TRUE\n```\n\n\n:::\n:::\n\n\n\n\nAn advantage of trace-compilation is that it even allows you to JIT-compile modules, which is currently not possible with `jit_compile`.\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nnet = nn_sequential(\n  nn_linear(10, 100),\n  nn_relu(),\n  nn_linear(100, 10)\n)\nnet_jit = jit_trace(net, torch_randn(10, 10))\n\ntorch_equal(net(x), net_jit(x))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] TRUE\n```\n\n\n:::\n:::\n\n\n\n\nTrace-compilation is restrictive because it only records operations applied to torch tensors and is unaware of R control flow, so you need to be careful when using it.\nFurthermore, it only accepts torch tensors as arguments.\nUnless you have dynamic inputs and outputs or modify the configuration of the module, trace-compilation should usually work.\nYou can also check this by running the original and trace-jitted module on some example inputs and see if they return the same result.\n\n:::{.callout-note}\nA trace-jitted module *does* respect the mode of the network, i.e., whether it is training or evaluating.\n:::\n\nIn `mlr3torch`, trace compilation is also available and can be enabled by setting `jit_trace = TRUE` in the learner.\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlearner = lrn(\"classif.mlp\", jit_trace = TRUE)\n```\n:::\n\n\n\n\nYou can also combine TorchScript with tracing:\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nnet_both = nn_module(\n  initialize = function() {\n    self$linear = nn_linear(1, 1)\n  },\n  forward = function(x) {\n    self$linear(sum_jit(x))\n  }\n)()\n\nnet_both(list(torch_randn(1), torch_randn(1)))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\ntorch_tensor\n 1.0027\n[ CPUFloatType{1} ][ grad_fn = <ViewBackward0> ]\n```\n\n\n:::\n\n```{.r .cell-code}\nnet_both(list(torch_randn(1)))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\ntorch_tensor\n0.01 *\n 8.5286\n[ CPUFloatType{1} ][ grad_fn = <ViewBackward0> ]\n```\n\n\n:::\n:::\n\n\n\n\n:::{.callout-note}\n## Quiz: Just In Time\n\n**Question 1**: Consider the trace-jitted function below. Can you predict the output of the last two lines? Can you explain why this happens?\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nf = function(a, b, multiply) {\n  if (multiply$item()) {\n    a * b\n  } else {\n    a + b\n  }\n}\nfjit = jit_trace(f, torch_tensor(1), torch_tensor(2), torch_tensor(TRUE))\n\nfjit(torch_tensor(2), torch_tensor(3), torch_tensor(TRUE))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\ntorch_tensor\n 6\n[ CPUFloatType{1} ]\n```\n\n\n:::\n\n```{.r .cell-code}\nfjit(torch_tensor(2), torch_tensor(3), torch_tensor(FALSE))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\ntorch_tensor\n 6\n[ CPUFloatType{1} ]\n```\n\n\n:::\n:::\n\n\n\n\n**Question 2**: Answer the same question for the following function:\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nf = function(a, b, multiply) {\n  torch_where(multiply, a * b, a + b)\n}\nfjit = jit_trace(f, torch_tensor(1), torch_tensor(2), torch_tensor(TRUE))\n\nfjit(torch_tensor(2), torch_tensor(3), torch_tensor(TRUE))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\ntorch_tensor\n 6\n[ CPUFloatType{1} ]\n```\n\n\n:::\n\n```{.r .cell-code}\nfjit(torch_tensor(2), torch_tensor(3), torch_tensor(FALSE))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\ntorch_tensor\n 5\n[ CPUFloatType{1} ]\n```\n\n\n:::\n:::\n\n\n\n:::\n\n### Mixed Precision Training\n\nAnother way to speed up the training process is to use mixed precision training.\nThis technique involves training the model using both 16-bit and 32-bit floating point numbers.\nThis allows reducing the memory footprint of the model and speeding up the training process.\n\nWe won't cover this here, but refer to the [torch documentation](https://torch.mlverse.org/docs/articles/amp) that explains how to do this.\n\n## Methodological Approaches\n\n### Validation and Early Stopping\n\nFor more details on this topic, see the [corresponding chapter](https://mlr3book.mlr-org.com/chapters/chapter15/predsets_valid_inttune.html) in the `mlr3` book.\n\nAs we have already seen in one of the previous notebooks, in deep learning, some part of the data is often used for validation purposes.\nThis allows monitoring the performance of the model on unseen data.\n\nIn `mlr3torch`, we can track the performance of the model on a validation set by specifying:\n\n* `validate`, which is the ratio of the data that is used for validation\n* `measures_valid`, which is a list of measures to use for validation\n* `eval_freq`, which is the frequency at which the validation is performed\n* `callbacks`, which is a list of callbacks to use during training, in this case, we use the `history` callback, which records the performance of the model on the validation set at regular intervals, enabling us to monitor and analyze the model's performance over time.\n\n:::{.callout-tip}\nWhile `mlr3torch` comes with predefined callbacks, it is also possible to define custom callbacks that modify the training process.\n:::\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ntask = tsk(\"sonar\")\n\nmlp_learner = lrn(\"classif.mlp\",\n  neurons = c(50, 50), batch_size = 256, epochs = 400,\n  optimizer = t_opt(\"adam\", lr = 0.003),\n  predict_type = \"prob\", jit_trace = TRUE,\n  # Validation / Performance Monitoring\n  validate = 0.3, # how much data to use for validation\n  measures_valid = msr(\"classif.logloss\"), # how to evaluate train performance\n  measures_train = msr(\"classif.logloss\"), # how to evaluate validation performance\n  callbacks = t_clbk(\"history\"), # history callbacks save train and validation performance\n  eval_freq = 10 # after how many training epochs to perform validation\n)\nmlp_learner$train(task)\nhistory = mlp_learner$model$callbacks$history\nstr(history)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nClasses 'data.table' and 'data.frame':\t40 obs. of  3 variables:\n $ epoch                : num  10 20 30 40 50 60 70 80 90 100 ...\n $ train.classif.logloss: num  0.678 0.644 0.586 0.507 0.465 ...\n $ valid.classif.logloss: num  0.674 0.637 0.567 0.491 0.435 ...\n - attr(*, \".internal.selfref\")=<externalptr> \n - attr(*, \"sorted\")= chr \"epoch\"\n```\n\n\n:::\n\n```{.r .cell-code}\nhead(history)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nKey: <epoch>\n   epoch train.classif.logloss valid.classif.logloss\n   <num>                 <num>                 <num>\n1:    10             0.6777272             0.6738506\n2:    20             0.6436647             0.6367521\n3:    30             0.5858993             0.5672350\n4:    40             0.5070930             0.4911240\n5:    50             0.4648626             0.4353934\n6:    60             0.3503119             0.4365502\n```\n\n\n:::\n:::\n\n\n\n\nBelow we plot the training and validation for the different epochs:\n\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](6-training-efficiency_files/figure-html/unnamed-chunk-30-1.png){fig-align='center' width=672}\n:::\n:::\n\n\n\n\nInstead of only monitoring the validation loss (and watching it get worse and worse), we can also stop the training process dynamically when the validation loss begins to increase.\nThis regularization technique is called early stopping, and it prevents overfitting during the training of iteratively trained machine learning models.\nIt involves monitoring the validation loss during training and stopping the training process when the validation loss begins to increase, indicating that the model is starting to overfit the training data.\n\nThe key configuration option for early stopping is the `patience` parameter, which defines the number of epochs to wait after the last improvement in validation loss before stopping the training. For example, if patience is set to 10, the training will continue for 10 additional epochs after the last observed improvement in validation loss. If no improvement is seen during this period, training will be halted.\n\nAdvantages of early stopping include:\n\n- **Prevention of Overfitting**: By stopping training when the model starts to overfit, we can achieve better generalization on unseen data.\n- **Resource Efficiency**: It saves computational resources by avoiding unnecessary training epochs once the model performance has plateaued.\n\nNow, let's train the learner again using early stopping with a patience of 10 epochs:\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nmlp_learner$param_set$set_values(\n  patience = 5\n)\nmlp_learner$train(task)\nmlp_learner$internal_tuned_values$epochs\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 160\n```\n\n\n:::\n:::\n\n\n\n\nBeyond only tuning the number of epochs, `mlr3`'s internal tuning mechanism also allows tuning the number of epochs internally while using an offline tuning method to optimize other hyperparameters.\nTo use this, we can set the parameters we want to tune `TuneTokens`:\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(mlr3tuning)\nmlp_learner$param_set$set_values(\n  epochs = to_tune(upper = 100, internal = TRUE),\n  opt.lr = to_tune(lower = 1e-4, upper = 1e-1, logscale = TRUE)\n)\n```\n:::\n\n\n\n\nWe could now pass this learner to a tuner, where the tuner would only optimize the learning rate, while the learner optimizes the epochs internally.\n\n## Architecture Design\n\nAnother essential aspect of training neural networks efficiently and effectively is the design of the network architecture, which can be a challenging task.\nHowever, for many tasks, there are well-known architectures that perform well and can be used as a starting point.\nUnless there is a specific reason to design a new architecture, it is recommended to use such an architecture.\n\n:::{.callout-note}\nBecause the Python deep learning ecosystem is so large, many more architectures are implemented in Python than in R.\nOne way to use them in R is to simply translate the PyTorch code to (R-)torch.\nWhile PyTorch and (R-)torch are quite similar, there are some differences, e.g., 1-based and 0-based indexing.\nThe `torch` website contains a [brief tutorial](https://torch.mlverse.org/docs/articles/python-to-r) on how to do this.\n:::\n\nNonetheless, we will cover important techniques that can be used to speed up the training process, namely *batch normalization* and *dropout*.\n\n### Batch Normalization\n\nBatch Normalization is an important technique in deep learning that contributed significantly to speeding up the training process.\n\nThe formula for batch normalization (during training) is given by:\n\n$$\n\\hat{x} = \\frac{x - \\mu_B}{\\sqrt{\\sigma_B^2 + \\epsilon}}\n$$\n\nwhere:\n\n- $\\hat{x}$ is the normalized output,\n- $x$ is the input,\n- $\\mu_B$ is the mean of the batch,\n- $\\sigma_B^2$ is the variance of the batch,\n- $\\epsilon$ is a small constant added for numerical stability.\n\nDuring inference, the module uses the running mean and variance of the training data to normalize the input.\n\nIn `torch`, different versions of batch normalization exist for different dimensions of the input tensor.\nBelow, we illustrate the batch normalization module using a 1D input tensor (the batch dimension does not count here)\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nx = torch_randn(10, 5)\nbn = nn_batch_norm1d(num_features = 5)\nbn(x)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\ntorch_tensor\n 1.4613 -1.3934 -0.2146  1.0406  0.1413\n-0.9634 -0.3388  1.7441  0.7744  2.1476\n-2.0328  0.5667 -2.0592  0.4071 -0.0529\n 0.6778  0.3264  0.2637 -0.2301 -0.0409\n-0.9243  0.1298 -0.6447 -1.5477 -2.1935\n 0.8150 -0.1962  0.7988 -1.5426  0.1137\n-0.2350 -2.0121 -0.1847  1.1725  0.0143\n 0.8381  0.6141  0.9971  1.0148 -0.5667\n 0.2166  0.7147 -0.7208 -0.1408 -0.0285\n 0.1467  1.5887  0.0203 -0.9482  0.4657\n[ CPUFloatType{10,5} ][ grad_fn = <NativeBatchNormBackward0> ]\n```\n\n\n:::\n:::\n\n\n\n\n:::{.callout-note}\n## Quiz: Batch Normalization\n\n**Question 1**: Earlier we have learned that `nn_module`s have buffers and parameters, where the latter are learned with gradient descent.\nDo you think the mean and variance are parameters or buffers?\n\n<details>\n<summary>Click for answer</summary>\nThey are both buffers as they only store the variance and running mean of all training samples seen, i.e., they are not updated using gradient information.\n</details>\n\n**Question 2**: Training vs. Evaluation Mode:\nWhile many `nn_module`s behave the same way irrespective of their mode, batch normalization is an example of a module that behaves differently during training and evaluation, i.e.,\nduring training, the module uses the mean and variance of the current batch, while during evaluation, it uses the running mean and variance of all training samples seen.\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nbn(x[1:10, ])\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\ntorch_tensor\n 1.4613 -1.3934 -0.2146  1.0406  0.1413\n-0.9634 -0.3388  1.7441  0.7744  2.1476\n-2.0328  0.5667 -2.0592  0.4071 -0.0529\n 0.6778  0.3264  0.2637 -0.2301 -0.0409\n-0.9243  0.1298 -0.6447 -1.5477 -2.1935\n 0.8150 -0.1962  0.7988 -1.5426  0.1137\n-0.2350 -2.0121 -0.1847  1.1725  0.0143\n 0.8381  0.6141  0.9971  1.0148 -0.5667\n 0.2166  0.7147 -0.7208 -0.1408 -0.0285\n 0.1467  1.5887  0.0203 -0.9482  0.4657\n[ CPUFloatType{10,5} ][ grad_fn = <NativeBatchNormBackward0> ]\n```\n\n\n:::\n:::\n\n\n\n\nWhich of the following statements is true and why?\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nbn$eval()\nequal1 = torch_equal(\n  torch_cat(list(bn(x[1:2, ]), bn(x[3:4, ]))),\n  bn(x[1:4, ])\n)\nbn$train()\nequal2 = torch_equal(\n  torch_cat(list(bn(x[1:2, ]), bn(x[3:4, ]))),\n  bn(x[1:4, ])\n)\n```\n:::\n\n\n\n\n<details>\n<summary>Click for answer</summary>\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nc(equal1, equal2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1]  TRUE FALSE\n```\n\n\n:::\n:::\n\n\n\n\nThe first statement is true because, in evaluation mode, the module uses the running mean and variance of all training samples seen.\nThe second statement is false because the first tensor uses different means and variances for rows 1-2 and 3-4, while the second tensor uses the same mean and variance for all rows.\n</details>\n:::\n\nTo illustrate its effectiveness, we will define a simple CNN, with and without batch normalization, train it on CIFAR-10, and compare their performance.\n\nTo build the neural networks, we will use `mlr3torch`, which allows building architectures from `PipeOp`s.\nThis makes the creation of network architectures easier, as we, e.g., don't have to specify auxiliary parameters (such as the input dimension of a linear layer).\nRecall that the `po(\"torch_ingress_ltnsr\")` is a special `PipeOp` that marks the input of the neural network.\nNote that `po(\"nn_relu_1\")` is equivalent to `po(\"nn_relu\", id = \"nn_relu_1\")`.\nWe need to specify unique ID parameters as this is required in `mlr3pipelines`.\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ncnn_bn = po(\"torch_ingress_ltnsr\") %>>%\n  po(\"nn_conv2d_1\", out_channels = 32, kernel_size = 3, stride = 1, padding = 1) %>>%\n  po(\"nn_batch_norm2d_1\") %>>%\n  po(\"nn_relu_1\") %>>%\n  po(\"nn_max_pool2d_1\", kernel_size = 2, stride = 2) %>>%\n  po(\"nn_conv2d_2\", out_channels = 64, kernel_size = 3, stride = 1, padding = 1) %>>%\n  po(\"nn_batch_norm2d_2\") %>>%\n  po(\"nn_relu_2\") %>>%\n  po(\"nn_max_pool2d_2\", kernel_size = 2, stride = 2)\n\ncnn = po(\"torch_ingress_ltnsr\") %>>%\n  po(\"nn_conv2d_1\", out_channels = 32, kernel_size = 3, stride = 1, padding = 1) %>>%\n  po(\"nn_relu_1\") %>>%\n  po(\"nn_max_pool2d_1\", kernel_size = 2, stride = 2) %>>%\n  po(\"nn_conv2d\", out_channels = 64, kernel_size = 3, stride = 1, padding = 1) %>>%\n  po(\"nn_relu_2\") %>>%\n  po(\"nn_max_pool2d_2\", kernel_size = 2, stride = 2)\n\nhead = po(\"nn_flatten\") %>>%\n  po(\"nn_linear\", out_features = 128) %>>%\n  po(\"nn_relu\") %>>%\n  po(\"nn_head\")\n\nmodel = po(\"torch_optimizer\", optimizer = t_opt(\"adam\", lr = 0.003)) %>>%\n  po(\"torch_model_classif\",\n    epochs = 100,\n    batch_size = 256,\n    predict_type = \"prob\",\n    device = \"cuda\"\n  )\n```\n:::\n\n\n\n\nWe evaluate the two models on the CIFAR-10 image classification task that we have introduced earlier.\nThere, the goal is to classify images into 10 different classes.\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nnet_bn = as_learner(cnn_bn %>>% head %>>% model)\nnet_bn$id = \"net_bn\"\nnet = as_learner(cnn %>>% head %>>% model)\nnet$id = \"net\"\n\ncifar10 = tsk(\"cifar10\")\nresampling = rsmp(\"holdout\")$instantiate(cifar10)\n\ndesign = benchmark_grid(\n  task = cifar10,\n  learner = list(net_bn, net),\n  resampling = resampling\n)\ndesign\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n      task learner resampling\n    <char>  <char>     <char>\n1: cifar10  net_bn    holdout\n2: cifar10     net    holdout\n```\n\n\n:::\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nbmr = benchmark(design)\nbmr$aggregate()\n```\n:::\n\n\n\n\n## Dropout\n\nDropout is a regularization technique used to prevent overfitting in neural networks by randomly setting a fraction of input units to zero during training. This encourages the network to learn more robust features that are not reliant on specific neurons, thereby improving its generalization capabilities.\nDuring each training iteration, dropout randomly \"drops\" a subset of neurons by setting their activations to zero with a specified probability (commonly between 20% to 50%). This forces the network to distribute the learned representations more evenly across neurons, reducing the reliance on any single neuron and mitigating overfitting.\nDropout is more commonly used in the context of fully connected layers.\n\n![](../assets/dropout.png){fig-align=\"center\" width=100%}\n\nSource: https://medium.com/konvergen/understanding-dropout-ddb60c9f98aa\n\nJust like batch normalization, it also has different behavior during training and evaluation.\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ndropout = nn_dropout(p = 0.5)\ndropout(x)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\ntorch_tensor\n 0.0000 -3.9488  0.0093  0.0000  0.7024\n-0.0000 -1.4141  0.0000  2.9566  5.7694\n-4.4366  0.7622 -0.0000  2.1163  0.2118\n 0.0000  0.0000  0.0000  0.6584  0.2422\n-0.0000 -0.0000 -0.9663 -0.0000 -5.1942\n 0.0000 -1.0714  2.3080 -0.0000  0.6326\n-1.1987 -5.4360  0.0000  3.8675  0.0000\n 0.0000  0.8761  2.7579  3.5069 -0.0000\n-0.3855  1.1178 -0.0000  0.8627  0.0000\n-0.0000  0.0000  0.0000 -0.0000  1.5217\n[ CPUFloatType{10,5} ]\n```\n\n\n:::\n\n```{.r .cell-code}\ndropout$eval()\ndropout(x)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\ntorch_tensor\n 0.9281 -1.9744  0.0046  1.7829  0.3512\n-1.2553 -0.7071  2.2261  1.4783  2.8847\n-2.2183  0.3811 -2.0875  1.0582  0.1059\n 0.2226  0.0924  0.5471  0.3292  0.1211\n-1.2201 -0.1440 -0.4831 -1.1782 -2.5971\n 0.3462 -0.5357  1.1540 -1.1725  0.3163\n-0.5994 -2.7180  0.0385  1.9338  0.1908\n 0.3669  0.4380  1.3789  1.7534 -0.5429\n-0.1927  0.5589 -0.5695  0.4313  0.1367\n-0.2556  1.6093  0.2711 -0.4924  0.7609\n[ CPUFloatType{10,5} ]\n```\n\n\n:::\n:::\n\n\n\n\nTo look at the effects, we will create a second classification head with dropout and then define new learners\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nhead_dropout = po(\"nn_flatten\") %>>%\n  po(\"nn_linear\", out_features = 128) %>>%\n  po(\"nn_relu\") %>>%\n  po(\"nn_dropout\", p = 0.5) %>>%\n  po(\"nn_head\")\n\nnet_bn_dropout = as_learner(cnn_bn %>>% head_dropout %>>% model)\nnet_bn_dropout$id = \"net_bn_dropout\"\nnet_dropout = as_learner(cnn %>>% head_dropout %>>% model)\nnet_dropout$id = \"net_dropout\"\n\ndesign2 = benchmark_grid(\n  task = cifar10,\n  learner = list(net_bn_dropout, net_dropout),\n  resampling = resampling\n)\n```\n:::\n\n\n\n\nNext, we run the second benchmark experiment and afterwards combine the results with the first benchmark experiment.\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nbmr2 = benchmark(design2)\nbmr = c(bmr, bmr2)\nautoplot(bmr)\n```\n:::\n\n\n\n\n:::{.callout-note}\n## Quiz: Dropout\n\n**Question 1**: Worse Training Loss: You are training a neural network with and without dropout. The training loss is higher with dropout, is this a bug?\n\n<details>\n<summary>Click for answer</summary>\nNot necessarily, as dropout is a regularization technique that prevents overfitting.\nIt's goal is to reduce the generalization performance of the model.\n</details>\n:::\n\n## Transfer Learning\n\nTransfer learning is a powerful technique in machine learning where a pre-trained model developed for a specific task is reused as the starting point for a model on a second, related task. Instead of training a model from scratch, which can be time-consuming and computationally expensive, transfer learning leverages the knowledge gained from a previously learned task to improve learning efficiency and performance on a new task.\n\nThe advantages of transfer learning are:\n\n1. Reduced Training Time: Leveraging a pre-trained model can significantly decrease the time required to train a new model, as the foundational feature extraction layers are already optimized.\n2. Improved Performance: Transfer learning can enhance model performance, especially when the new task has limited training data. The pre-trained model's knowledge helps in achieving better generalization.\n3. Resource Efficiency: Utilizing pre-trained models reduces the computational resources needed, making it feasible to develop sophisticated models without extensive hardware.\n\nWhen the model is then trained on a new task, only the last layer is replaced with a new output layer to adjust for the new task.\n\nThis is visualized below:\n\n![](../assets/transfer-learning.svg)\n\nSource: https://en.wikipedia.org/wiki/Transfer_learning\n\n`mlr3torch` connects various pretrained image networks that are available in the [`torchvision` package](https://torchvision.mlverse.org/).\nThe ResNet-18 model is a popular pre-trained model that was pretrained on ImageNet.\nWe can use the pretrained weights by setting the `pretrained` parameter to `TRUE`.\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nresnet = lrn(\"classif.resnet18\",\n  pretrained = TRUE,\n  epochs = 2,\n  batch_size = 256,\n  validate = 0.3,\n  measures_valid = msr(\"classif.logloss\"),\n  device = \"cuda\",\n  predict_type = \"prob\",\n  id = \"pretrained\"\n)\nresnet_no_pretrain = resnet$clone(deep = TRUE)\nresnet_no_pretrain$param_set$set_values(\n  pretrained = FALSE\n)\nresnet_no_pretrain$id = \"not_pretrained\"\n\ngrid = benchmark_grid(\n  task = tsk(\"cifar10\"),\n  learner = list(resnet, resnet_no_pretrain),\n  resampling = rsmp(\"insample\")\n)\n\nbmr = benchmark(grid, store_models = TRUE)\nbmr$aggregate()\n```\n:::\n\n\n\n\nWhen fine-tuning a pretrained model like ResNet-18, it's common to observe instabilities in gradients, which can manifest as fluctuating validation performance.\nThis can e.g. be because the learning rate is too high (compared to the learning rate that was used during pretraining).\n\nTo address this, one can:\n\n1. Use a smaller learning rate for the pretrained layers than for the new output head.\n2. Freeze the pretrained layers (for some epochs) and only train the new output head.\n\nIn `mlr3torch` this can be achieved via the callback mechanism.\nFor the unfreezing, there even exists a predefined callback `t_clbk(\"unfreeze\")`.\nTo create a custom callback, the `torch_callback()` function can be used.\nA tutorial on this can be found on the [`mlr3torch` package website](https://mlr3torch.mlr-org.com/index.html).\n\n:::{.callout-note}\n## In-Context Learning\n\nLarge  foundation models (such as GPT-4) even allow to perform tasks on which they were not pretrained on without any finetuning.\nThis is referred to as in-context learning or zero-shot learning.\nThere, the task is fed into the model during inference: \"Hey ChatGPT, is What is the sentiment of this sentence. Return -1 for sad, 0 for neutral, 1 for happy: <sentence>\"\n:::\n\n## Data Augmentation\n\nData augmentation is a technique used to increase the diversity and quantity of training data without actually collecting new data.\nBy applying various transformations to the existing dataset, data augmentation helps improve the generalization capabilities of machine learning models, reduce overfitting, and enhance model robustness.\nThis is especially crucial when you have limited data.\n\nData augmentation for images can consist of rotation, flipping, translating, grey scaling, etc.\nWhich data augmentation is admissible, depends on the task:\n\n- If the modeling task is to predict whether there is a mark in the top right corner of an image, vertical or horizontal flipping is not admissible.\n- If the goal is to predict whether there is a mark somewhere in the image, it would be admissible.\n\nIn other words, the data augmentation must be compatible with the invariances of the task.\n\nIn `mlr3torch`, data augmentation is available via `PipeOp`s of the form `po(\"augment_\")`.\nCurrently, only augemntation operators from the `torchvision` package are available, but you can also add your own.\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\naugment = po(\"augment_random_resized_crop\") %>>%\n  po(\"augment_random_horizontal_flip\") %>>%\n  po(\"augment_random_vertical_flip\")\n```\n:::\n\n\n\n\nWe can just create a new `GraphLearner` that includes the augemntation steps as well as the learner from above:\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nresnet_augmented = as_learner(augment %>>% resnet)\nresnet_augmented$id = \"resnet_augmented\"\nresnet_augmented$train(task = cifar10)\n```\n:::\n\n\n\n\n:::{.callout-note}\n## Quiz: Data Augmentation\n\n**Question 1**: Do you think data augmentation should be applied to the validation set?\n\n<details>\n<summary>Click for answer</summary>\nNo, as the purpose of data augmentation is not to improve an individual prediction, it will not be applied during test time and hence also not to the validation set.\nLooking at the performance of augmented validation data is, however, also not a mistake.\n</details>\n:::\n",
    "supporting": [
      "6-training-efficiency_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}