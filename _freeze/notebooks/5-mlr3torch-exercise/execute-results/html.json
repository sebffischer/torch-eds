{
  "hash": "711204c30d76397e502f87114cbdbfa5",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Training Neural Networks with mlr3torch\"\n---\n\n\n\n\n\n\n**Question 1:** Hello World!\n\nIn this exercise, you will train your first neural network with `mlr3torch`.\n\nAs a task, we will use the 'Indian Liver Patient' dataset where the goal is to predict whether a patient has liver disease or not.\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(mlr3verse)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nLoading required package: mlr3\n```\n\n\n:::\n\n```{.r .cell-code}\nlibrary(mlr3torch)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nLoading required package: mlr3pipelines\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nLoading required package: torch\n```\n\n\n:::\n\n```{.r .cell-code}\nilpd <- tsk(\"ilpd\")\nilpd\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n<TaskClassif:ilpd> (583 x 11): Indian Liver Patient Data\n* Target: diseased\n* Properties: twoclass\n* Features (10):\n  - dbl (5): albumin, albumin_globulin_ratio, direct_bilirubin, total_bilirubin, total_protein\n  - int (4): age, alanine_transaminase, alkaline_phosphatase, aspartate_transaminase\n  - fct (1): gender\n```\n\n\n:::\n\n```{.r .cell-code}\nautoplot(ilpd)\n```\n\n::: {.cell-output-display}\n![](5-mlr3torch-exercise_files/figure-html/unnamed-chunk-2-1.png){fig-align='center' width=672}\n:::\n:::\n\n\n\nWe remove the *gender* column from the task, so we need to deal with numeric features for now.\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nilpd_num = ilpd$clone(deep = TRUE)\nilpd_num$select(setdiff(ilpd_num$feature_names, \"gender\"))\n```\n:::\n\n\n\n\nTrain a simple multi layer perceptron (`lrn(\"classif.mlp\")`) with 2 hidden layers with 100 neurons each.\nSet the batch size to 32, the learning rate to 0.001 and the number of epochs to 20.\nThen, resample the learner on the task with a cross-validation with 5 folds and evaluate the results using classification error and false positive rate (FPR).\nIs the result good?\n\n<details>\n<summary>Hint</summary>\n* The parameter for the learning rate is `opt.lr`\n* Probability predictions are made by setting the `predict_type` field to `\"prob\"`.\n</details>\n\n\n::: {.content-visible when-meta=solutions}\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nmlp <- lrn(\"classif.mlp\",\n  neurons = c(100, 100),\n  batch_size = 32,\n  epochs = 20,\n  opt.lr = 0.001\n)\ncv10 <- rsmp(\"cv\", folds = 10)\nrr1 <- resample(task = ilpd_num, learner = mlp, resampling = cv10)\nrr1$aggregate(msrs(c(\"classif.ce\", \"classif.fpr\")))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n classif.ce classif.fpr \n  0.3706312   0.5849387 \n```\n\n\n:::\n:::\n\n\n\nWhile the classification error is low, this is not a good measure due to the imbalanced class distribution.\nThis is confirmed by the FPR, which is relatively high.\n:::\n\n**Question 2:** Preprocessing\n\nIn the previous task, we have operated on the `ilpd_num` task where we excluded the categorical `gender` column.\nThis was done because the MLP learner operates on numeric features only.\nWe will now create a more complex `GraphLearner` that also incudes one-hot encoding of the `gender` column.\nResample this learner on the original `ilpd` task and evaluate the results using the same measures as before.\n\n<details>\n<summary>Hint</summary>\nConcatenate `po(\"encode\")` with a `lrn(\"classif.mlp\")` using `%>>%` to create the `GraphLearner`.\nFor available options on the encoding, see `po(\"encode\")$help()`.\n</details>\n\n:::{.content-visible when-meta=solutions}\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nencoder <- po(\"encode\", method = \"one-hot\")\nglrn <- as_learner(encoder %>>% mlp)\nrr2 <- resample(ilpd, glrn, cv10)\nrr2$aggregate(msrs(c(\"classif.ce\", \"classif.fpr\")))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n classif.ce classif.fpr \n  0.3137347   0.8365021 \n```\n\n\n:::\n:::\n\n\n:::\n\n**Question 3**: Benchmarking\n\nInstead of resampling a single learner, we now want to compare the performance of our MLP with a simple classification tree\nCreate a benchmark design and compare the performance of the two learners.\n\n<details>\n<summary>Hint</summary>\nCreate a classification tree via `lrn(\"classif.rpart\")`.\nA benchmark design can be created via `benchmark_grid()`.\nTo run a benchmark, pass the design to `benchmark()`.\n</details>\n\n\n:::{.content-visible when-meta=solutions}\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ndesign <- benchmark_grid(\n  task = ilpd,\n  learner = list(glrn, lrn(\"classif.rpart\", predict_type = \"prob\")),\n  resampling = cv10\n)\nbmr <- benchmark(design)\nbmr$aggregate(msrs(c(\"classif.ce\", \"classif.tpr\")))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n      nr task_id         learner_id resampling_id iters classif.ce classif.tpr\n   <int>  <char>             <char>        <char> <int>      <num>       <num>\n1:     1    ilpd encode.classif.mlp            cv    10  0.2812683   0.9145299\n2:     2    ilpd      classif.rpart            cv    10  0.3035652   0.8301628\nHidden columns: resample_result\n```\n\n\n:::\n:::\n\n\n:::\n",
    "supporting": [
      "5-mlr3torch-exercise_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}