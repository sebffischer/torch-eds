{
  "hash": "394ea7a3e6200155cee14de00db0e761",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Torch Tensors\"\n---\n\n\n\n\n\n\n* **Torch Website**: [https://torch.mlverse.org/](https://torch.mlverse.org/docs/)\n* **API Docs**: [https://torch.mlverse.org/docs/](https://torch.mlverse.org/)\n* **Book by Sigrid Keydana**: [Deep Learning and Scientific Computing with R torch](https://skeydan.github.io/Deep-Learning-and-Scientific-Computing-with-R-torch/) on which part of this is based.\n* Since `torch` mimics `PyTorch` and the latter has a larger community, you can often learn from the [PyTorch documentation](https://pytorch.org/docs/stable/index.html).\n\n# Installation\n\nTo use `torch`, install it from CRAN:\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ninstall.packages(\"torch\")\n```\n:::\n\n\n\nAfterward, run:\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ntorch::install_torch()\n```\n:::\n\n\n\nIf both commands succeed, you are ready to go.\nOtherwise, you can consult [this guide](https://torch.mlverse.org/docs/articles/installation) on how to install torch.\nYou can check whether you have successfully installed cuda support (requires an NVIDIA GPU) by running:\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(torch)\ncuda_is_available()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] FALSE\n```\n\n\n:::\n:::\n\n\n\n# Torch Tensors\n\nTensors are the fundamental data structure in torch, serving as the backbone for both deep learning and scientific computing operations. While similar to R arrays, tensors offer enhanced capabilities that make them particularly suited for modern computational tasks, namely *GPU acceleration* and *automatic differentiation (autograd)*.\n\n## Creating Tensors\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# From R matrices\nx_matrix <- matrix(1:6, nrow = 2, ncol = 3)\ntensor_x <- torch_tensor(x_matrix)\nprint(tensor_x)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\ntorch_tensor\n 1  3  5\n 2  4  6\n[ CPULongType{2,3} ]\n```\n\n\n:::\n\n```{.r .cell-code}\nzeros_tensor <- torch_zeros(2, 3)      # Creates a tensor of zeros\nones_tensor <- torch_ones(2, 3)        # Creates a tensor of ones\nlike_tensor <- torch_zeros_like(ones_tensor)  # Creates a zeros tensor with the same shape as ones_tensor\n```\n:::\n\n\n\n### Random Sampling\n\nYou can also randomly sample torch tensors:\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nnormal_tensor <- torch_randn(2, 3)    # Samples from N(0,1)\nuniform_tensor <- torch_rand(2, 3)    # Samples from U(0,1)\n```\n:::\n\n\n\n::: {.callout-warning}\n## Random Seeds in torch\ntorch maintains its own random number generator, separate from R's.\n\nSetting R's random seed with `set.seed()` does not affect torch's random operations. Instead, use `torch_manual_seed()` to control the reproducibility of torch operations.\n:::\n\n### Missing Values\n\n:::{.callout-note}\n## Quiz: NaN vs NA\n\n**Question 1**: What is the difference between `NaN` and `NA` in R?\n\n<details>\n<summary>Click for answer</summary>\n`NaN` is a floating-point value that represents an undefined or unrepresentable value (such as `0 / 0`).\n\n`NA` is a missing value indicator used in vectors, matrices, and data frames to represent unknown or missing data.\n</details>\n:::\nTorch tensors do not have a native representation for R's `NA` values. When converting R vectors containing `NA`s to torch tensors, you need to be cautious:\n\n* *Double*: `NA_real_` becomes `NaN`\n\n\n  ::: {.cell layout-align=\"center\"}\n  \n  ```{.r .cell-code}\n  torch_tensor(NA_real_)\n  ```\n  \n  ::: {.cell-output .cell-output-stdout}\n  \n  ```\n  torch_tensor\n  nan\n  [ CPUFloatType{1} ]\n  ```\n  \n  \n  :::\n  :::\n\n\n\n* *Integer*: `NA_integer_` becomes the smallest negative value:\n\n\n  ::: {.cell layout-align=\"center\"}\n  \n  ```{.r .cell-code}\n  torch_tensor(NA_integer_)\n  ```\n  \n  ::: {.cell-output .cell-output-stdout}\n  \n  ```\n  torch_tensor\n  -2.1475e+09\n  [ CPULongType{1} ]\n  ```\n  \n  \n  :::\n  :::\n\n\n\n* *Logical*: `NA` becomes `TRUE`:\n\n\n  ::: {.cell layout-align=\"center\"}\n  \n  ```{.r .cell-code}\n  torch_tensor(NA)\n  ```\n  \n  ::: {.cell-output .cell-output-stdout}\n  \n  ```\n  torch_tensor\n   1\n  [ CPUBoolType{1} ]\n  ```\n  \n  \n  :::\n  :::\n\n\n\nYou should handle missing values carefully before converting them to torch tensors.\n\n## Tensor Properties\n\n### Shape\n\nLike R arrays, each tensor has a shape and a dimension:\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nprint(tensor_x$shape)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 2 3\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(tensor_x$dim()) # dim(tensor_x) also works\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 2\n```\n\n\n:::\n:::\n\n\n\n### Data Type\n\nFurthermore, each tensor has a datatype. Unlike base R, where typically there is one `integer` type (32 bits) and one floating-point type (`double`, 64 bits), torch differentiates between different precisions:\n\n* *Floating point:*\n\n\n\n  ::: {.cell layout-align=\"center\"}\n  \n  ```{.r .cell-code}\n  float32_tensor <- torch_ones(2, 3, dtype = torch_float32())  # Default float\n  float64_tensor <- torch_ones(2, 3, dtype = torch_float64())  # Double precision\n  float16_tensor <- torch_ones(2, 3, dtype = torch_float16())  # Half precision\n  ```\n  :::\n\n\n\n  Usually, you work with 32-bit floats.\n\n* *Integer:*\n\n\n\n  ::: {.cell layout-align=\"center\"}\n  \n  ```{.r .cell-code}\n  int32_tensor <- torch_ones(2, 3, dtype = torch_int32())\n  int64_tensor <- torch_ones(2, 3, dtype = torch_int64())  # Long\n  int16_tensor <- torch_ones(2, 3, dtype = torch_int16())  # Short\n  int8_tensor  <- torch_ones(2, 3, dtype = torch_int8())    # Byte\n  uint8_tensor <- torch_ones(2, 3, dtype = torch_uint8())  # Unsigned byte\n  ```\n  :::\n\n\n\n* *Boolean:*\n\n\n\n  ::: {.cell layout-align=\"center\"}\n  \n  ```{.r .cell-code}\n  bool_tensor <- torch_ones(2, 3, dtype = torch_bool())\n  ```\n  :::\n\n\n\nYou can convert between datatypes using the `$to()` method:\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Converting between datatypes\nx <- torch_ones(2, 3)  # Default float32\nx_int <- x$to(dtype = torch_int32())\n```\n:::\n\n\n\nNote that floats are converted to integers by truncating, not by rounding.\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ntorch_tensor(2.999)$to(dtype = torch_int())\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\ntorch_tensor\n 2\n[ CPUIntType{1} ]\n```\n\n\n:::\n\n```{.r .cell-code}\ntorch_tensor(-2.999)$to(dtype = torch_int())\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\ntorch_tensor\n-2\n[ CPUIntType{1} ]\n```\n\n\n:::\n:::\n\n\n\n### Device\n\nEach tensor lives on a \"device\", where common options are:\n\n* *cpu* for CPU, which is available everywhere\n* *cuda* for NVIDIA GPUs\n* *mps* for Apple Silicon (M1/M2/M3) GPUs on macOS\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Create a tensor and move it to CUDA if available\nx <- torch_randn(2, 3)\nif (cuda_is_available()) {\n  x <- x$to(device = torch_device(\"cuda\"))\n  # x <- x$cuda() also works\n} else {\n  print(\"CUDA not available; tensor remains on CPU\")\n}\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"CUDA not available; tensor remains on CPU\"\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(x$device)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\ntorch_device(type='cpu') \n```\n\n\n:::\n\n```{.r .cell-code}\nx <- x$to(device = \"cpu\")\n# x <- x$cpu() also works\nprint(x$device)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\ntorch_device(type='cpu') \n```\n\n\n:::\n:::\n\n\n\nGPU acceleration enables massive parallelization of tensor operations, often providing 10-100x speedups compared to CPU processing for large-scale computations.\n\n::: {.callout-warning}\n## Device Compatibility\nTensors must reside on the same device to perform operations between them.\n:::\n\n## Converting Tensors Back to R\n\nYou can easily convert torch tensors back to R using `as_array()`, `as.matrix()`, or `$item()`:\n\n* 0-dimensional tensors (scalars) are converted to R vectors with length 1:\n\n  ```r:1-tensor.qmd\n  torch_scalar_tensor(1)$item() # as_array() also works\n  ```\n\n* 1-dimensional tensors are converted to R vectors:\n\n  ```r:1-tensor.qmd\n  as_array(torch_randn(3))\n  ```\n* $>1$-dimensional tensors are converted to R arrays:\n\n\n\n  ::: {.cell layout-align=\"center\"}\n  \n  ```{.r .cell-code}\n  as_array(torch_randn(2, 2))\n  ```\n  \n  ::: {.cell-output .cell-output-stdout}\n  \n  ```\n             [,1]      [,2]\n  [1,] -1.4168206 0.8429176\n  [2,] -0.6306752 1.2340047\n  ```\n  \n  \n  :::\n  :::\n\n\n\n## Basic Tensor Operations\n\nTorch provides two main syntaxes for tensor operations: function-style (`torch_*()`) and method-style (using `$`).\n\nHere's an example with matrix multiplication:\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Create example tensors\na <- torch_tensor(matrix(1:6, nrow=2, ncol=3))\nb <- torch_tensor(matrix(7:12, nrow=3, ncol=2))\n\n# Matrix multiplication - two equivalent ways\nc1 <- torch_matmul(a, b)  # Function style\nc2 <- a$matmul(b)         # Method style\n\ntorch_equal(c1, c2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] TRUE\n```\n\n\n:::\n:::\n\n\n\nBelow, there is another example using addition:\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Addition - two equivalent ways\nx <- torch_ones(2, 2)\ny <- torch_ones(2, 2)\nz1 <- torch_add(x, y)  # Function style\nz2 <- x$add(y)         # Method style\n```\n:::\n\n\n\n::: {.callout-tip}\n## In-place Operations\nOperations that modify the tensor directly are marked with an underscore suffix (`_`). These operations are more memory efficient as they do not allocate a new tensor:\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nx <- torch_ones(2, 2)\nx$add_(1)  # Adds 1 to all elements in place\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\ntorch_tensor\n 2  2\n 2  2\n[ CPUFloatType{2,2} ]\n```\n\n\n:::\n\n```{.r .cell-code}\nx\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\ntorch_tensor\n 2  2\n 2  2\n[ CPUFloatType{2,2} ]\n```\n\n\n:::\n:::\n\n\n:::\n\nYou can also apply common summary functions to torch tensors:\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nx = torch_randn(1000)\nmean(x)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\ntorch_tensor\n-0.0239668\n[ CPUFloatType{} ]\n```\n\n\n:::\n\n```{.r .cell-code}\nmax(x)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\ntorch_tensor\n3.89537\n[ CPUFloatType{} ]\n```\n\n\n:::\n\n```{.r .cell-code}\nsd(x)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 1.028835\n```\n\n\n:::\n:::\n\n\n\nAccessing elements from a tensor is also similar to R arrays and matrices, i.e., it is 1-based.\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nx <- matrix(1:6, nrow = 3)\nxt <- torch_tensor(x)\nx[1:2, 1]\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 1 2\n```\n\n\n:::\n\n```{.r .cell-code}\nxt[1:2, 1]\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\ntorch_tensor\n 1\n 2\n[ CPULongType{2} ]\n```\n\n\n:::\n:::\n\n\n\nOne difference between indexing torch vectors and standard R vectors is the behavior regarding negative indices. While R vectors remove the element at the specified index, torch vectors return elements from the beginning.\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nx[-1, 1]\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 2 3\n```\n\n\n:::\n\n```{.r .cell-code}\nxt[-1, 1]\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\ntorch_tensor\n3\n[ CPULongType{} ]\n```\n\n\n:::\n:::\n\n\n\n::: {.callout-warning}\nWhile (R) torch is 1-based, PyTorch is 0-based. When translating PyTorch code to R, you need to be careful with this difference.\n:::\n\nAnother convenient feature in torch is the `..` syntax for indexing:\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\narr <- array(1:24, dim = c(4, 3, 2))\narr[1:2, , ] # works\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n, , 1\n\n     [,1] [,2] [,3]\n[1,]    1    5    9\n[2,]    2    6   10\n\n, , 2\n\n     [,1] [,2] [,3]\n[1,]   13   17   21\n[2,]   14   18   22\n```\n\n\n:::\n\n```{.r .cell-code}\narr[1:2, ]    # does not work\n```\n\n::: {.cell-output .cell-output-error}\n\n```\nError in arr[1:2, ]: incorrect number of dimensions\n```\n\n\n:::\n:::\n\n\n\nIn torch, you can achieve the same result as follows:\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ntensor <- torch_tensor(arr)\ntensor[1:2, ..]\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\ntorch_tensor\n(1,.,.) = \n   1  13\n   5  17\n   9  21\n\n(2,.,.) = \n   2  14\n   6  18\n  10  22\n[ CPULongType{2,3,2} ]\n```\n\n\n:::\n:::\n\n\n\nYou can also specify indices after the `..` operator:\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ntensor[.., 1]\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\ntorch_tensor\n  1   5   9\n  2   6  10\n  3   7  11\n  4   8  12\n[ CPULongType{4,3} ]\n```\n\n\n:::\n:::\n\n\n\nNote that when you select a single element from a dimension, the dimension is removed:\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ndim(tensor[.., 1])\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 4 3\n```\n\n\n:::\n\n```{.r .cell-code}\ndim(tensor[.., 1, drop = FALSE])\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 4 3 1\n```\n\n\n:::\n:::\n\n\n\nTensors also support indexing by boolean masks, which will result in a 1-dimensional tensor:\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ntensor[tensor > 15]\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\ntorch_tensor\n 17\n 21\n 18\n 22\n 19\n 23\n 16\n 20\n 24\n[ CPULongType{9} ]\n```\n\n\n:::\n:::\n\n\n\nWe can also extract the first two rows and columns of the tensor from the first index of the third dimension:\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ntensor[1:2, 1:2, 1]\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\ntorch_tensor\n 1  5\n 2  6\n[ CPULongType{2,2} ]\n```\n\n\n:::\n:::\n\n\n\n\n## Broadcasting Rules\n\nAnother difference between R arrays and torch tensors is how operations on tensors with different shapes are handled. For example, in R, we cannot add a matrix with shape `(1, 2)` to a matrix with shape `(2, 3)`:\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nm1 <- matrix(1:4, nrow = 2)\nm2 <- matrix(1:2, nrow = 2)\nm1 + m2\n```\n\n::: {.cell-output .cell-output-error}\n\n```\nError in m1 + m2: non-conformable arrays\n```\n\n\n:::\n:::\n\n\n\nBroadcasting (similar to \"recycling\" in R) allows torch to perform operations between tensors of different shapes.\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nt1 <- torch_tensor(m1)\nt2 <- torch_tensor(m2)\nt1 + t2\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\ntorch_tensor\n 2  4\n 4  6\n[ CPULongType{2,2} ]\n```\n\n\n:::\n:::\n\n\n\nThere are strict rules that define when two shapes are compatible:\n\n1. If tensors have a different number of dimensions, prepend 1's to the shape of the lower-dimensional tensor until they match.\n2. Two dimensions are compatible when:\n   * They are equal, or\n   * One of them is 1 (which will be stretched to match the other)\n3. If any dimension pair is incompatible, broadcasting fails.\n\n::: {.callout-note}\n## Quiz: Broadcasting Rules\n\n**Question 1**: What would be the resulting shape when broadcasting a tensor of shape `(2, 1, 3)` with a tensor of shape `(4, 3)`?\n\n<details>\n<summary>Click for answer</summary>\nThe resulting shape would be `(2, 4, 3)`. Here's why:\n\n1. Prepend one to the rank of the second tensor to get `(1, 4, 3)`.\n2. Going dimension by dimension:\n   * First: 2 vs 1 -> Compatible, expand to 2\n   * Second: 1 vs 4 -> Compatible, expand to 4\n   * Third: 3 vs 3 -> Compatible, remains 3\n3. All pairs are compatible, so broadcasting succeeds.\n</details>\n\n**Question 2**: Would broadcasting work between tensors of shape `(2, 3)` and `(3, 2)`?\n\n<details>\n<summary>Click for answer</summary>\nNo, broadcasting would fail in this case. Here's why:\n\n1. Both tensors have the same rank (2), so no prepending is needed.\n2. Going dimension by dimension:\n   * First: 2 vs 3 -> Incompatible (neither is 1)\n   * Second: 3 vs 2 -> Incompatible (neither is 1)\n3. Since both dimension pairs are incompatible, broadcasting fails.\n</details>\n:::\n\n## Reshaping Tensors\n\nTorch provides several ways to reshape tensors while preserving their data:\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Create a sample tensor\nx <- torch_tensor(0:15)\nprint(x)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\ntorch_tensor\n  0\n  1\n  2\n  3\n  4\n  5\n  6\n  7\n  8\n  9\n 10\n 11\n 12\n 13\n 14\n 15\n[ CPULongType{16} ]\n```\n\n\n:::\n:::\n\n\n\nWe can reshape this tensor with shape `(16)` to a tensor with shape `(4, 4)`.\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ny <- x$reshape(c(4, 4))\ny\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\ntorch_tensor\n  0   1   2   3\n  4   5   6   7\n  8   9  10  11\n 12  13  14  15\n[ CPULongType{4,4} ]\n```\n\n\n:::\n:::\n\n\n\nWhen `x` is reshaped to `y`, we can *imagine* it as initializing a new tensor of the desired shape and then filling up the rows and columns of the new tensor by iterating over the rows and columns of the old tensor:\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ny2 <- torch_zeros(4, 4)\nfor (j in 1:4) { # columns\n  for (i in 1:4) { # rows\n    y2[i, j] <- y[i, j]\n  }\n}\nsum(abs(y - y2))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\ntorch_tensor\n0\n[ CPUFloatType{} ]\n```\n\n\n:::\n:::\n\n\n\nInternally, this type of reshaping is (in many cases) implemented by changing the *stride* of the tensor without altering the underlying data.\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nx$stride()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 1\n```\n\n\n:::\n\n```{.r .cell-code}\ny$stride()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 4 1\n```\n\n\n:::\n:::\n\n\n\nThe value of the stride indicates how many elements to skip to get to the next element along each dimension:\nIf we move from element `x[1]` (`1`) to element `x[2]` (`2`), we move one index along the columns of `y`.\nIf we move from `x[1]` to `x[5]` (`5`), i.e., 4 steps, we move one index along the rows of `y`.\n\nThis means, for example, that reshaping torch tensors can be considerably more efficient than permuting R arrays, as the latter will always allocate a new, reordered vector, while the former just changes the strides.\n\nThe functionality of strides is illustrated in the image below.\n\n![2D Tensor Strides](../assets/2D_tensor_strides.png){fig-align=\"center\" width=100%}\nSource: [How to Represent a Tensor or ndarray](https://martinlwx.github.io/en/how-to-reprensent-a-tensor-or-ndarray/)\n\n::: {.callout-note}\n## Quiz: Strides\n\n**Question 1**: How do you need to change the strides from a matrix with strides `(4, 1)` to transpose it?\n\n<details>\n<summary>Click for answer</summary>\nThe matrix can be transposed by changing the strides from `(4, 1)` to `(1, 4)`.\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ny$t()$stride()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 1 4\n```\n\n\n:::\n:::\n\n\n\n</details>\n:::\n\nWhen reshaping tensors, you can also infer a dimension by setting it to `-1`:\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nx$reshape(c(-1, 4))$shape\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 4 4\n```\n\n\n:::\n:::\n\n\n\nOf course, not all reshaping operations are valid. The number of elements in the original tensor and the reshaped tensor must be the same:\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nx$reshape(6)\n```\n\n::: {.cell-output .cell-output-error}\n\n```\nError in (function (self, shape) : shape '[6]' is invalid for input of size 16\n```\n\n\n:::\n:::\n\n\n\n## Reference Semantics\n\nOne key property of torch tensors is that they have *reference semantics*. This is different from R, where objects usually have *value semantics*.\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nx <- torch_ones(2)\ny <- x\ny[1] <- 5\nx # was modified\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\ntorch_tensor\n 5\n 1\n[ CPUFloatType{2} ]\n```\n\n\n:::\n:::\n\n\n\nThis differs from R, where objects typically have *value semantics*:\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nx <- c(1, 1)\ny <- x\ny[1] <- 5\nx # was not modified\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 1 1\n```\n\n\n:::\n:::\n\n\n\n\n:::{.callout-note}\nAnother notable exception to values semantics are `R6` classes, which are used in the `mlr3` ecosystem.\n:::\n\nWhen one tensor (`y`) shares underlying data with another tensor (`x`), this is called a *view*. It is also possible to obtain a view on a subset of a tensor, e.g., via slicing:\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nx <- torch_arange(1, 10)\ny <- x[1:3]\ny[1] <- 100\nx[1]\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\ntorch_tensor\n100\n[ CPUFloatType{} ]\n```\n\n\n:::\n:::\n\n\n\nUnfortunately, similar operations might sometimes create a view and sometimes allocate a new tensor. In the example below, we create a subset that is a non-contiguous sequence, and hence a new tensor is allocated:\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nx <- torch_arange(1, 10)\ny <- x[c(1, 3, 5)]\ny[1] <- 100\nx[1]\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\ntorch_tensor\n1\n[ CPUFloatType{} ]\n```\n\n\n:::\n:::\n\n\n\nIf it is important to create a copy of a vector, you can call the `$clone()` method:\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nx <- torch_arange(1, 3)\ny <- x$clone()\ny[1] <- 10\nx[1] # is still 1\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\ntorch_tensor\n1\n[ CPUFloatType{} ]\n```\n\n\n:::\n:::\n\n\n\n::: {.callout-warning}\nThis is also the case for the `$reshape()` methods from the last section, which will in some cases create a view and in other cases allocate a new tensor with the desired shape. If you want to ensure that you create a view on a tensor, you can use the `$view()` method, which will fail if the required view is not possible.\n:::\n\n::: {.callout-note}\n## Quiz: Contiguous Data\n\n**Question 1**: Reshaping a 2D Tensor\n\nConsider the tensor below:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nx1 <- torch_tensor(matrix(1:6, nrow = 2, byrow = FALSE))\nx1\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\ntorch_tensor\n 1  3  5\n 2  4  6\n[ CPULongType{2,3} ]\n```\n\n\n:::\n:::\n\n\n\nWhat is the result of `x1$reshape(6)`, i.e., what are the first, second, ..., sixth elements?\n\n<details>\n<summary>Click for answer</summary>\nThis will result in `(1, 3, 5, 2, 4, 6)` because we (imagine that) first iterate over the rows and then the columns when \"creating\" the new tensor.\n</details>\n:::\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}