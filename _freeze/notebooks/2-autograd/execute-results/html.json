{
  "hash": "d38728d1ba3c7de839a56f9215aca7fc",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Autograd\"\n---\n\n\n\n\n\n\n# Overview\n\nAutomatic differentiation (autograd) is one of torch's key features, enabling the automatic computation of gradients for optimization tasks like training neural networks. Unlike numerical differentiation, which approximates gradients using finite differences, autograd computes exact gradients by tracking operations as they are performed and automatically applying the chain rule of calculus. This makes it possible to efficiently compute gradients of complex functions with respect to many parameters—a critical requirement for training modern neural networks.\n\nAutograd works by building a dynamic computational graph of operations, where each node represents a tensor and each edge represents a mathematical operation.\n\n**Why do we need automatic differentiation?**\n\nIn deep learning, training a model requires iteratively updating parameters to minimize a loss function, which measures the difference between predictions and actual data. These updates depend on calculating gradients of the loss with respect to model parameters, information used by optimization algorithms like stochastic gradient descent (SGD). *Automatic Differentiation* eliminates the need to manually derive these gradients, which is error-prone.\n\n## Enabling Gradient Tracking\n\nTo use autograd, tensors must have their `requires_grad` field set to `TRUE`. This can either be set during tensor construction or changed afterward using the in-place modifier `$requires_grad_(TRUE)`. In the context of deep learning, we track the gradients of the weights of a neural network. The simplest \"neural network\" is a linear model with slope $a$ and bias $b$ and a single input $x$.\n\nThe forward pass is defined as:\n\n$$\\hat{y} = a \\times x + b$$\n\nWe might be interested in how the prediction $\\hat{y}$ changes for the given $x$ when we change the weight $a$ or the bias $b$. We will later use this to adjust the weights $a$ and $b$ to improve predictions, i.e., to perform gradient-based optimization. To write down the gradients, let $u = a \\times x$ denote the intermediate tensor from the linear predictor.\n\n* **Weight $a$**:\n\n  This is expressed by the gradient $\\frac{\\partial \\hat{y}}{\\partial a}$. We can compute the derivative using the chain rule as:\n\n  $$\\frac{\\partial \\hat{y}}{\\partial a} = \\frac{\\partial \\hat{y}}{\\partial u} \\cdot \\frac{\\partial u}{\\partial a} = 1 \\cdot x = x$$\n\n* **Bias $b$**:\n\n  $$\\frac{\\partial \\hat{y}}{\\partial b} = 1$$\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(torch)\n\na <- torch_tensor(2, requires_grad = TRUE)\na$requires_grad\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] TRUE\n```\n\n\n:::\n\n```{.r .cell-code}\nb <- torch_tensor(1, requires_grad = TRUE)\nx <- torch_tensor(3)\n```\n:::\n\n\n\n\nWe can use the weights and input to perform a forward pass:\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nu <- a * x\ny <- u + b\n```\n:::\n\n\n\n\nWhen you perform operations on tensors with gradient tracking, torch builds a computational graph on the fly. In the figure below:\n\n* Blue tensors are those for which we want to calculate gradients.\n* The violet node is an intermediate tensor.\n* The yellow boxes are differentiable functions.\n* The green node is the final tensor with respect to which we want to calculate gradients.\n\n\n\n\n```{mermaid}\ngraph TD\n    a[a] --> mul[Multiply]\n    x[x] --> mul\n    mul --> u[u]\n    u --> add[Add]\n    b[b] --> add\n    add --> y[y]\n\n    %% Gradient flow\n    y_grad[dy/du = 1, dy/db = 1] -.-> y\n    u_grad[du/da = x] -.-> u\n    a_grad[dy/da = x] -.-> a\n    b_grad[dy/db = 1] -.-> b\n\n    %% Styling\n    classDef input fill:#a8d5ff,stroke:#333\n    classDef op fill:#ffe5a8,stroke:#333\n    classDef output fill:#a8ffb6,stroke:#333\n    classDef grad fill:#ffa8a8,stroke:#333,stroke-dasharray:5,5\n    classDef intermediate fill:#d5a8ff,stroke:#333\n    classDef nograd fill:#e8e8e8,stroke:#333\n\n    class a,b input\n    class mul,add op\n    class y output\n    class u intermediate\n    class y_grad,u_grad,a_grad,b_grad grad\n    class x nograd\n```\n\n\n\n\nEach intermediate tensor knows how to calculate gradients with respect to its inputs.\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ny$grad_fn\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nAddBackward0\n```\n\n\n:::\n\n```{.r .cell-code}\nu$grad_fn\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nMulBackward0\n```\n\n\n:::\n:::\n\n\n\n\nTo calculate the gradients $\\frac{\\partial y}{\\partial a}$ and $\\frac{\\partial y}{\\partial b}$, we can traverse this computational graph backward, invoke the differentiation functions, and multiply the individual derivatives according to the chain rule. In `torch`, this is done by calling `$backward()` on `y`.\nNote that `$backward()` can only be called on scalar tensors.\nAfterwards, the gradients are accessible in the `$grad` field of the tensors `a` and `b`:\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Compute gradients\ny$backward()\n\n# Access gradients\nprint(a$grad)  # dy/da = x = 3\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\ntorch_tensor\n 3\n[ CPUFloatType{1} ]\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(b$grad)  # dy/db = 1\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\ntorch_tensor\n 1\n[ CPUFloatType{1} ]\n```\n\n\n:::\n:::\n\n\n\n\nNote that only tensors with `$requires_grad` set to `TRUE` store their gradients. For the intermediate value `u`, no gradient is stored.\n\n:::{.callout-tip}\nWhen you want to perform an operation on tensors that require gradients without tracking this specific operation, you can use `with_no_grad(...)`.\n:::\n\nIn the next section, we will show how we can use gradients to train a simple linear model.\n\n## A Simple Linear Model\n\nWe can use autograd to fit a simple linear regression model. Let's first generate some synthetic data:\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(ggplot2)\n\n# Set random seed for reproducibility\ntorch_manual_seed(42)\n\n# Generate synthetic data\nn <- 100\na_true <- 2.5\nb_true <- 1.0\n\n# Create input X and add noise to output Y\nX <- torch_randn(n)\nnoise <- torch_randn(n) * 0.5\nY <- X * a_true + b_true + noise\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](2-autograd_files/figure-html/unnamed-chunk-8-1.png){fig-align='center' width=672}\n:::\n:::\n\n\n\n\nFirst, we randomly initialize our parameters `a` and `b`.\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Initialize parameters with random values\na <- torch_randn(1, requires_grad = TRUE)\nb <- torch_randn(1, requires_grad = TRUE)\n```\n:::\n\n\n\n\nTo optimize the parameters $a$ and $b$, we need to define the *Loss Function* that quantifies the discrepancy between our predictions $\\hat{y}$ and the observed values $Y$. The standard loss for linear regression is the L2 loss:\n\n$$ L(y, \\hat{y}) = (y - \\hat{y})^2$$\n\nThe graphic below visualizes the relationship between the parameters $a$ and $b$ with the average L2 loss over all datapoints, i.e., the Mean Squared Error (MSE). For parameters $a$ and $b$ that are on the same contour line, the same loss is observed. The color gradient indicates the magnitude of the MSE. In this case, lighter values mark areas with higher loss, and darker values mark areas with lower loss. The red point marks the minimum loss, while the blue point shows the starting values of the parameters.\n\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](2-autograd_files/figure-html/unnamed-chunk-10-1.png){fig-align='center' width=672}\n:::\n:::\n\n\n\n\nWe can optimize the parameters $a$ and $b$ to converge to the minimum by using **gradient descent**. Gradient descent is a fundamental optimization algorithm that helps us find the minimum of a function by iteratively moving in the direction of steepest descent.\n\n## Understanding Gradient Descent\n\nThe gradient of a function points in the direction of the steepest increase—like pointing uphill on mountainous terrain. Therefore, the negative gradient points in the direction of the steepest decrease—like pointing downhill.\n\nGradient descent uses this property to iteratively:\n\n1. Calculate the gradient at the current position.\n2. Take a small step in the opposite direction of the gradient.\n3. Repeat until we reach a minimum.\n\nNote that the gradient only tells us in which direction we have to go, not how far. The length of the step should not be:\n\n- **Too large** because the gradient approximation only holds in a small neighborhood.\n- **Too small** as otherwise the convergence will be slow.\n\nThe general update formula for the weights $a$ and $b$ is:\n\n$$a_{t+1} = a_t - \\eta \\frac{\\partial L}{\\partial a_t}$$\n$$b_{t+1} = b_t - \\eta \\frac{\\partial L}{\\partial b_t}$$\n\nwhere $\\eta$ is the learning rate, and $L$ is the loss function.\n\nIn practice, when dealing with large datasets, computing the gradient over the entire dataset can be computationally expensive.\nInstead, we often use **Stochastic Gradient Descent (SGD)**, where the gradient is estimated using only a few observations (a so called 'batch'), but more on that later.\n\nWe start by implementing a single gradient step. Note that if we repeatedly call `loss$backward()`, the gradients in `a` and `b` would accumulate, so we set them to `0` before performing the update. The return value of the update will be the parameter values and the loss so we can plot them later. Also, note that we mutate the parameters `a` and `b` in-place (suffix `_`).\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nupdate_params <- function(X_batch, Y_batch, lr, a, b) {\n  # Perform forward pass, calculate loss\n  Y_hat <- X_batch * a + b\n  loss <- mean((Y_hat - Y_batch)^2)\n\n  # Calculate gradients\n  loss$backward()\n\n  # We don't want to track gradients when we update the parameters.\n  with_no_grad({\n    a$sub_(lr * a$grad)\n    b$sub_(lr * b$grad)\n  })\n\n  # Ensure gradients are zero\n  a$grad$zero_()\n  b$grad$zero_()\n\n  list(\n    a = a$item(),\n    b = b$item(),\n    loss = loss$item()\n  )\n}\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(data.table)\n\n# Hyperparameters\nlr <- 0.02\nepochs <- 10\nbatch_size <- 10\n\n# Split data into 10 batches of size 10\nbatches <- split(sample(1:100), rep(seq_len(batch_size), length.out = 100))\nhistory <- list()\nfor (epoch in seq_len(epochs)) {\n  for (step in 1:10) {\n    result <- update_params(X[batches[[step]]], Y[batches[[step]]], lr, a, b)\n    history <- append(history, list(as.data.table(result)))\n  }\n}\n\nhistory = rbindlist(history)\n```\n:::\n\n\n\n\nThis example demonstrates how we can use torch's autograd to implement gradient descent for fitting a simple linear regression model. The dashed red lines show the progression of the model during training, with increasing opacity for later steps. The blue line represents the true relationship.\n\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](2-autograd_files/figure-html/unnamed-chunk-13-1.png){fig-align='center' width=672}\n:::\n:::\n\n\n\n\nWe can also visualize the parameter updates over time:\n\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](2-autograd_files/figure-html/unnamed-chunk-14-1.png){fig-align='center' width=672}\n:::\n:::\n\n\n\n\nOf course, better solutions exist for estimating a simple linear model, but this example demonstrates how we can utilize an autograd system to estimate the parameters of a model.\n",
    "supporting": [
      "2-autograd_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}