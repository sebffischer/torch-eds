{
  "hash": "811b36f552832767701703876eac07a8",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Optimizer\"\nsolutions: false\n---\n\n---\ntitle: \"Optimizer Exercises\"\n---\n\n\n\n\n\n\n**Question 1**\n\nIn this exercise, the task is to play around with the settings for the optimization of a neural network.\nWe start by generating some (highly non-linear) synthetic data using the `mlbench` package.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(torch)\ndata <- mlbench::mlbench.friedman3(n = 3000, sd = 0.1)\nX <- torch_tensor(data$x)\nX[1:2, ]\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\ntorch_tensor\n   40.8977   745.3378     0.3715     3.4246\n   88.3017  1148.7073     0.5288     6.5953\n[ CPUFloatType{2,4} ]\n```\n\n\n:::\n\n```{.r .cell-code}\nY <- torch_tensor(data$y)$unsqueeze(2)\nY[1:2, ]\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\ntorch_tensor\n 1.5238\n 1.3572\n[ CPUFloatType{2,1} ]\n```\n\n\n:::\n:::\n\n\nThe associated machine learning task is to predict the output `Y` from the input `X`.\n\nNext, we create a dataset for it using the `tensor_dataset()` function.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nds <- tensor_dataset(X, Y)\nds$.getitem(1)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[[1]]\ntorch_tensor\n  40.8977\n 745.3378\n   0.3715\n   3.4246\n[ CPUFloatType{4} ]\n\n[[2]]\ntorch_tensor\n 1.5238\n[ CPUFloatType{1} ]\n```\n\n\n:::\n:::\n\n\nWe can create two sub-datasets -- for training and validation -- using `dataset_subset()`.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nids_train <- sample(1000, 700)\nids_valid <- setdiff(seq_len(1000), ids_train)\nds_train <- dataset_subset(ds, ids_train)\nds_valid <- dataset_subset(ds, ids_valid)\n```\n:::\n\n\nThe network that we will be fitting is a simple MLP:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nnn_mlp <- nn_module(\"nn_mlp\",\n  initialize = function() {\n    self$lin1 <- nn_linear(4, 50)\n    self$lin2 <- nn_linear(50, 50)\n    self$lin3 <- nn_linear(50, 1)\n  },\n  forward = function(x) {\n    x |>\n      self$lin1() |>\n      nnf_relu() |>\n      self$lin2() |>\n      nnf_relu() |>\n      self$lin3()\n  }\n)\n```\n:::\n\n\nThe code to compare different optimizer configurations is provided through the (provided) `compare_configs()` function.\nIt takes as arguments:\n\n* `epochs`: The number of epochs to train for. Defaults to 30.\n* `batch_size`: The batch size to use for training. Defaults to 16.\n* `lr`: The learning rate to use for training. Defaults to 0.01.\n* `weight_decay`: The weight decay to use for training. Defaults to 0.01.\n* `beta1`: The momentum parameter to use for training. Defaults to 0.9.\n* `beta2`: The adaptive step size parameter to use for training. Defaults to 0.999.\n\nOne of the arguments (except for `epochs`) must be a list of values.\nThe function will then run the same training configuration for each of the values in the list and visualize the results.\n\n<details>\n<summary>Implementation of `compare_configs`</summary>\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(ggplot2)\ncompare_configs <- function(epochs = 30, batch_size = 16, lr = 0.01, weight_decay = 0.01, beta1 = 0.9, beta2 = 0.999) {\n  # Identify which parameter is a list\n  args <- list(batch_size = batch_size, lr = lr, weight_decay = weight_decay, beta1 = beta1, beta2 = beta2)\n  is_list <- sapply(args, is.list)\n\n  if (sum(is_list) != 1) {\n    stop(\"One of the arguments must be a list\")\n  }\n\n  list_arg_name <- names(args)[is_list]\n  list_args <- args[[list_arg_name]]\n  other_args <- args[!is_list]\n\n  # Run train_valid for each value in the list\n  results <- lapply(list_args, function(arg) {\n    network <- with_torch_manual_seed(seed = 123, nn_mlp())\n    other_args[[list_arg_name]] <- arg\n    train_valid(network, ds_train = ds_train, ds_valid = ds_valid, epochs = epochs, batch_size = other_args$batch_size,\n      lr = other_args$lr, betas = c(other_args$beta1, other_args$beta2), weight_decay = other_args$weight_decay)\n  })\n\n  # Combine results into a single data frame\n  combined_results <- do.call(rbind, lapply(seq_along(results), function(i) {\n    df <- results[[i]]\n    df$config <- paste(list_arg_name, \"=\", list_args[[i]])\n    df\n  }))\n\n  upper <- if (max(combined_results$valid_loss) > 10) quantile(combined_results$valid_loss, 0.98) else max(combined_results$valid_loss)\n\n  ggplot(combined_results, aes(x = epoch, y = valid_loss, color = config)) +\n    geom_line() +\n    theme_minimal() +\n    labs(x = \"Epoch\", y = \"Validation RMSE\", color = \"Configuration\") +\n    ylim(min(combined_results$valid_loss), upper)\n}\ntrain_loop <- function(network, dl_train, opt) {\n  network$train()\n  coro::loop(for (batch in dl_train) {\n    opt$zero_grad()\n    Y_pred <- network(batch[[1]])\n    loss <- nnf_mse_loss(Y_pred, batch[[2]])\n    loss$backward()\n    opt$step()\n  })\n}\n\nvalid_loop <- function(network, dl_valid) {\n  network$eval()\n  valid_loss <- c()\n  coro::loop(for (batch in dl_valid) {\n    Y_pred <- with_no_grad(network(batch[[1]]))\n    loss <- sqrt(nnf_mse_loss(Y_pred, batch[[2]]))\n    valid_loss <- c(valid_loss, loss$item())\n  })\n  mean(valid_loss)\n}\n\ntrain_valid <- function(network, ds_train, ds_valid, epochs, batch_size, ...) {\n  opt <- optim_ignite_adamw(network$parameters, ...)\n  train_losses <- numeric(epochs)\n  valid_losses <- numeric(epochs)\n  dl_train <- dataloader(ds_train, batch_size = batch_size)\n  dl_valid <- dataloader(ds_valid, batch_size = batch_size)\n  for (epoch in seq_len(epochs)) {\n    train_loop(network, dl_train, opt)\n    valid_losses[epoch] <- valid_loop(network, dl_valid)\n  }\n  data.frame(epoch = seq_len(epochs), valid_loss = valid_losses)\n}\n```\n:::\n\n\n</details>\n\nYou can e.g. call the function like below:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ncompare_configs(epochs = 30, lr = list(0.1, 0.2), weight_decay = 0.02)\n```\n\n::: {.cell-output-display}\n![](4-optimizer-exercise-task_files/figure-html/unnamed-chunk-7-1.png){fig-align='center' width=672}\n:::\n:::\n\n\nExplore a few hyperparameter settings and make some observations as to how they affect the trajectory of the validation loss.\n\n\n::: {.content-visible when-meta=solutions}\n**Solution**\n\nThere is not really a 'solution' to this exercise.\nWe will go through some of the configurations and explain what is happening.\n\n*Learning rate*\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ncompare_configs(epochs = 30, lr = list(1,  0.1, 0.01, 0.001))\n```\n\n::: {.cell-output-display}\n![](4-optimizer-exercise-task_files/figure-html/unnamed-chunk-8-1.png){fig-align='center' width=672}\n:::\n:::\n\n\n*Batch Size*\n\nFor this configuration, we can see that larger batch sizes lead to considerably better results as they allow for *more* updates for the given number of epochs.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ncompare_configs(epochs = 30, batch_size = list(2, 4, 8, 16, 32, 64))\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: Removed 4 rows containing missing values or values outside the scale range (`geom_line()`).\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](4-optimizer-exercise-task_files/figure-html/unnamed-chunk-9-1.png){fig-align='center' width=672}\n:::\n:::\n\n\n*Weight Decay*\n\nFor too large values of the weight decay, we see that the network struggles to get a good validation loss.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ncompare_configs(epochs = 30, weight_decay = list(0.2, 0.1, 0.05, 0.025, 0.0125))\n```\n\n::: {.cell-output-display}\n![](4-optimizer-exercise-task_files/figure-html/unnamed-chunk-10-1.png){fig-align='center' width=672}\n:::\n:::\n\n\n*Beta 1*\n\nFor the momentum parameter, we can observe that too large values for the momentum parameter lead to oscillations because the local information is not used enough.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ncompare_configs(epochs = 30, beta1 = list(0.5, 0.85, 0.9, 0.95, 0.99))\n```\n\n::: {.cell-output-display}\n![](4-optimizer-exercise-task_files/figure-html/unnamed-chunk-11-1.png){fig-align='center' width=672}\n:::\n:::\n\n\n*Beta 2*\n\nFor larger values of `beta2`, the loss trajectory is considerably smoother.\nWhile smaller values will -- in this scenario -- lead to more oscillations, they also lead to a better fit.\nOnce the value of `beta2` is getting too small, the performance will also deteriorate.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ncompare_configs(epochs = 30, beta2 = list(0.5, 0.6, 0.7, 0.8, 0.9, 0.99, 0.999, 0.9999))\n```\n\n::: {.cell-output-display}\n![](4-optimizer-exercise-task_files/figure-html/unnamed-chunk-12-1.png){fig-align='center' width=672}\n:::\n:::\n\n:::\n\n**Question 2**: Optimization with Momentum\n\nIn this exercise, you will build a gradient descent optimizer with momentum.\nAs a use case, we will minimize the *Rosenbrock* function.\nThe function is defined as:\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nrosenbrock <- function(x, y) {\n  (1 - x)^2 + 2 * (y - x^2)^2\n}\nrosenbrock(torch_tensor(-1), torch_tensor(-1))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\ntorch_tensor\n 12\n[ CPUFloatType{1} ]\n```\n\n\n:::\n:::\n\n\nThe 'parameters' we will be optimizing is the position of a point `(x, y)`, which will both be updated using gradient descent.\nThe figure below shows the Rosenbrock function, where darker values indicate lower values.\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](4-optimizer-exercise-task_files/figure-html/unnamed-chunk-14-1.png){fig-align='center' width=672}\n:::\n:::\n\n\nThe task is to implement the `optim_step()` function.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\noptim_step <- function(x, y, lr, x_momentum, y_momentum, beta) {\n  ...\n}\n```\n:::\n\n\nIt will receive as arguments, the current values `x` and `y`, as well as the momentum values `x_momentum` and `y_momentum` (all scalar tensors).\nThe function should then:\n\n1. Perform a forward pass by calling the `rosenbrock()` function.\n2. Call `$backward()` on the result of the forward pass.\n3. First multiply both momentum values in-place using `$mul_()` with `beta`, then add (`1 - beta`) times the gradient to the momentum values using `$add_()`.\n4. Update the parameters using `$sub_()` with `lr * x_momentum` and `lr * y_momentum`.\n   Perform this update within `with_no_grad()` as we don't want to track the gradients for the parameters.\n\nTo perform in-place updates, you can use the `$mul_()` and `$add_()` methods.\n\nThe update rule is given exemplarily for `x`:\n\n$$\n\\begin{aligned}\nv_{t + 1} &= \\beta v_t + (1 - \\beta) \\nabla_{x} f(x_t, y_t) \\\\\n\\end{aligned}\n$$\n\n\n$$\n\\begin{aligned}\nx_{t+1} &= x_t - \\eta v_{t + 1}\n\\end{aligned}\n$$\n\nTo test your optimizer, you can use the following code:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\noptimize_rosenbrock <- function(steps, lr, beta) {\n  x <- torch_tensor(-1, requires_grad = TRUE)\n  y <- torch_tensor(2, requires_grad = TRUE)\n  momentum_x <- torch_tensor(0)\n  momentum_y <- torch_tensor(0)\n\n  trajectory <- data.frame(\n    x = numeric(steps + 1),\n    y = numeric(steps + 1),\n    value = numeric(steps + 1)\n  )\n  for (step in seq_len(steps)){\n    optim_step(x, y, lr, momentum_x, momentum_y, beta)\n    x$grad$zero_()\n    y$grad$zero_()\n    trajectory$x[step] <- x$item()\n    trajectory$y[step] <- y$item()\n  }\n  trajectory$x[steps + 1] <- x$item()\n  trajectory$y[steps + 1] <- y$item()\n\n  plot_rosenbrock() +\n    geom_path(data = trajectory, aes(x = x, y = y, z = NULL), color = \"red\") +\n    labs(title = \"Optimization Path on Rosenbrock Function\", x = \"X-axis\", y = \"Y-axis\")\n}\n```\n:::\n\n\n::: {.content-visible when-meta=solutions}\n**Solution**\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\noptim_step <- function(x, y, lr, x_momentum, y_momentum, beta) {\n  value <- rosenbrock(x, y)\n  value$backward()\n  x_momentum$mul_(beta)\n  x_momentum$add_((1 - beta) * x$grad)\n  y_momentum$mul_(beta)\n  y_momentum$add_((1 - beta) * y$grad)\n  with_no_grad({\n    x$sub_(lr * x_momentum)\n    y$sub_(lr * y_momentum)\n  })\n  value$item()\n}\noptimize_rosenbrock(steps = 500, lr = 0.01, beta = 0.9)\n```\n\n::: {.cell-output-display}\n![](4-optimizer-exercise-task_files/figure-html/unnamed-chunk-16-1.png){fig-align='center' width=672}\n:::\n:::\n\n:::\n\n**Question 3**: Weight Decay\n\nIn exercise 2, we have optimized the Rosenbrock function.\nDoes it make sense to also use weight decay for this optimization?\n\n::: {.content-visible when-meta=solutions}\n**Solution**\nNo, it does not make sense to use weight decay for this optimization as it is intended to prevent overfitting which is not a problem in this case.\nThis is, because there is no uncertainty in the function we are optimizing.\n:::\n\n",
    "supporting": [
      "4-optimizer-exercise-task_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}