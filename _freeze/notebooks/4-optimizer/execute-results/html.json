{
  "hash": "18a93901598572eb4302e67110af5f08",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Optimizers\"\n---\n\n\n\n\n\n\n## Overview\n\nIn this notebook, we will cover the optimization aspect of deep learning and how to work with optimizers in `torch`. Optimizers are algorithms that iteratively adjust the parameters of a neural network to minimize the loss function during training. They define how the networks *learn* from the data.\n\nLet's denote with $L(\\theta)$ the loss function, which assigns the empirical risk given data $\\{(x_i, y_i)\\}_{i = 1}^n$ to a parameter vector $\\theta$:\n$$L(\\theta) = \\sum_{i=1}^n L(f_\\theta(x_i), y_i)$$\nHere, $f_\\theta$ is the model's prediction function, $x_i$ is the $i$-th sample in the training data, and $y_i$ is the corresponding target value.\n\nThe goal of the optimizer is to find the parameter vector $\\theta^*$ that minimizes the loss function $L(\\theta)$:\n$$\\theta^* = \\arg \\min_\\theta L(\\theta)$$\n\nThis is done by iteratively updating the parameter vector $\\theta$ using the gradient of the loss function with respect to the parameter vector. The simplified update formula for a parameter $\\theta$ at time step $t$ is given by:\n\n$$\\theta_{t+1} = \\theta_t - \\eta \\frac{\\partial L}{\\partial \\theta_t}$$\n\nWhere:\n\n- $\\theta_t$ is the current value of the parameter vector at time step $t$.\n- $\\theta_{t+1}$ is the new value of the parameter after the update.\n- $\\eta$ (eta) is the learning rate, which controls how big of a step we take.\n- $\\frac{\\partial L}{\\partial \\theta_t}$ is the derivative of the loss function $L$ with respect to parameter $\\theta$, i.e., the gradient.\n\n::: {.callout-note}\n## Quiz: Learning Rate\n\nQuestion 1: Can you explain what happens when the learning rate is too high? What happens when it is too low?\n\n<details>\n<summary>Click for answer</summary>\nA too high learning rate will cause the parameters to overshoot the minimum and diverge. A too low learning rate will cause the parameters to converge slowly.\n![](../assets/lr_size.png)\nSource: https://stackoverflow.com/questions/62690725/small-learning-rate-vs-big-learning-rate\n</details>\n:::\n\nThe optimizers used in practice differ from the above formula, as:\n\n1. The gradient is estimated from a batch rather than the entire training dataset.\n2. The simplistic update formula is extended with:\n   - Weight decay\n   - Momentum\n   - Adaptive learning rates\n\nBefore we cover these more advanced approaches (specifically their implementation in AdamW), we will first focus on the vanilla version of Stochastic Gradient Descent (SGD).\n\n## Mini-Batch Effects in SGD\n\nWhen using mini-batches, the gradient becomes a noisy estimate of the gradient over the full dataset. With $\\nabla L^i_t := \\frac{\\partial L^i}{\\partial \\theta_t}$ being the gradient of the loss function with respect to the entire parameter vector estimated using $(x_i, y_i)$, the mini-batch gradient is given by:\n\n$$\\nabla L^B_t = \\frac{1}{|B|} \\sum_{i \\in B} \\nabla L^i_t$$\n\nwhere $B$ is the batch of samples and $|B|$ is the batch size.\n\nThe update formula for SGD is then given by:\n\n$$\\theta_{t+1} = \\theta_t - \\eta \\nabla L^B_t$$\n\nThis is visualized in the image below:\n\n![](../assets/gd_vs_sgd.png)\n\n::: {.callout-note}\n## Quiz: Vanilla SGD\n\n**Question 1:** What happens when the batch size is too small or too large?\n\n<details>\n<summary>Click for answer</summary>\n**Trade-offs with Batch Size**:\n\n- Larger batches provide more accurate gradient estimates.\n- Smaller batches introduce more noise but allow more frequent parameter updates.\n![](../assets/lr_size.png)\n</details>\n\n**Question 2:** The mini-batch gradient is an approximation of the gradient over the full dataset. Does the latter also approximate something? If so, what?\n\n<details>\n<summary>Click for answer</summary>\nIn machine learning, we assume that the data is drawn from a distribution $P$. The gradient over the full dataset is an expectation over this distribution:\n\n$$\\nabla L = \\mathbb{E}_{x \\sim P} \\nabla L(f_\\theta(x), y)$$\n\nThe mini-batch gradient is an empirical estimate of this gradient, i.e., the expectation over a finite sample from the distribution.\n</details>\n:::\n\nBecause deep learning models can have many parameters and computing gradients is expensive, understanding the effects of different batch sizes and convergence is important. The computational cost (which we define as the time it takes to perform one optimization step) of a gradient update using a batch size $b$ consists of:\n\n1. Loading the batch into memory (if the data does not fit into RAM).\n2. The forward pass of the model.\n3. The backward pass of the model.\n4. The update of the parameters.\n\nWe will discuss point 1 later, and point 4 does not depend on the batch size, so we can ignore it.\n\n::: {.callout-note}\n## Quiz: Bang for Your Buck\n\n**Question 1:**\nTrue or False: The cost of performing a gradient update using a batch size of $2$ is twice the cost of a batch size of $1$.\n\n<details>\n<summary>Click for answer</summary>\nFalse. Because GPUs can perform many operations simultaneously, the cost of performing a gradient update using a batch size of $2$ is not twice the cost of a batch size of $1$.\n</details>\n\n**Question 2:**\nThe standard error of the mini-batch gradient estimate (which characterizes the precision of the gradient estimate) can be written as:\n\n$$\\text{SE}_{\\nabla L^B_t} = \\frac{\\sigma_{\\nabla L_t}}{\\sqrt{|B|}}$$\n\nwhere $\\sigma_{\\nabla L_t}$ is the standard deviation of the gradient estimate relative to the batch size.\n\nDescribe the dynamics of the standard error when increasing the batch size: How do you need to increase a batch size from $1$ to achieve half the standard error? What about increasing a batch size from $100$?\n\n<details>\n<summary>Click for answer</summary>\nThe standard error decreases as the batch size increases, but with diminishing returns. To halve the standard error:\n\n- Increase the batch size from $1$ to $4$.\n- Increase the batch size from $100$ to $400$.\n\nThis is because the standard error is inversely proportional to the square root of the batch size.\n</details>\n\n:::\n\n## Mini-Batch Gradient Descent: It's not all about runtime\n\nAs we have now covered some of the dynamics of a simple gradient-based optimizer, we can examine the final parameter vector $\\theta^*$ that the optimizer converges to. When using a gradient-based optimizer, the updates will stop once the gradient is close to zero. We will now discuss the type of solutions where this is true and their properties.\n\nWe need to distinguish *saddle points* from *local minima* from *global minima*:\n\n![](../assets/minimum_vs_saddlepoint.png)\n\nIn deep learning, where high-dimensional parameter spaces are common, saddle points are more likely to occur than local minima [@dauphin2014identifying]. However, due to the stochastic nature of SGD, optimizers will find local minima instead of saddle points @pmlr-v80-daneshmand18a.\n\n::: {.callout-note}\n## Quiz: Local vs. Global Minima, Generalization\n\nQuestion 1: Do you believe SGD will find local or global minima? Explain your reasoning.\n\n<details>\n<summary>Click for answer</summary>\nBecause the gradient only has **local** information about the loss function, SGD finds local minima.\n</details>\n\nQuestion 2: Assuming we have found a $\\theta^*$ that has low training loss, does this ensure that we have found a good model?\n\n<details>\n<summary>Click for answer</summary>\nNo, because we only know that the model has low training loss, but not the test loss.\n</details>\n:::\n\nSGD has been empirically shown to find solutions that generalize well to unseen data. This phenomenon is attributed to the implicit regularization effects of SGD, where the noise introduced by mini-batch sampling helps guide the optimizer towards broader minima with smaller L2 norms. These broader minima are typically associated with better generalization performance compared to sharp minima.\n\n![](../assets/flat_minima_generalization.png)\n\nSource: https://www.researchgate.net/figure/Flat-minima-results-in-better-generalization-compared-to-sharp-minima-Pruning-neural_fig2_353068686\n\nThese properties are also known as *implicit regularization* of SGD. Regularization generally refers to techniques that prevent overfitting and improve generalization. There are also explicit regularization techniques, which we will cover next.\n\n### Weight Decay\n\nBecause weight decay in SGD is equivalent to adding a regularization penalty term to the loss function, we can draw a parallel to the regularization techniques used in statistics such as ridge regression. Regularization in machine learning/statistics is used to prevent overfitting by adding a penalty term to the model's loss function, which discourages overly complex models that might fit noise in the training data. It helps improve generalization to unseen data. For example, in ridge regression, the regularization term penalizes large coefficients by adding the squared magnitude of the coefficients to the loss function:\n\n$$\n\\mathcal{L}(y, \\hat{y}) = \\sum_{i=1}^n \\left(y_i - \\hat{y}_i\\right)^2 + \\lambda \\sum_{j=1}^p \\beta_j^2\n$$\n\nThis will make the model prefer less complex solutions, where complexity is measured by the L2 norm of the coefficients.\n\n:::{.callout-note}\nFor more complex optimizers such as Adam, weight decay is not equivalent to adding a regularization penalty term to the loss function. However, the main idea of both approaches is still to shrink the weights to $0$ during training.\n:::\n\nHere, $\\lambda$ controls the strength of the regularization, $y_i$ are the observed values, $\\hat{y}_i$ are the predicted values, and $w_i$ are the model coefficients.\n\nIf we integrate weight decay into the gradient update formula, we get the following:\n\n$$\\theta_{t+1} = \\theta_t - \\eta \\big(\\frac{\\partial L}{\\partial \\theta_t} - \\lambda \\theta_t\\big)$$\n\nThis formula shows that the weight decay term ($- \\lambda \\theta_t$) effectively shrinks the weights during each update, helping to prevent overfitting.\n\n![](../assets/regularization.png)\n\n## Momentum\n\nMomentum is a technique that helps accelerate gradient descent by using an exponential moving average of past gradients. Like a ball rolling down a hill, momentum helps the optimizer:\n\n- Move faster through areas of consistent gradient direction.\n- Push through sharp local minima and saddle points.\n- Dampen oscillations in areas where the gradient frequently changes direction.\n\nThe exponential moving momentum update can be expressed mathematically as:\n\n$$\n(1 - \\beta) \\sum_{\\tau=1}^{t} \\beta^{t-\\tau} \\nabla_{\\theta} \\mathcal{L}(\\theta_{\\tau-1})\n$$\n\nIn order to avoid having to keep track of all the gradients, we can calculate the update in two steps as follows:\n\n$$\nv_t = \\beta_1 v_{t-1} + (1 - \\beta_1) \\nabla_\\theta L(\\theta_t)\n$$\n\n$$\n\\theta_{t+1} = \\theta_t - \\eta \\frac{v_t}{1 - \\beta_1^t}\n$$\n\nThe hyperparameter $\\beta_1$ is the momentum decay rate (typically 0.9), $v_t$ is the exponential moving average of gradients, and $\\eta$ is the learning rate as before. Note that dividing by $1 - \\beta_1^t$ counteracts a bias because $v_0$ is initialized to $0$.\n\n![](../assets/momentum.png)\n\n## Adaptive Learning Rates\n\nAdaptive learning rate methods automatically adjust the learning rate for each parameter during training. This is particularly useful because:\n\n1. Different parameters may require different learning rates.\n2. The optimal learning rate often changes during training.\n\nBefore, we had one global learning rate $\\eta$ for all parameters. However, learning rates are now allowed to:\n\n1. Change over time.\n2. Be different for different parameters.\n\nOur vanilla SGD update formula is now generalized to handle adaptive learning rates:\n\n$$\\theta_{t+1} = \\theta_t - \\eta_t \\cdot \\frac{\\nabla_\\theta L(\\theta_t)}{\\sqrt{v_t} + \\epsilon}$$\n\nHere, $\\eta_t$ is now not a scalar learning rate, but a vector of learning rates for each parameter, and '$\\cdot$' denotes the element-wise multiplication. Further, $\\epsilon$ is a small constant for numerical stability.\n\nIn AdamW, the adaptive learning rate is controlled by the second moment estimate (squared gradients):\n\n$$v_t = \\beta_2 v_{t-1} + (1-\\beta_2)(g_t)^2$$\n$$\\hat{\\eta}_t = \\eta \\frac{1}{\\sqrt{v_t + \\epsilon}}$$\n\nIn words, this means: In steep directions where the gradient is large, the learning rate is small and vice versa. The parameters $\\beta_2$ and $\\epsilon$ are hyperparameters that control the decay rate and numerical stability of the second moment estimate.\n\n![](../assets/adagrad.png)\n\nWhen combining weight decay, adaptive learning rates, and momentum, we get the AdamW optimizer. It therefore has parameters:\n\n* `lr`: The learning rate.\n* `weight_decay`: The weight decay parameter.\n* `betas`: The momentum parameters ($\\beta_1$ and $\\beta_2$).\n* `eps`: The numerical stability parameter.\n\nNote that AdamW also has another configuration parameter `amsgrad`, which is disabled by default in `torch`, but which can help with convergence.\n\n# Optimizers in torch\n\n`torch` provides several common optimizers, including SGD, Adam, AdamW, RMSprop, and Adagrad. The main optimizer API consists of:\n\n1. Initializing the optimizer, which requires passing the parameters of the module to be optimized and setting the optimizer's hyperparameters such as the learning rate.\n2. `step()`: Update parameters using current gradients.\n3. `zero_grad()`: Reset gradients of all the parameters to zero before each backward pass.\n4. Just like `nn_module`s, they have a `$state_dict()` which can, for example, be saved to later load it using `$load_state_dict()`.\n\nWe will focus on the AdamW optimizer, but the others work analogously.\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(torch)\nformals(optim_adamw)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n$params\n\n\n$lr\n[1] 0.001\n\n$betas\nc(0.9, 0.999)\n\n$eps\n[1] 1e-08\n\n$weight_decay\n[1] 0.01\n\n$amsgrad\n[1] FALSE\n```\n\n\n:::\n:::\n\n\n\n\nTo construct it, we first need to create a model and then pass the parameters of the model to the optimizer so it knows which parameters to optimize.\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nmodel = nn_linear(1, 1)\nopt <- optim_adamw(model$parameters, lr = 0.2)\n```\n:::\n\n\n\n\nTo illustrate the optimizer, we will again generate some synthetic training data:\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ntorch_manual_seed(1)\nX = torch_randn(1000, 1)\nbeta = torch_randn(1, 1)\nY = X * beta + torch_randn(1000, 1) * 2\n```\n:::\n\n\n\n\nThis represents data from a simple linear model with some noise:\n\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](4-optimizer_files/figure-html/unnamed-chunk-5-1.png){fig-align='center' width=672}\n:::\n:::\n\n\n\n\nPerforming a (full) gradient update using the AdamW optimizer consists of:\n\n1. Calculating the forward pass\n\n\n\n\n   ::: {.cell layout-align=\"center\"}\n   \n   ```{.r .cell-code}\n   y_hat = model(X)\n   ```\n   :::\n\n\n\n2. Calculating the loss\n\n\n\n\n   ::: {.cell layout-align=\"center\"}\n   \n   ```{.r .cell-code}\n   loss = mean((y_hat - Y)^2)\n   ```\n   :::\n\n\n\n\n3. Performing a backward pass\n\n\n\n\n   ::: {.cell layout-align=\"center\"}\n   \n   ```{.r .cell-code}\n   loss$backward()\n   ```\n   :::\n\n\n\n\n4. Applying the update rule\n\n\n\n\n   ::: {.cell layout-align=\"center\"}\n   ::: {.cell-output .cell-output-stdout}\n   \n   ```\n   NULL\n   ```\n   \n   \n   :::\n   :::\n\n\n\n\nNote that after the optimizer step, the gradients are not reset to zero but are unchanged.\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nmodel$weight$grad\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\ntorch_tensor\n 0.5993\n[ CPUFloatType{1,1} ]\n```\n\n\n:::\n:::\n\n\n\n\nIf we were to perform another backward pass, the gradient would be added to the current gradient. If this is not desired, we can set an individual gradient of a tensor to zero:\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nmodel$weight$grad$zero_()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\ntorch_tensor\n 0\n[ CPUFloatType{1,1} ]\n```\n\n\n:::\n:::\n\n\n\n\nOptimizers also offer a convenient way to set all gradients of the parameters managed by them to zero using `$zero_grad()`:\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nopt$zero_grad()\nmodel$weight$grad\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\ntorch_tensor\n 0\n[ CPUFloatType{1,1} ]\n```\n\n\n:::\n:::\n\n\n\n\n::: {.callout-note}\n## Quiz: Guess which Parameter is Varied\n\nWe will now show some real trajectories of the AdamW optimizer applied to the linear regression problem from above where one specific parameter is varied. Recall that:\n\n* $\\eta$: The learning rate controls the step size of the optimizer.\n* $\\lambda$: The weight decay parameter controls the bias of the optimization towards a parameter being close to zero. A value of $0$ means no weight decay.\n* $\\beta_1$: The momentum parameter. A value of $0$ means no momentum.\n* $\\beta_2$: The second moment parameter. A value of $0$ means no second moment adjustment.\n\nThe plots below show contour lines of the empirical loss function, i.e., two values that are on the same contour line have the same loss.\n\nQuestion 1: Which parameter is varied here? Explain your reasoning.\n\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](4-optimizer_files/figure-html/unnamed-chunk-13-1.png){fig-align='center' width=768}\n:::\n:::\n\n\n\n<details>\n<summary>Click for answer</summary>\nThe learning rate is varied. This can be seen as the gradient updates for the right trajectory are larger than for the left trajectory.\n</details>\n\nQuestion 2: Which parameter is varied below? Explain your reasoning.\n\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](4-optimizer_files/figure-html/unnamed-chunk-14-1.png){fig-align='center' width=768}\n:::\n:::\n\n\n\n<details>\n<summary>Click for answer</summary>\nThe weight decay is varied. We can see this as the final parameter value for the right trajectory is closer to zero than for the left trajectory.\n</details>\n\nQuestion 3: Which parameter is varied below? Explain your reasoning. Can you explain why this happens?\n\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](4-optimizer_files/figure-html/unnamed-chunk-15-1.png){fig-align='center' width=768}\n:::\n:::\n\n\n\n<details>\n<summary>Click for answer</summary>\nThe momentum parameter $\\beta_1$ is varied. There is no momentum on the left side, so the gradient steps are more noisy. On the right side, the momentum is set to $0.9$, so over time, momentum in the 'correct' direction is accumulated.\n</details>\n\nQuestion 4: Which parameter is varied below? Explain your reasoning.\n\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](4-optimizer_files/figure-html/unnamed-chunk-16-1.png){fig-align='center' width=768}\n:::\n:::\n\n\n\n\n<details>\n<summary>Click for answer</summary>\nThe $\\beta_2$ parameter is varied. There is no second moment adjustment on the left side, but there is on the right side. Because the gradients in the direction of the bias are larger than in the direction of the weight, the second moment adjustment helps to reduce the learning rate in the direction of the bias.\n</details>\n:::\n\n## Learning Rate Schedules\n\nWhile we have already covered dynamic learning rates, it can still be beneficial to use a **learning rate scheduler** to further improve convergence. There, the learning rate is not a constant scalar, but a function of the current epoch. The update formula for the simple SGD optimizer is now:\n\n$$\\theta_{t+1} = \\theta_t - \\eta_t \\cdot \\frac{\\nabla_\\theta L(\\theta_t)}{\\sqrt{v_t} + \\epsilon}$$\n\n**Decaying learning rates**:\n\nThis includes gradient decay, cosine annealing, and cyclical learning rates. The general idea is to start with a high learning rate and then gradually decrease it over time.\n\n**Warmup**:\n\nWarmup is a technique that gradually increases the learning rate from a small value to a larger value over a specified number of epochs. For an explanation of why warmup is beneficial, see @kalra2024warmup.\n\n**Cyclical Learning Rates**:\n\nCyclical learning rates are a technique that involves periodically increasing and decreasing the learning rate. This can help the optimizer to traverse saddle points faster and find better solutions.\n\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](4-optimizer_files/figure-html/unnamed-chunk-17-1.png){fig-align='center' width=672}\n:::\n:::\n\n\n\n\nIn `torch`, learning rate schedulers are prefixed by `lr_`, such as the simple `lr_step`, where the learning rate is multiplied by a factor of `gamma` every `step_size` epochs. In order to use them, we need to pass the optimizer to the scheduler and specify additional arguments.\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nscheduler = lr_step(opt, step_size = 2, gamma = 0.1)\n```\n:::\n\n\n\n\nThe main API of a learning rate scheduler is the `$step()` method, which updates the learning rate. For some schedulers, this needs to be called after each optimization step, for others after each epoch. You can find this out by consulting the documentation of the specific scheduler.\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nopt$param_groups[[1L]]$lr\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.2\n```\n\n\n:::\n\n```{.r .cell-code}\nscheduler$step()\nopt$param_groups[[1L]]$lr\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.2\n```\n\n\n:::\n\n```{.r .cell-code}\nscheduler$step()\nopt$param_groups[[1L]]$lr\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.02\n```\n\n\n:::\n:::\n\n\n\n\n## Setting the Learning Rate\n\nArguably the most important hyperparameter is the learning rate. While we have now discussed the dynamics of the optimizer hyperparameters, the primary practical concern is how to set them. As a start, one can see whether good results can be achieved with the default hyperparameters. Alternatively, one can look at how others (e.g., in scientific papers) have set the hyperparameters for similar architectures and tasks.\n\nWhen setting the learning rate, it is a good idea to then inspect the loss over time to see whether the learning rate is too high (instability) or too low (slow convergence). Below, we show the learning curve for two different learning rates.\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(mlr3torch)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nLoading required package: mlr3\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nLoading required package: mlr3pipelines\n```\n\n\n:::\n\n```{.r .cell-code}\nepochs = 40\nl1 = lrn(\"classif.mlp\", batch_size = 32, epochs = epochs, opt.lr = 0.01, callbacks = t_clbk(\"history\"), measures_train = msr(\"classif.logloss\"), predict_type = \"prob\", neurons = 100)\nl2 = lrn(\"classif.mlp\", batch_size = 32, epochs = epochs, opt.lr = 0.001, callbacks = t_clbk(\"history\"), measures_train = msr(\"classif.logloss\"), predict_type = \"prob\", neurons = 100)\ntask = tsk(\"spam\")\nl1$train(task)\nl2$train(task)\n\nd = data.frame(\n  epoch = rep(1:epochs, times = 2),\n  logloss = c(l1$model$callbacks$history$train.classif.logloss, l2$model$callbacks$history$train.classif.logloss),\n  lr = rep(c(\"0.01\", \"0.001\"), each = epochs)\n)\n\n\nggplot(d, aes(x = epoch, y = logloss, color = lr)) +\n  geom_line() +\n  theme_minimal()\n```\n\n::: {.cell-output-display}\n![](4-optimizer_files/figure-html/unnamed-chunk-20-1.png){fig-align='center' width=672}\n:::\n:::\n\n\n\n\nWhen no good results can easily be achieved with defaults or with learning rates from the literature, one can employ hyperparameter optimization to find good learning rates. It is recommended to tune the learning rate on a logarithmic scale.\n\n## Saving an Optimizer\n\nIn order to resume training at a later stage, we can save the optimizer's state using `$state_dict()`.\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nstate_dict = opt$state_dict()\n```\n:::\n\n\n\n\nThis state dictionary contains:\n\n1. The `$param_groups` which contains the parameters and their associated hyperparameters.\n2. The `$state` which contains the optimizer's internal state, such as the momentum and second moment estimates.\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nstate_dict$param_groups[[1L]]\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n$params\n[1] 1 2\n\n$lr\n[1] 0.02\n\n$betas\n[1] 0.900 0.999\n\n$eps\n[1] 1e-08\n\n$weight_decay\n[1] 0.01\n\n$amsgrad\n[1] FALSE\n\n$initial_lr\n[1] 0.2\n```\n\n\n:::\n:::\n\n\n\n\n::: {.callout-note}\nIt is possible to set different parameters (such as learning rate) for different parameter groups.\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\no2 = optim_adamw(list(\n  list(params = torch_tensor(1), lr = 1),\n  list(params = torch_tensor(2), lr = 2)\n))\no2$param_groups[[1L]]$lr\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 1\n```\n\n\n:::\n\n```{.r .cell-code}\no2$param_groups[[2L]]$lr\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 2\n```\n\n\n:::\n:::\n\n\n\n:::\n\nThe `$state` field contains the state for each parameter, which is currently empty as we have not performed any updates yet.\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nstate_dict$state\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n$`1`\n$`1`$step\ntorch_tensor\n1\n[ CPUFloatType{} ]\n\n$`1`$exp_avg\ntorch_tensor\n0.01 *\n 5.9926\n[ CPUFloatType{1,1} ]\n\n$`1`$exp_avg_sq\ntorch_tensor\n0.0001 *\n 3.5912\n[ CPUFloatType{1,1} ]\n\n\n$`2`\n$`2`$step\ntorch_tensor\n1\n[ CPUFloatType{} ]\n\n$`2`$exp_avg\ntorch_tensor\n0.01 *\n 2.1779\n[ CPUFloatType{1} ]\n\n$`2`$exp_avg_sq\ntorch_tensor\n1e-05 *\n 4.7433\n[ CPUFloatType{1} ]\n```\n\n\n:::\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nstep = function() {\n  opt$zero_grad()\n  ((model(torch_tensor(1)) - torch_tensor(1))^2)$backward()\n  opt$step()\n}\nreplicate(step(), n = 2)\n```\n:::\n\n\n\n\nAfter performing two steps, the state dictionary contains the state for each parameter:\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nopt$state_dict()$state[[\"1\"]]\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n$step\ntorch_tensor\n3\n[ CPUFloatType{} ]\n\n$exp_avg\ntorch_tensor\n-0.6202\n[ CPUFloatType{1,1} ]\n\n$exp_avg_sq\ntorch_tensor\n0.01 *\n 2.5144\n[ CPUFloatType{1,1} ]\n```\n\n\n:::\n:::\n\n\n\n\nJust like for the `nn_module`, we can save the optimizer state using `torch_save()`.\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\npth = tempfile(fileext = \".pth\")\ntorch_save(state_dict, pth)\n```\n:::\n\n\n\n\n::: {.callout-warning}\nGenerally, we don't want to save the whole optimizer, as this also contains the weight tensors of the model that one usually wants to save separately.\n:::\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nstate_dict2 = torch_load(pth)\nopt2 <- optim_adamw(model$parameters, lr = 0.2)\nopt2$load_state_dict(state_dict2)\n```\n:::\n",
    "supporting": [
      "4-optimizer_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}