{
  "hash": "2a42378a008778854eaaaed91e8a7bd5",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Training Neural Networks with mlr3torch\"\n---\n\n\n\n\n\n\n## Why Use `mlr3torch`?\n\n`mlr3torch` is a package that extends the `mlr3` framework with deep learning capabilities, allowing the application of deep learning techniques to both tabular and non-tabular data. The package implements many routines common in deep learning and allows users to focus on the actual problem at hand. Some advantages of using `mlr3torch` over 'only' `torch` are:\n\n- **Less Code**: Avoid writing repetitive boilerplate code by utilizing predefined network architectures or easily building custom ones tailored to your specific needs.\n\n- **mlr3 Integration**: Especially for users with experience in the `mlr3` framework, working with `mlr3torch` should feel familiar. Due to the integration into the `mlr3` framework, many `mlr3` features like hyperparameter tuning, preprocessing, and resampling are readily available for `mlr3torch`.\n\nHowever, as `mlr3torch` is a framework, it is less flexible than `torch` itself, so knowing both is recommended. Another helpful R package that provides many useful functions to train neural networks is [`luz`](https://mlverse.github.io/luz/).\n\n## `mlr3` Recap\n\nBefore diving into `mlr3torch`, we will briefly review the core building blocks of the `mlr3` machine learning framework. For reference, we recommend the [mlr3 book](https://mlr3book.mlr-org.com/) that explains the `mlr3` framework in more detail. Additionally, the [mlr3 website](https://mlr-org.com/) contains more tutorials and overviews.\n\n### Task\n\nA task is a machine learning problem on a dataset. It consists of the data itself and some metadata such as the features or the target variable. To create an example task that comes with `mlr3`, we can use the `tsk()` function:\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(mlr3)\ntsk(\"iris\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n<TaskClassif:iris> (150 x 5): Iris Flowers\n* Target: Species\n* Properties: multiclass\n* Features (4):\n  - dbl (4): Petal.Length, Petal.Width, Sepal.Length, Sepal.Width\n```\n\n\n:::\n:::\n\n\n\nTo create a custom `Task` from a `data.frame`, we can use the `as_task_<type>` converters:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nhead(iris)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n1          5.1         3.5          1.4         0.2  setosa\n2          4.9         3.0          1.4         0.2  setosa\n3          4.7         3.2          1.3         0.2  setosa\n4          4.6         3.1          1.5         0.2  setosa\n5          5.0         3.6          1.4         0.2  setosa\n6          5.4         3.9          1.7         0.4  setosa\n```\n\n\n:::\n\n```{.r .cell-code}\ntsk_iris <- as_task_classif(iris, id = \"iris\", target = \"Species\")\ntsk_iris\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n<TaskClassif:iris> (150 x 5)\n* Target: Species\n* Properties: multiclass\n* Features (4):\n  - dbl (4): Petal.Length, Petal.Width, Sepal.Length, Sepal.Width\n```\n\n\n:::\n:::\n\n\n\n::: {.callout-tip}\nTo get the help page for an `mlr3` object, you can call `tsk_iris$help()`.\n:::\n\nYou can access the data of a task using the `$data()` method, which accepts arguments `rows` and `cols` to select specific rows and columns.\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ntsk_iris$data(rows = 1:5, cols = c(\"Sepal.Length\", \"Sepal.Width\"))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   Sepal.Length Sepal.Width\n          <num>       <num>\n1:          5.1         3.5\n2:          4.9         3.0\n3:          4.7         3.2\n4:          4.6         3.1\n5:          5.0         3.6\n```\n\n\n:::\n:::\n\n\n\nUsing the `mlr3viz` extension, we can get an overview of the task:\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(mlr3viz)\nautoplot(tsk_iris)\n```\n\n::: {.cell-output-display}\n![](5-mlr3torch_files/figure-html/unnamed-chunk-5-1.png){fig-align='center' width=672}\n:::\n:::\n\n\n\n### Learner\n\nA learner is a machine learning algorithm that can be `$train()`ed on a `Task` and `$predict()`ed on a `Task`. An overview of all learners is shown on our [website](https://mlr-org.com/learners.html). We can construct one by passing the name of the learner to the `lrn()` function.\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlrn_tree <- lrn(\"classif.rpart\")\n```\n:::\n\n\n\nNext, we need to split the data into a training and test set and apply the learner on the former.\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nsplit <- partition(tsk_iris, ratio = 0.8)\nlrn_tree$train(tsk_iris, row_ids = split$train)\n```\n:::\n\n\n\nThe trained model can be accessed via the `$model` slot of the learner:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nprint(lrn_tree$model)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nn= 120 \n\nnode), split, n, loss, yval, (yprob)\n      * denotes terminal node\n\n1) root 120 75 virginica (0.33333333 0.29166667 0.37500000)  \n  2) Petal.Length< 2.45 40  0 setosa (1.00000000 0.00000000 0.00000000) *\n  3) Petal.Length>=2.45 80 35 virginica (0.00000000 0.43750000 0.56250000)  \n    6) Petal.Length< 4.75 32  1 versicolor (0.00000000 0.96875000 0.03125000) *\n    7) Petal.Length>=4.75 48  4 virginica (0.00000000 0.08333333 0.91666667) *\n```\n\n\n:::\n:::\n\n\n\nTo make predictions on the test set, we can use the `$predict()` method of the learner:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\npredictions <- lrn_tree$predict(tsk_iris, row_ids = split$test)\n```\n:::\n\n\n\nTo make predictions on `data.frame`s, we can use the `$predict_newdata()` method of the learner:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nnew_data <- iris[1:2, ]\nlrn_tree$predict_newdata(new_data)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n<PredictionClassif> for 2 observations:\n row_ids  truth response\n       1 setosa   setosa\n       2 setosa   setosa\n```\n\n\n:::\n:::\n\n\n\n### Performance Evaluation\n\nTo assess the quality of the predictions, we can use a `Measure`. `mlr3` comes with many predefined measures, and we can construct them by passing the name of the measure to the `msr()` function. Below, we construct the mean classification accuracy measure -- which can only be applied to classification tasks -- and use it to evaluate the predictions.\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nacc <- msr(\"classif.acc\")\npredictions$score(acc)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nclassif.acc \n  0.9333333 \n```\n\n\n:::\n:::\n\n\n\nFor more elaborate evaluation strategies, we can use `rsmp()` to define a `Resampling` strategy that can be executed using `resample()`.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nrsmp_cv <- rsmp(\"cv\", folds = 3)\n\nrr <- resample(\n  task       = tsk_iris,\n  learner    = lrn_tree,\n  resampling = rsmp_cv\n)\n\n# Aggregate the results\nrr$aggregate(msr(\"classif.acc\"))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nclassif.acc \n  0.9466667 \n```\n\n\n:::\n:::\n\n\n\n### Hyperparameter Tuning\n\nHyperparameter tuning is an essential process in machine learning to optimize the performance of models by selecting the best combination of hyperparameters. In the `mlr3` framework, hyperparameter tuning is facilitated by the `mlr3tuning` extension, which provides a flexible and powerful interface for defining, searching, and evaluating hyperparameters.\n\n#### Key Concepts\n\n- **Hyperparameters**: Configurable settings provided to the learning algorithm before training begins, such as learning rate, number of trees in a random forest, or regularization parameters.\n\n- **Search Space**: The range of values or distributions from which hyperparameters are sampled during the tuning process.\n\n- **Resampling Strategy**: A method to evaluate the performance of a hyperparameter configuration, commonly using techniques like cross-validation or bootstrapping.\n\n- **Tuner**: An algorithm that explores the search space to find the optimal hyperparameters. Common tuners include grid search, random search, and Bayesian optimization.\n\n#### Workflow\n\n1. **Define the Search Space**: Specify the range and distribution of hyperparameters to explore.\n\n\n\n   ::: {.cell layout-align=\"center\"}\n   \n   ```{.r .cell-code}\n   library(mlr3tuning)\n   ```\n   \n   ::: {.cell-output .cell-output-stderr}\n   \n   ```\n   Loading required package: paradox\n   ```\n   \n   \n   :::\n   \n   ```{.r .cell-code}\n   lrn_tree$configure(\n     cp = to_tune(lower = 0.001, upper = 0.1),\n     maxdepth = to_tune(lower = 1, upper = 30)\n   )\n   ```\n   :::\n\n\n\n2. **Choose a Resampling Strategy**: Determine how to evaluate each hyperparameter configuration's performance.\n\n\n\n   ::: {.cell layout-align=\"center\"}\n   \n   ```{.r .cell-code}\n   rsmp_tune <- rsmp(\"cv\", folds = 3)\n   ```\n   :::\n\n\n\n3. **Select a Tuner**: Decide on the algorithm that will search through the hyperparameter space.\n\n\n\n   ::: {.cell layout-align=\"center\"}\n   \n   ```{.r .cell-code}\n   tuner <- tnr(\"random_search\")\n   ```\n   :::\n\n\n\n4. **Select a Measure**: Define the metric to optimize during tuning.\n\n\n\n   ::: {.cell layout-align=\"center\"}\n   \n   ```{.r .cell-code}\n   msr_tune <- msr(\"classif.acc\")\n   ```\n   :::\n\n\n\n5. **Execute Tuning**: Run the tuning process to find the optimal hyperparameters. Here we also specify our budget of 10 evaluations.\n\n\n\n   ::: {.cell layout-align=\"center\"}\n   \n   ```{.r .cell-code}\n   tune_result <- tune(\n     task = tsk_iris,\n     learner = lrn_tree,\n     resampling = rsmp_tune,\n     measure = msr_tune,\n     tuner = tuner,\n     term_evals = 10L\n   )\n   ```\n   :::\n\n\n\n6. **Apply the Best Hyperparameters**: Update the learner with the best-found hyperparameters and retrain the model.\n\n\n\n   ::: {.cell layout-align=\"center\"}\n   \n   ```{.r .cell-code}\n   lrn_tree$param_set$values <- tune_result$result_learner_param_vals\n   lrn_tree$train(tsk_iris)\n   ```\n   :::\n\n\n\n::: {.callout-note}\n## Quiz: Tuning Performance\n\nQuestion 1: Estimating the performance of a tuned model:\n\nThrough the tuning archive, we can access the performance of the best-found hyperparameter configuration.\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ntune_result$archive$data[order(classif.acc, decreasing = TRUE), ][1, classif.acc]\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.94\n```\n\n\n:::\n:::\n\n\n\nDo you think this is a good estimate for the performance of the final model? Explain your answer.\n\n<details>\n<summary>Click for answer</summary>\nOne reason why we would expect the performance of the final model to be worse than the performance of the best-found hyperparameter configuration is due to *optimization bias*:\nWe choose the model configuration with the highest validation performance.\nThis selection process biases the result since the chosen model is the best among several trials.\nTo illustrate this, imagine that we take the maximum of 10 random numbers drawn from a normal distribution with mean 0.\nThe maximum over those numbers is larger than $0$, even though this is the mean of the generating distribution.\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](5-mlr3torch_files/figure-html/unnamed-chunk-20-1.png){fig-align='center' width=672}\n:::\n:::\n\n\n\n</details>\n:::\n\nThese two steps can also be encapsulated in the `AutoTuner` class, which first finds the best hyperparameters and then trains the model with them.\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nat <- auto_tuner(\n  learner = lrn_tree,\n  resampling = rsmp_tune,\n  measure = msr_tune,\n  term_evals = 10L,\n  tuner = tuner\n)\n```\n:::\n\n\n\nThe `AutoTuner` can be used just like any other `Learner`.\nTo get a valid performance estimate of the tuning process, we can `resample()` it on the task.\nThis is called *nested resampling*: the outer resampling is for evaluation and the inner resampling is for tuning.\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nrr <- resample(tsk_iris, at, rsmp_tune)\nrr$aggregate()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nclassif.ce \n0.05333333 \n```\n\n\n:::\n:::\n\n\n\n### Learning Pipelines\n\nIn many cases, we don't only fit a single learner but a whole learning pipeline.\nCommon use cases include the preprocessing of the data, e.g., for imputing missing values, scaling the data, or encoding categorical features, but many other operations are possible.\nThe `mlr3` extension `mlr3pipelines` is a toolbox for defining such learning pipelines.\nIts core building block is the `PipeOp` that can be constructed using the `po()` function.\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(mlr3pipelines)\npca <- po(\"pca\")\n```\n:::\n\n\n\nJust like a learner, it has a `$train()` and `$predict()` method, and we can apply it to a `Task` using these methods.\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\npca$train(list(tsk_iris))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n$output\n<TaskClassif:iris> (150 x 5)\n* Target: Species\n* Properties: multiclass\n* Features (4):\n  - dbl (4): PC1, PC2, PC3, PC4\n```\n\n\n:::\n\n```{.r .cell-code}\npca$predict(list(tsk_iris))[[1L]]\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n<TaskClassif:iris> (150 x 5)\n* Target: Species\n* Properties: multiclass\n* Features (4):\n  - dbl (4): PC1, PC2, PC3, PC4\n```\n\n\n:::\n:::\n\n\n\nUsually, such `PipeOp`s are combined with a `Learner` into a full learning `Graph`.\nThis is possible using the `%>>%` chain operator.\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(mlr3pipelines)\ngraph <- po(\"pca\") %>>% lrn(\"classif.rpart\")\nprint(graph)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nGraph with 2 PipeOps:\n            ID         State      sccssors prdcssors\n        <char>        <char>        <char>    <char>\n           pca <<UNTRAINED>> classif.rpart          \n classif.rpart <<UNTRAINED>>                     pca\n```\n\n\n:::\n\n```{.r .cell-code}\ngraph$plot(horizontal = TRUE)\n```\n\n::: {.cell-output-display}\n![](5-mlr3torch_files/figure-html/unnamed-chunk-25-1.png){fig-align='center' width=672}\n:::\n:::\n\n\n\nThe resulting `Graph` can be converted back into a `Learner` using the `as_learner()` function and used just like any other `Learner`.\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nglrn <- as_learner(graph)\nglrn$train(tsk_iris)\n```\n:::\n\n\n\n## Brief Introduction to `mlr3torch`\n\n`mlr3torch` builds upon the same components as `mlr3`, only that we use Deep `Learner`s, and can also work on non-tabular data.\nA simple example learner is the `lrn(\"classif.mlp\")` learner, which is a Multi-Layer Perceptron (MLP) for classification tasks.\n\n### Using a Predefined Torch Learner\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(mlr3torch)\nlrn_mlp <- lrn(\"classif.mlp\",\n  neurons = c(50, 50), # Two hidden layers with 50 neurons each\n  batch_size = 256, # Number of samples per gradient update\n  epochs = 30, # Number of training epochs\n  device = \"auto\", # Uses GPU if available, otherwise CPU\n  shuffle = TRUE, # because iris is sorted\n  optimizer = t_opt(\"adam\") # Adam optimizer\n)\n```\n:::\n\n\n\nThis multi-layer perceptron can be used just like the classification tree above.\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlrn_mlp$train(tsk_iris, row_ids = split$train)\n```\n:::\n\n\n\nThe trained `nn_module` can be accessed via the `$model` slot of the learner:\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlrn_mlp$model$network\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nAn `nn_module` containing 2,953 parameters.\n\n── Modules ─────────────────────────────────────────────────────────────────────────────────────────────────────────────\n• 0: <nn_linear> #250 parameters\n• 1: <nn_relu> #0 parameters\n• 2: <nn_dropout> #0 parameters\n• 3: <nn_linear> #2,550 parameters\n• 4: <nn_relu> #0 parameters\n• 5: <nn_dropout> #0 parameters\n• 6: <nn_linear> #153 parameters\n```\n\n\n:::\n:::\n\n\n\nBesides the trained network, the `$model` of the learner also contains the `$state_dict()` of the optimizer and other information.\n\nHaving trained the neural network on the training set, we can now make predictions on the test set:\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\npredictions <- lrn_mlp$predict(tsk_iris, row_ids = split$test)\npredictions$score(msr(\"classif.acc\"))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nclassif.acc \n        0.5 \n```\n\n\n:::\n:::\n\n\n\nUsing the benchmarking facilities of `mlr3`, we can also easily compare the classification tree with our deep learning learner:\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Define the resampling strategy\nrsmp_cv <- rsmp(\"cv\", folds = 3)\n\n# Create a benchmark grid to compare both learners\nbenchmark_grid <- benchmark_grid(\n  tasks = tsk_iris,\n  learners = list(lrn_tree, lrn_mlp),\n  resampling = rsmp_cv\n)\n\n# Run the benchmark\nrr_benchmark <- benchmark(benchmark_grid)\n\n# Aggregate the results\nresults_benchmark <- rr_benchmark$aggregate(msr(\"classif.acc\"))\n\n# Print the results\nprint(results_benchmark)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n      nr task_id    learner_id resampling_id iters classif.acc\n   <int>  <char>        <char>        <char> <int>       <num>\n1:     1    iris classif.rpart            cv     3        0.94\n2:     2    iris   classif.mlp            cv     3        0.64\nHidden columns: resample_result\n```\n\n\n:::\n:::\n\n\n\n### Validation Performance\n\nTracking validation performance is crucial for understanding how well your neural network is learning and to detect issues such as overfitting.\nIn the `mlr3` machine learning framework, this can be easily done by specifying the `$validate` field of a `Learner`.\nNote that this is not possible for all `Learner`s, but only for those with the `\"validation\"` property.\nThis includes boosting algorithms such as XGBoost or CatBoost, and also the `mlr3torch` learners.\n\nBelow, we set the validation ratio to 30% of the training data, specify the measures to record, and set the callbacks of the learner to record the history of the training process.\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlrn_mlp$configure(\n  validate = 0.3,\n  callbacks = t_clbk(\"history\"),\n  predict_type = \"prob\",\n  measures_valid = msr(\"classif.logloss\"),\n  measures_train = msr(\"classif.logloss\")\n)\n```\n:::\n\n\n\n:::{.callout-tip}\nThe `$configure()` method of a `Learner` allows you to simultaneously set fields and hyperparameters of a learner.\n:::\n\nWhen we now train the learner, 30% of the training data is used for validation, and the loss is recorded in the history of the learner.\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlrn_mlp$train(tsk_iris)\n```\n:::\n\n\n\nAfter training, the results of the callback can be accessed via the model.\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nhead(lrn_mlp$model$callbacks$history)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nKey: <epoch>\n   epoch train.classif.logloss valid.classif.logloss\n   <num>                 <num>                 <num>\n1:     1              1.216219             1.0216911\n2:     2              1.090373             1.0141709\n3:     3              1.103897             1.0070298\n4:     4              1.163823             1.0011182\n5:     5              1.100077             0.9970761\n6:     6              1.111388             0.9937620\n```\n\n\n:::\n:::\n\n\n\nAdditionally, the final validation scores can be accessed via the `$internal_valid_scores` field of the learner.\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlrn_mlp$internal_valid_scores\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n$classif.logloss\n[1] 0.8368236\n```\n\n\n:::\n:::\n\n\n\n### Defining a Custom Torch Learner\n\n`mlr3torch` also allows defining custom architectures by assembling special `PipeOp`s in a `Graph`.\nAs a starting point in the graph, we need to mark the entry of the Neural Network using an ingress pipeop.\nBecause we are working with a task with only one numeric feature, we can use `po(\"torch_ingress_num\")`.\nThere also exist inputs for categorical features (`po(\"torch_ingress_cat\")`) and generic tensors (`po(\"torch_ingress_ltnsr\")`).\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ningress <- po(\"torch_ingress_num\")\n```\n:::\n\n\n\nThe next steps in the graph are the actual layers of the neural network.\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\narchitecture <- po(\"nn_linear_1\", out_features = 100) %>>%\n  po(\"nn_relu_1\") %>>%\n  po(\"nn_linear_2\", out_features = 100) %>>%\n  po(\"nn_relu_2\") %>>%\n  po(\"nn_head\")\n\narchitecture$plot(horizontal = TRUE)\n```\n\n::: {.cell-output-display}\n![](5-mlr3torch_files/figure-html/unnamed-chunk-37-1.png){fig-align='center' width=672}\n:::\n:::\n\n\n\nAfter specifying the architecture, we need to set the remaining parts for the learner, which are the loss, optimizer, and the remaining training configuration such as the epochs, device, or the batch size.\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ngraph <- ingress %>>% architecture %>>%\n  po(\"torch_loss\", loss = \"cross_entropy\") %>>%\n  po(\"torch_optimizer\", optimizer = t_opt(\"adam\")) %>>%\n  po(\"torch_model_classif\", epochs = 10, batch_size = 256)\n```\n:::\n\n\n\nJust like before, we can convert the graph into a `Learner` using `as_learner()` and train it on the task:\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nglrn <- as_learner(graph)\nglrn$train(tsk_iris, row_ids = split$train)\n```\n:::\n\n\n\n### Working with Non-Tabular Data\n\nIn the `mlr3` ecosystem, the data of a task is always stored in a `data.frame` or `data.table`.\nTo work with non-tabular data, the `mlr3torch` package offers a custom datatype, the `lazy_tensor`, which can be stored in a `data.table`.\n\nAs an example to showcase this, we can use the CIFAR-10 dataset, which is a dataset of 60,000 32x32 color images in 10 classes, with 6,000 images per class.\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ntsk_cifar <- tsk(\"cifar10\")\ntsk_cifar\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n<TaskClassif:cifar10> (60000 x 2): CIFAR-10 Classification\n* Target: class\n* Properties: multiclass\n* Features (1):\n  - lt (1): image\n```\n\n\n:::\n:::\n\n\n\nThe image below shows some examples from the dataset:\n\n![](images/cifar10.jpg)\n\n:::{.callout-tip}\nTo avoid having to re-download the dataset every time, you can set the `mlr3torch.cache` option to `TRUE`.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\noptions(mlr3torch.cache = TRUE)\n```\n:::\n\n\n:::\n\nWhen accessing the data, only the images are represented as `lazy_tensor`s, the labels are still stored as a factor column:\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ntsk_cifar$head()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n        class           image\n       <fctr>   <lazy_tensor>\n1:       frog <tnsr[3x32x32]>\n2:      truck <tnsr[3x32x32]>\n3:      truck <tnsr[3x32x32]>\n4:       deer <tnsr[3x32x32]>\n5: automobile <tnsr[3x32x32]>\n6: automobile <tnsr[3x32x32]>\n```\n\n\n:::\n:::\n\n\n\nA `lazy_tensor` column can be compared to the `torch::dataset` class that we have seen earlier.\nThis means it does not necessarily store the data in memory, but only stores the information on how to load the data.\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nimage_vec <- tsk_cifar$data(cols = \"image\")[[1L]]\nhead(image_vec)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n<ltnsr[6]>\n[1] <tnsr[3x32x32]> <tnsr[3x32x32]> <tnsr[3x32x32]> <tnsr[3x32x32]> <tnsr[3x32x32]> <tnsr[3x32x32]>\n```\n\n\n:::\n:::\n\n\n\nTo access the data as `torch_tensor`s, we can call the `materialize()` function:\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nimage_tensor <- materialize(image_vec)[[1L]]\n```\n:::\n\n\n\nTo construct the CIFAR-10 task ourselves, we need to first:\n\n1. Construct a `torch::dataset` that returns the images as `torch_tensor`s.\n2. Create a `factor()` vector that contains the labels.\n\nWe can access the dataset via the `torchvision` package.\nFor simplicity, we will only load the training data.\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ncifar10_original <- torchvision::cifar10_dataset(root = \"data/cifar10\", train = TRUE, download = TRUE)\nimage_array <- cifar10_original$x\nstr(image_array)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n int [1:50000, 1:32, 1:32, 1:3] 59 154 255 28 170 159 164 28 134 125 ...\n```\n\n\n:::\n:::\n\n\n\nThe array contains 50,000 images (rows) of shape `32x32x3`.\nThe last dimension contains the channels, i.e., the RGB values of the pixels.\nWe reshape this so the channel dimension is the first dimension, which is the standard format for images in `torch`.\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nimage_array <- aperm(image_array, c(1, 4, 2, 3))\ndim(image_array)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 50000     3    32    32\n```\n\n\n:::\n:::\n\n\n\nNext, we create a `torch::dataset()` that loads individual images as a `torch_tensor`.\nTo convert this to a `lazy_tensor` later, the `$.getitem()` method needs to return a named list.\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ndataset_cifar10 <- dataset(\"cifar10\",\n  initialize = function(x) {\n    self$x <- x\n  },\n  .getitem = function(i) {\n    list(image = torch_tensor(self$x[i, , , ]))\n  },\n  .length = function() {\n    nrow(self$x)\n  }\n)\n```\n:::\n\n\n\nThe above object is not yet a dataset, but a dataset constructor, so we create the actual dataset by calling it with the image array.\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ncifar10 <- dataset_cifar10(image_array)\n```\n:::\n\n\n\nWe can check that this works correctly by accessing the first image.\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nstr(cifar10$.getitem(1))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nList of 1\n $ image:Long [1:3, 1:32, 1:32]\n```\n\n\n:::\n:::\n\n\n\nTo convert this to a `lazy_tensor`, we can use the `as_lazy_tensor()` function.\nThe only thing we need to specify is the output shapes of the tensors, which we set to `c(NA, 3, 32, 32)`.\nThe `NA` is used to indicate that the first dimension (batch dimension) can be of any size.\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ncifar10_lt <- as_lazy_tensor(cifar10, dataset_shapes = list(image = c(NA, 3, 32, 32)))\nhead(cifar10_lt)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n<ltnsr[6]>\n[1] <tnsr[3x32x32]> <tnsr[3x32x32]> <tnsr[3x32x32]> <tnsr[3x32x32]> <tnsr[3x32x32]> <tnsr[3x32x32]>\n```\n\n\n:::\n:::\n\n\n\n:::{.callout-tip}\nTo check that transformations on images were applied correctly, it can be useful to inspect the images, e.g., using `torchvision::tensor_image_browse()`.\n:::\n\nNext, we create the `factor` vector that contains the labels.\nFor that, we use the data stored in the original `torchvision::cifar10_dataset()` object.\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlabels <- factor(cifar10_original$y, labels = cifar10_original$classes[1:10])\nstr(labels)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n Factor w/ 10 levels \"airplane\",\"automobile\",..: 7 10 10 5 2 2 3 8 9 4 ...\n```\n\n\n:::\n:::\n\n\n\nNext, we create a `data.table` that contains the images and labels.\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ncifar10_dt <- data.table(image = cifar10_lt, label = labels)\nhead(cifar10_dt)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n             image      label\n     <lazy_tensor>     <fctr>\n1: <tnsr[3x32x32]>       frog\n2: <tnsr[3x32x32]>      truck\n3: <tnsr[3x32x32]>      truck\n4: <tnsr[3x32x32]>       deer\n5: <tnsr[3x32x32]> automobile\n6: <tnsr[3x32x32]> automobile\n```\n\n\n:::\n:::\n\n\n\nThis table can now be converted to an `mlr3::Task` using the `as_task_<type>` converters.\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ntsk_cifar <- as_task_classif(cifar10_dt, id = \"cifar10\", target = \"label\")\ntsk_cifar\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n<TaskClassif:cifar10> (50000 x 2)\n* Target: label\n* Properties: multiclass\n* Features (1):\n  - lt (1): image\n```\n\n\n:::\n:::\n\n\n\nWe will now try to train a simple multi-layer perceptron -- the one we have defined above -- on the images.\nOne problem that we have is that the images are of shape `32x32x3`, but the `nn_linear` layer expects a flat input of size `3072` (32 * 32 * 3).\n\nThis is where the `lazy_tensor` datatype comes in handy.\nWe can use the `PipeOp`s to transform the data before it is loaded.\nHere, the `-1` in the shape `c(-1, 3072)` indicates that the first dimension (batch dimension) can be of any size.\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nreshaper <- po(\"trafo_reshape\", shape = c(-1, 3072))\ntsk_cifar_flat <- reshaper$train(list(tsk_cifar))[[1L]]\ntsk_cifar_flat$head()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n        label         image\n       <fctr> <lazy_tensor>\n1:       frog  <tnsr[3072]>\n2:      truck  <tnsr[3072]>\n3:      truck  <tnsr[3072]>\n4:       deer  <tnsr[3072]>\n5: automobile  <tnsr[3072]>\n6: automobile  <tnsr[3072]>\n```\n\n\n:::\n:::\n\n\n\nNote that this transformation is not applied eagerly, but only when the data is actually loaded.\n\n:::{.callout-note}\nIn this case, as all the images are stored in memory, we could have also applied the transformation directly to the `array` representing the images, but decided not to do this for demonstration purposes.\n:::\n\nWe can now use almost the same graph as before on the flattened task.\nWe only need to exchange the ingress, as the new task has as data a `lazy_tensor` instead of numeric vectors.\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ngraph <- po(\"torch_ingress_ltnsr\") %>>% architecture %>>%\n  po(\"torch_loss\", loss = t_loss(\"cross_entropy\")) %>>%\n  po(\"torch_optimizer\", optimizer = t_opt(\"adam\")) %>>%\n  po(\"torch_model_classif\", epochs = 10, batch_size = 256)\nglrn <- as_learner(graph)\n```\n:::\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nglrn$train(tsk_cifar_flat)\n```\n:::\n\n\n\nAlternatively, we can integrate the flattening step into the graph from which the `GraphLearner` was created.\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ngraph_with_flattening <- reshaper %>>% graph\nglrn_with_flattening <- as_learner(graph_with_flattening)\n```\n:::\n\n\n\nThis learner can now be applied directly to the (unflattened) task.\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nglrn_with_flattening$param_set$set_values(torch_model_classif.epochs = 0)\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nglrn_with_flattening$train(tsk_cifar)\n```\n:::\n\n\n\n### Saving an `mlr3torch` Learner\n\nWe have seen earlier that torch tensors cannot simply be saved using `saveRDS()`.\nThe same also applies to the `mlr3torch` learners.\nTo save an `mlr3torch` learner, we need to call the `$marshal()` method first.\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\npth <- tempfile(fileext = \".rds\")\nglrn_with_flattening$marshal()\nsaveRDS(glrn_with_flattening, pth)\n```\n:::\n\n\n\nAfterward, we can load the learner again using `readRDS()` and the `$unmarshal()` method.\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nglrn_with_flattening <- readRDS(pth)\nglrn_with_flattening$unmarshal()\n```\n:::\n",
    "supporting": [
      "5-mlr3torch_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}