---
title: "Automatic Differentiation with torch"
---

```{r, include = FALSE}
set.seed(123)
torch::torch_manual_seed(123)
```

# Overview

Automatic differentiation (autograd) is one of torch's key features, enabling automatic computation of gradients for optimization tasks like training neural networks.
Unlike numerical differentiation which approximates gradients using finite differences, autograd computes exact gradients by tracking operations as they are performed and automatically applying the chain rule of calculus.
This makes it possible to efficiently compute gradients of complex functions with respect to many parameters - a critical requirement for training modern neural networks.
Autograd works by building a dynamic computational graph of operations, where each node represents a tensor and each edge represents a mathematical operation.

**Why do we need automatic differentation?**

In deep learning, training a model requires iteratively updating parameters to minimize a loss function, which measures the difference between predictions and actual data.
These updates depend on calculating gradients of the loss with respect to model parameters, information used by optimization algorithms like stochastic gradient descent (SGD).
*Automatic Differentiation* obviates the need to manually derive these gradients, which is error-prone.

## Enabling Gradient Tracking

To use autograd, tensors must have their `requires_grad` field be `TRUE`.
This can either be set during construction, but also changed afterwards using the in-place modifier `$requires_grad_(TRUE)`.
In the context of deep learning, we track the gradients of the weights of a neural network.
The simpleste "neural network" is a linear model where we have slope $a$ and bias $b$ and a single input $x$.

The forward pass is defined as:
$\hat{y} = a \times x + b$

We could be interested in how the prediction $\hat{y}$ changes for the given $x$ when we change the weight $a$ or the bias $b$.
We will later use this to change the weights $a$ and $b$ to lead to better predictions, i.e. we will perform gradient-based optimization.
To write down the gradients, let $u = a \times x$ denote the temporary tensor from the linear predictor.

* Weight *a*:
  This is expressed by the gradient $\frac{\partial \hat{y}}{\partial a}$.
  We can write down the derivative $\frac{\partial \hat{y}}{\partial a}$ using the chain rule as $\frac{\partial \hat{y}}{\partial a} = \frac{\partial \hat{y}}{\partial u} \cdot \frac{\partial u}{\partial a} = 1 \cdot x = x$.
* Bias *b*: $\frac{\partial \hat{y}}{\partial b} = 1$



```{r}
library(torch)

a <- torch_tensor(2, requires_grad = TRUE)
a$requires_grad
b <- torch_tensor(1, requires_grad = TRUE)
x <- torch_tensor(3)
```

We can use the weights and input to perform a forward pass:


```{r}
u = a * x
y = u + b
```


When you perform operations on tensors with gradient tracking, builds a computational graph on the fly.
In the figure below:

* blue tensors are those for which we want to calculate gradients
* the violet node is an intermediate tensor
* the yellow boxes are differentiable functions
* the green node is the final tensor w.r.t. which we want to calculate gradients.

```{mermaid}
graph TD
    a[a] --> mul[Multiply]
    x[x] --> mul
    mul --> u[u]
    u --> add[Add]
    b[b] --> add
    add --> y[y]

    %% Gradient flow
    y_grad[dy/du = 1, dy/db = 1] -.-> y
    u_grad[du/da = x] -.-> u
    a_grad[dy/da = x] -.-> a
    b_grad[dy/db = 1] -.-> b

    %% Styling
    classDef input fill:#a8d5ff,stroke:#333
    classDef op fill:#ffe5a8,stroke:#333
    classDef output fill:#a8ffb6,stroke:#333
    classDef grad fill:#ffa8a8,stroke:#333,stroke-dasharray:5,5
    classDef intermediate fill:#d5a8ff,stroke:#333
    classDef nograd fill:#e8e8e8,stroke:#333

    class a,b input
    class mul,add op
    class y output
    class u intermediate
    class y_grad,u_grad,a_grad,b_grad grad
    class x nograd
```


Each (intermediate) tensor knows how to calculate gradients with respect to its inputs.

```{r}
y$grad_fn
u$grad_fn
```

In order to calculate the gradients $\frac{\partial y}{\partial a}$ and $\frac{\partial y}{\partial b}$ we can traverse this computational graph backwards, call the derivatinge functions and multiply the individual derivatives according to the chain rule.
In `torch` this is done by calling `$backward()` on `y`:
The gradients are then accessible in the `$grad` field of the tensors `a` and `b`:

```{r}
# Compute gradients
y$backward()

# Access gradients
print(a$grad)  # dy/da = x = 3
print(b$grad)  # dy/db = 1
```

Note that only those tensors for which we set `$requires_grad` to `TRUE` store the tensors.
For the intermediate value `u`, no gradient is stored.

:::{.callout-tip}
When you want to perform an operation on tensors that require gradients without tracking this specific operation, you can use `with_no_grad(...)`.
:::



In the next section we will show how we can use gradients to train a simple linear model.

## A Simple Linear Model

We can use autograd to fit a simple linear regression model. Let's first generate some synthetic data:

```{r}
library(ggplot2)

# Set random seed for reproducibility
torch_manual_seed(42)

# Generate synthetic data
n <- 100
a_true <- 2.5
b_true <- 1.0

# Create input X and add noise to output Y
X <- torch_randn(n)
noise <- torch_randn(n) * 0.5
Y <- X * a_true + b_true + noise
```


```{r, echo = FALSE}
# Convert to R vectors for plotting
x_r <- as.numeric(X)
y_r <- as.numeric(Y)

# Plot the data
p <- ggplot(data.frame(x = x_r, y = y_r), aes(x = x, y = y)) +
  geom_point(alpha = 0.5) +
  geom_abline(intercept = 1.0, slope = 2.5, linewidth = 1,
              color = "blue") +
  theme_minimal() +
  labs(title = "Simulated Linear Regression Data",
       x = "X", y = "Y")
p
```

First, we randomly initialize our parameters `a` and `b`.

```{r}
# Initialize parameters with random values
a <- torch_randn(1, requires_grad = TRUE)
b <- torch_randn(1, requires_grad = TRUE)
```

In order to optimize the parameters $a$ and $b$, we need to define the *Loss Function* that quantifies the discrepancy between our predictions $\hat{y}$ and the observed values $y$.
The standard loss for linear regression is the L2 loss:
$$ L(y, \hat{y}) = (y - \hat{y})^2$$

The graphic below visualizes the relationship between the parameters $a$ and $b$ with the average L2 loss over all datapoints, i.e., the Mean Square Error (MSE).
For parameters $a$ and $b$ that are on the same contour line, the same loss is observed.
The color gradient indicates the magnitude of the MSE.
In this case, ligher values mark areas with higher loss and darker values mark areas with lower loss.
The red point marks the minimum loss, while the blue point shows the starting values of the parameters.

```{r, echo = FALSE}
# Create grid of a and b values
a_range <- seq(0, 5, length.out = 50)
b_range <- seq(-1, 3, length.out = 50)
grid <- expand.grid(a = a_range, b = b_range)

# Calculate MSE for each combination of a and b
grid$mse <- sapply(1:nrow(grid), function(i) {
  a_val <- grid$a[i]
  b_val <- grid$b[i]
  y_pred <- X * a_val + b_val
  mean(as.array((Y - y_pred)^2))  # Changed from y to Y
})

# Fit linear model to get minimum
d <- data.frame(y = as_array(Y), x = as_array(X))
model <- lm(y ~ x, data = d)
a_min <- coef(model)[2] # slope coefficient
b_min <- coef(model)[1] # intercept coefficient
# Create contour plot
atmp = a$item()
btmp = b$item()
ggplot(grid, aes(x = a, y = b, z = mse)) +
  geom_contour_filled() +
  scale_fill_viridis_d() +
  theme_minimal() +
  geom_point(aes(x = a_min, y = b_min), color = "red", size = 3) +
  geom_point(aes(x = atmp, y = btmp), color = "blue", size = 3) +
  annotate("text", x = a_min + 0.3, y = b_min + 0.4,
           label = sprintf("Minimum:\na = %.2f\nb = %.2f", a_min, b_min), color = "red") +
  annotate("text", x = as.numeric(a) + 0.3, y = as.numeric(b) + 0.4,
           label = sprintf("Start:\na = %.2f\nb = %.2f", as.numeric(a), as.numeric(b)), color = "blue") +
  labs(title = "Loss Surface",
       x = "Slope a",
       y = "Bias b",
       fill = "MSE") +
  coord_fixed()
```

We can optimize the parameters $a$ and $b$ to converge to the minimum by using **gradient descent**.
Gradient descent is a fundamental optimization algorithm that helps us find the minimum of a function by iteratively moving in the direction of steepest descent.

## Understanding Gradient Descent

The gradient of a function points in the direction of steepest increase - like pointing uphill on a mountainous terrain.
Therefore, the negative gradient points in the direction of steepest decrease - like pointing downhill.
Gradient descent uses this property to iteratively:

1. Calculate the gradient at the current position
2. Take a small step in the opposite direction of the gradient
3. Repeat until we reach a minimum

Note that the gradient only tells us in which direction we have to go, not how far.
The length of the step should not be:

- **too large** because the gradient approximation only holds in a small neighbourhood
- **too small** as otherwise the convergence will be slow.

The general update formula for the weights $a$ and $b$ is:

$$a_{t+1} = a_t - \eta \frac{\partial L}{\partial a_t}$$
$$b_{t+1} = b_t - \eta \frac{\partial L}{\partial b_t}$$

where $\eta$ is the learning rate and $L$ is the loss function.

In practice, when dealing with large datasets, computing the gradient over the entire dataset can be computationally expensive. Instead, we often use **Stochastic Gradient Descent (SGD)**, where we:

1. Randomly sample a small batch of data points
2. Compute the gradient only on this batch
3. Update the parameters using this approximate gradient

While the gradients from SGD are noisier than full gradient descent, they:

- Require less compute and memory per update
- Implicitly regularize the optimization, which improves generalization
- Can help escape local minima due to the noise
- Often converge faster in practice

The batch size is another hyperparameter - larger batches give more stable gradients but require more computation, while smaller batches introduce more noise but allow for more frequent updates.


We start by implementing a single gradient step.
Note that if we would repeatedly call `loss$backward()`, the gradients in `a` and `b` would accumulate, so we set them to `0` before performing the update.
The return value of the upgrade will be the parameter values and the loss so we can plot them later.
Also note that we mutate the parameters `a` and `b` in-place (suffix `_`).

```{r}
update_params <- function(X_batch, Y_batch, lr, a, b) {
  # perform forward pass, calculate loss
  Y_hat = X_batch * a + b
  loss = mean((Y_hat - Y_batch)^2)
  # calculate gradients
  loss$backward()
  # we don't want to track gradients when we update the parameters.
  with_no_grad({
    a$sub_(lr * a$grad)
    b$sub_(lr * b$grad)
  })
  # make sure gradients are 0
  a$grad$zero_()
  b$grad$zero_()
  list(
    a = a$item(),
    b = b$item(),
    loss = loss$item()
  )
}
```

```{r}
library(data.table)
# hyperparameters
lr <- 0.02
epochs <- 10
batch_size <- 10

# Split data into 10 batches of size 10
batches <- split(sample(1:100), rep(seq_len(batch_size), length.out = 100))
history <- list()
for (epoch in seq_len(epochs)) {
  for (step in 1:10) {
    result = update_params(X[batches[[step]]], Y[batches[[step]]], lr, a, b)
    history = append(history, list(as.data.table(result)))
  }
}

history = rbindlist(history)
```

This example demonstrates how we can use torch's autograd to implement gradient descent for fitting a simple linear regression model.
The dashed red lines show the progression of the model during training, with increasing opacity for later steps.
The Blue line is the true relationship.

```{r, echo = FALSE}
# plot results with lines showing progression
plot_steps <- seq(1, epochs * length(batches), length.out = epochs)
for (i in plot_steps) {
  p <- p + geom_abline(
    intercept = history[i, "b"][[1]],
    slope = history[i, "a"][[1]],
    color = "red",
    alpha = i/(length(plot_steps) + 10),
    linetype = "dashed"
  )
}
p
```


We can also visualize the parameter udpates over time:

```{r, echo = FALSE}
# Plot parameter updates on the contour plot
ggplot(data = grid, aes(x = a, y = b, z = mse)) +
  geom_contour_filled() +
  scale_fill_viridis_d() +
  theme_minimal() +
  # Add path of parameter updates
  geom_path(data = history,
            aes(x = a, y = b, z = NULL),
            color = "red",
            arrow = arrow(length = unit(0.1, "cm"),
                         type = "closed",
                         ends = "last")) +
  # Add start and end points
  geom_point(data = history[1,], aes(x = a, y = b, z = NULL), color = "darkred", size = 3) +
  geom_point(data = history[nrow(history),], aes(x = a, y = b, z = NULL), color = "darkred", size = 3) +
  annotate("text", x = history[1,]$a + 0.3, y = history[1,]$b,
           label = "Start", color = "darkred") +
  annotate("text", x = history[nrow(history),]$a + 0.3, y = history[nrow(history),]$b,
           label = "End", color = "darkred") +
  labs(title = "Parameter Updates on Loss Landscape",
       x = "Slope a",
       y = "Bias b",
       fill = "MSE") +
  coord_fixed()
```

Of course, better solutions exist for estimating a simple linear model, but the example demonstrated how we can make use of an autograd system to estimate the parameters of a model.

## Exercises:

1. Extend the approach in the notebook to multivariate regression.

