[
  {
    "objectID": "README.html",
    "href": "README.html",
    "title": "Deep Learning with (mlr3)torch in R :fire:",
    "section": "",
    "text": "This is a course, containing six tutorials and corresponding exercises (with solutions). The topics are:\n\nTorch Tensors\nAutograd\nModules and Data\nOptimizers\nIntro to mlr3torch (and mlr3 recap)\nTraining Efficiency\nUse Case\n\n\n\n\nIn order to be able to successfully run the notebooks and solve the exercises, make sure that you have the right environment set up.\n\n\nThese notebooks were developed using R 4.4.2, so ideally you should use the same version. You can check your current R version by running R --version in your terminal. If you have the same R version, you can skip the rest of this section.\nA convenient way to simultaneously maintain different versions of R and to easily switch between them is to use rig, the R installation manager. It’s documentation contains instructions on how to install it on Windows, macOS, and Linux.\nAfter having installed rig, you can install R 4.4.2 and make it the default by running the following command in your terminal:\nrig add 4.4.2\nrig default 4.4.2\nVerify that the R version is 4.4.2 by running R --version in your terminal.\n\n\n\nFor managing libraries, we use the renv package. If you don’t have renv installed, you can install it by running the following command:\ninstall.packages(\"renv\")\nNext, initialize the renv environment by running the following command:\nrenv::init()\nFinally, restore the renv environment by running the following command. This might take some time as it downloads all the required libraries.\nrenv::restore()\nTo be able to use the torch package, you need to also run this additional command:\ntorch::install_torch()\nIf this completes successfully, you are ready to go! :rocket:\nTo optionally check whether you have GPU support installed (this is not necessary for the exercises), run the following command:\ntorch::cuda_is_available()\n\n\n\n\nSome of the content is based on the book Deep Learning and Scientific Computing with R torch by Sigrid Keydana."
  },
  {
    "objectID": "README.html#overview-book",
    "href": "README.html#overview-book",
    "title": "Deep Learning with (mlr3)torch in R :fire:",
    "section": "",
    "text": "This is a course, containing six tutorials and corresponding exercises (with solutions). The topics are:\n\nTorch Tensors\nAutograd\nModules and Data\nOptimizers\nIntro to mlr3torch (and mlr3 recap)\nTraining Efficiency\nUse Case"
  },
  {
    "objectID": "README.html#setup-card_index_dividers",
    "href": "README.html#setup-card_index_dividers",
    "title": "Deep Learning with (mlr3)torch in R :fire:",
    "section": "",
    "text": "In order to be able to successfully run the notebooks and solve the exercises, make sure that you have the right environment set up.\n\n\nThese notebooks were developed using R 4.4.2, so ideally you should use the same version. You can check your current R version by running R --version in your terminal. If you have the same R version, you can skip the rest of this section.\nA convenient way to simultaneously maintain different versions of R and to easily switch between them is to use rig, the R installation manager. It’s documentation contains instructions on how to install it on Windows, macOS, and Linux.\nAfter having installed rig, you can install R 4.4.2 and make it the default by running the following command in your terminal:\nrig add 4.4.2\nrig default 4.4.2\nVerify that the R version is 4.4.2 by running R --version in your terminal.\n\n\n\nFor managing libraries, we use the renv package. If you don’t have renv installed, you can install it by running the following command:\ninstall.packages(\"renv\")\nNext, initialize the renv environment by running the following command:\nrenv::init()\nFinally, restore the renv environment by running the following command. This might take some time as it downloads all the required libraries.\nrenv::restore()\nTo be able to use the torch package, you need to also run this additional command:\ntorch::install_torch()\nIf this completes successfully, you are ready to go! :rocket:\nTo optionally check whether you have GPU support installed (this is not necessary for the exercises), run the following command:\ntorch::cuda_is_available()"
  },
  {
    "objectID": "README.html#credit",
    "href": "README.html#credit",
    "title": "Deep Learning with (mlr3)torch in R :fire:",
    "section": "",
    "text": "Some of the content is based on the book Deep Learning and Scientific Computing with R torch by Sigrid Keydana."
  },
  {
    "objectID": "notebooks/7-usecase-exercise.html",
    "href": "notebooks/7-usecase-exercise.html",
    "title": "Practical Use Case",
    "section": "",
    "text": "In this notebook, the goal is to solve an actual problem using a neural network. As a baseline model, we will use a simple tree-based model.\nYou are free to use either only torch or mlr3torch."
  },
  {
    "objectID": "notebooks/6-training-efficiency-exercise.html",
    "href": "notebooks/6-training-efficiency-exercise.html",
    "title": "Training Efficiency",
    "section": "",
    "text": "Question 1: Validation\nIn this exercise, we will once again train a simple multi-layer perceptron on the Indian Liver Patient Dataset (ILPD). Create a learner that:\n\nUses 2 hidden layers with 100 neurons each.\nUtilizes a batch size of 128.\nTrains for 200 epochs.\nEmploys a validation set comprising 30% of the data.\nTracks the training and validation log-loss during training.\nUtilizes trace-jitting to speed up the training process.\nEmploys the history callback to record the training and validation log-loss during training.\n\nAfterward, plot the validation log-loss, which is accessible via learner$model$callbacks$history.\nBelow, we create the task and remove the gender feature for simplicity.\n\nlibrary(mlr3verse)\n\nLoading required package: mlr3\n\nlibrary(mlr3torch)\n\nLoading required package: mlr3pipelines\n\n\nLoading required package: torch\n\nilpd_num &lt;- tsk(\"ilpd\")\nilpd_num$select(setdiff(ilpd_num$feature_names, \"gender\"))\nilpd_num\n\n&lt;TaskClassif:ilpd&gt; (583 x 10): Indian Liver Patient Data\n* Target: diseased\n* Properties: twoclass\n* Features (9):\n  - dbl (5): albumin, albumin_globulin_ratio, direct_bilirubin, total_bilirubin, total_protein\n  - int (4): age, alanine_transaminase, alkaline_phosphatase, aspartate_transaminase\n\n\n\n\nHint\n\n\nTo specify the validation set, use the validate field, which can either be set during construction or by calling $configure().\nTrace-jitting can be enabled via the jit_trace parameter.\nThe history callback can be constructed via t_clbk(\"history\") and needs to be passed during the construction of the learner.\nThe validation and measures can be specified via measures_valid and take a measure object that is constructed via msr().\n\n\nQuestion 2: Early Stopping Enable early stopping to prevent overfitting and re-train the learner (using a patience of 10). Print the final validation performance of the learner and the early stopped results. You can consult the documentation of LearnerTorch on how to access these two results (section Active Bindings).\n\n\nHint\n\nYou can enable early stopping by setting the patience parameter.\n\nQuestion 3: Early Stopping and Dropout Tuning\nWhile early stopping in itself is already useful, mlr3torch also allows you to simultaneously tune the number of epochs using early stopping while tuning other hyperparameters via traditional hyperparameter tuning from mlr3tuning.\nOne thing we have not covered so far is that the MLP learner we have used so far also uses a dropout layer. The dropout probability can be configured via the p parameter.\nYour task is to tune the dropout probability p in the range \\([0, 1]\\) and the epochs using early stopping (using the configuration from the previous exercise).\nTo adapt this to work with early stopping, you need to set the:\n\nepochs to to_tune(upper = &lt;value&gt;, internal = TRUE): This tells the Tuner that the learner will tune the number of epochs itself.\n$validate field of the \"test\" so the same data is used for tuning and validation.\nTuning measure to msr(\"internal_valid_score\", minimize = TRUE). We set minimize to TRUE because we have used the log-loss as a validation measure.\n\nApart from this, the tuning works just like in tutorial 5. Use 3-fold cross-validation for the tuning and evaluate 10 configurations.\nRun the tuning and print the optimal configuration."
  },
  {
    "objectID": "notebooks/6-training-efficiency-exercise-solution.html",
    "href": "notebooks/6-training-efficiency-exercise-solution.html",
    "title": "Training Efficiency",
    "section": "",
    "text": "Question 1: Validation\nIn this exercise, we will once again train a simple multi-layer perceptron on the Indian Liver Patient Dataset (ILPD). Create a learner that:\n\nUses 2 hidden layers with 100 neurons each.\nUtilizes a batch size of 128.\nTrains for 200 epochs.\nEmploys a validation set comprising 30% of the data.\nTracks the training and validation log-loss during training.\nUtilizes trace-jitting to speed up the training process.\nEmploys the history callback to record the training and validation log-loss during training.\n\nAfterward, plot the validation log-loss, which is accessible via learner$model$callbacks$history.\nBelow, we create the task and remove the gender feature for simplicity.\n\nlibrary(mlr3verse)\n\nLoading required package: mlr3\n\nlibrary(mlr3torch)\n\nLoading required package: mlr3pipelines\n\n\nLoading required package: torch\n\nilpd_num &lt;- tsk(\"ilpd\")\nilpd_num$select(setdiff(ilpd_num$feature_names, \"gender\"))\nilpd_num\n\n&lt;TaskClassif:ilpd&gt; (583 x 10): Indian Liver Patient Data\n* Target: diseased\n* Properties: twoclass\n* Features (9):\n  - dbl (5): albumin, albumin_globulin_ratio, direct_bilirubin, total_bilirubin, total_protein\n  - int (4): age, alanine_transaminase, alkaline_phosphatase, aspartate_transaminase\n\n\n\n\nHint\n\n\nTo specify the validation set, use the validate field, which can either be set during construction or by calling $configure().\nTrace-jitting can be enabled via the jit_trace parameter.\nThe history callback can be constructed via t_clbk(\"history\") and needs to be passed during the construction of the learner.\nThe validation and measures can be specified via measures_valid and take a measure object that is constructed via msr().\n\n\nSolution\n\nlibrary(ggplot2)\n\nmlp &lt;- lrn(\"classif.mlp\",\n  neurons = c(100, 100),\n  batch_size = 128,\n  epochs = 200,\n  predict_type = \"prob\",\n  validate = 0.3,\n  jit_trace = TRUE,\n  callbacks = t_clbk(\"history\"),\n  measures_valid = msr(\"classif.logloss\")\n)\n\nmlp$train(ilpd_num)\nhead(mlp$model$callbacks$history)\n\n   epoch valid.classif.logloss\n   &lt;num&gt;                 &lt;num&gt;\n1:     1              3.373034\n2:     2              5.475234\n3:     3              4.667771\n4:     4              3.047842\n5:     5              1.563049\n6:     6              0.958690\n\nggplot(mlp$model$callbacks$history) +\n  geom_line(aes(x = epoch, y = valid.classif.logloss)) +\n  labs(\n    y = \"Log-Loss (Validation)\",\n    x = \"Epoch\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\nQuestion 2: Early Stopping Enable early stopping to prevent overfitting and re-train the learner (using a patience of 10). Print the final validation performance of the learner and the early stopped results. You can consult the documentation of LearnerTorch on how to access these two results (section Active Bindings).\n\n\nHint\n\nYou can enable early stopping by setting the patience parameter.\n\nSolution\n\nmlp$configure(\n  patience = 10\n)\nmlp$train(ilpd_num)\nmlp$internal_tuned_values\n\n$epochs\n[1] 24\n\nmlp$internal_valid_scores\n\n$classif.logloss\n[1] 0.5598296\n\n\nQuestion 3: Early Stopping and Dropout Tuning\nWhile early stopping in itself is already useful, mlr3torch also allows you to simultaneously tune the number of epochs using early stopping while tuning other hyperparameters via traditional hyperparameter tuning from mlr3tuning.\nOne thing we have not covered so far is that the MLP learner we have used so far also uses a dropout layer. The dropout probability can be configured via the p parameter.\nYour task is to tune the dropout probability p in the range \\([0, 1]\\) and the epochs using early stopping (using the configuration from the previous exercise).\nTo adapt this to work with early stopping, you need to set the:\n\nepochs to to_tune(upper = &lt;value&gt;, internal = TRUE): This tells the Tuner that the learner will tune the number of epochs itself.\n$validate field of the \"test\" so the same data is used for tuning and validation.\nTuning measure to msr(\"internal_valid_score\", minimize = TRUE). We set minimize to TRUE because we have used the log-loss as a validation measure.\n\nApart from this, the tuning works just like in tutorial 5. Use 3-fold cross-validation for the tuning and evaluate 10 configurations.\nRun the tuning and print the optimal configuration.\nSolution\n\nlibrary(mlr3torch)\n\nmlp$configure(\n  epochs = to_tune(upper = 100, internal = TRUE),\n  p = to_tune(lower = 0, upper = 1),\n  validate = \"test\"\n)\n\ntuner &lt;- tnr(\"random_search\")\nresampling &lt;- rsmp(\"cv\", folds = 3)\nmeasure &lt;- msr(\"internal_valid_score\", minimize = TRUE)\n\nti &lt;- tune(\n  tuner = tuner,\n  task = ilpd_num,\n  learner = mlp,\n  resampling = resampling,\n  measure = measure,\n  term_evals = 10\n)\n\nti$learner_result_param_vals\n\nNULL"
  },
  {
    "objectID": "notebooks/5-mlr3torch-exercise.html",
    "href": "notebooks/5-mlr3torch-exercise.html",
    "title": "Training Neural Networks with mlr3torch",
    "section": "",
    "text": "Question 1: Hello World!\nIn this exercise, you will train your first neural network with mlr3torch.\nAs a task, we will use the ‘Indian Liver Patient’ dataset where the goal is to predict whether a patient has liver disease or not.\n\nlibrary(mlr3verse)\n\nLoading required package: mlr3\n\nlibrary(mlr3torch)\n\nLoading required package: mlr3pipelines\n\n\nLoading required package: torch\n\nilpd &lt;- tsk(\"ilpd\")\nilpd\n\n&lt;TaskClassif:ilpd&gt; (583 x 11): Indian Liver Patient Data\n* Target: diseased\n* Properties: twoclass\n* Features (10):\n  - dbl (5): albumin, albumin_globulin_ratio, direct_bilirubin, total_bilirubin, total_protein\n  - int (4): age, alanine_transaminase, alkaline_phosphatase, aspartate_transaminase\n  - fct (1): gender\n\nautoplot(ilpd)\n\n\n\n\n\n\n\n\nWe remove the gender column from the task, so we need to deal with numeric features for now.\n\nilpd_num = ilpd$clone(deep = TRUE)\nilpd_num$select(setdiff(ilpd_num$feature_names, \"gender\"))\n\nTrain a simple multi layer perceptron (lrn(\"classif.mlp\")) with 2 hidden layers with 100 neurons each. Set the batch size to 32, the learning rate to 0.001 and the number of epochs to 20. Then, resample the learner on the task with a cross-validation with 5 folds and evaluate the results using classification error and false positive rate (FPR). Is the result good?\n\n\nHint\n\n\nThe parameter for the learning rate is opt.lr\nProbability predictions are made by setting the predict_type field to \"prob\".\n\n\nQuestion 2: Preprocessing\nIn the previous task, we have operated on the ilpd_num task where we excluded the categorical gender column. This was done because the MLP learner operates on numeric features only. We will now create a more complex GraphLearner that also incudes one-hot encoding of the gender column. Resample this learner on the original ilpd task and evaluate the results using the same measures as before.\n\n\nHint\n\nConcatenate po(\"encode\") with a lrn(\"classif.mlp\") using %&gt;&gt;% to create the GraphLearner. For available options on the encoding, see po(\"encode\")$help().\n\nQuestion 3: Benchmarking\nInstead of resampling a single learner, we now want to compare the performance of our MLP with a simple classification tree Create a benchmark design and compare the performance of the two learners.\n\n\nHint\n\nCreate a classification tree via lrn(\"classif.rpart\"). A benchmark design can be created via benchmark_grid(). To run a benchmark, pass the design to benchmark()."
  },
  {
    "objectID": "notebooks/5-mlr3torch-exercise-solution.html",
    "href": "notebooks/5-mlr3torch-exercise-solution.html",
    "title": "Training Neural Networks with mlr3torch",
    "section": "",
    "text": "Question 1: Hello World!\nIn this exercise, you will train your first neural network with mlr3torch.\nAs a task, we will use the ‘Indian Liver Patient’ dataset where the goal is to predict whether a patient has liver disease or not.\n\nlibrary(mlr3verse)\n\nLoading required package: mlr3\n\nlibrary(mlr3torch)\n\nLoading required package: mlr3pipelines\n\n\nLoading required package: torch\n\nilpd &lt;- tsk(\"ilpd\")\nilpd\n\n&lt;TaskClassif:ilpd&gt; (583 x 11): Indian Liver Patient Data\n* Target: diseased\n* Properties: twoclass\n* Features (10):\n  - dbl (5): albumin, albumin_globulin_ratio, direct_bilirubin, total_bilirubin, total_protein\n  - int (4): age, alanine_transaminase, alkaline_phosphatase, aspartate_transaminase\n  - fct (1): gender\n\nautoplot(ilpd)\n\n\n\n\n\n\n\n\nWe remove the gender column from the task, so we need to deal with numeric features for now.\n\nilpd_num = ilpd$clone(deep = TRUE)\nilpd_num$select(setdiff(ilpd_num$feature_names, \"gender\"))\n\nTrain a simple multi layer perceptron (lrn(\"classif.mlp\")) with 2 hidden layers with 100 neurons each. Set the batch size to 32, the learning rate to 0.001 and the number of epochs to 20. Then, resample the learner on the task with a cross-validation with 5 folds and evaluate the results using classification error and false positive rate (FPR). Is the result good?\n\n\nHint\n\n\nThe parameter for the learning rate is opt.lr\nProbability predictions are made by setting the predict_type field to \"prob\".\n\n\n\nmlp &lt;- lrn(\"classif.mlp\",\n  neurons = c(100, 100),\n  batch_size = 32,\n  epochs = 20,\n  opt.lr = 0.001\n)\ncv10 &lt;- rsmp(\"cv\", folds = 10)\nrr1 &lt;- resample(task = ilpd_num, learner = mlp, resampling = cv10)\nrr1$aggregate(msrs(c(\"classif.ce\", \"classif.fpr\")))\n\n classif.ce classif.fpr \n  0.3706312   0.5849387 \n\n\nWhile the classification error is low, this is not a good measure due to the imbalanced class distribution. This is confirmed by the FPR, which is relatively high.\nQuestion 2: Preprocessing\nIn the previous task, we have operated on the ilpd_num task where we excluded the categorical gender column. This was done because the MLP learner operates on numeric features only. We will now create a more complex GraphLearner that also incudes one-hot encoding of the gender column. Resample this learner on the original ilpd task and evaluate the results using the same measures as before.\n\n\nHint\n\nConcatenate po(\"encode\") with a lrn(\"classif.mlp\") using %&gt;&gt;% to create the GraphLearner. For available options on the encoding, see po(\"encode\")$help().\n\n\nencoder &lt;- po(\"encode\", method = \"one-hot\")\nglrn &lt;- as_learner(encoder %&gt;&gt;% mlp)\nrr2 &lt;- resample(ilpd, glrn, cv10)\nrr2$aggregate(msrs(c(\"classif.ce\", \"classif.fpr\")))\n\n classif.ce classif.fpr \n  0.3137347   0.8365021 \n\n\nQuestion 3: Benchmarking\nInstead of resampling a single learner, we now want to compare the performance of our MLP with a simple classification tree Create a benchmark design and compare the performance of the two learners.\n\n\nHint\n\nCreate a classification tree via lrn(\"classif.rpart\"). A benchmark design can be created via benchmark_grid(). To run a benchmark, pass the design to benchmark().\n\n\ndesign &lt;- benchmark_grid(\n  task = ilpd,\n  learner = list(glrn, lrn(\"classif.rpart\", predict_type = \"prob\")),\n  resampling = cv10\n)\nbmr &lt;- benchmark(design)\nbmr$aggregate(msrs(c(\"classif.ce\", \"classif.tpr\")))\n\n      nr task_id         learner_id resampling_id iters classif.ce classif.tpr\n   &lt;int&gt;  &lt;char&gt;             &lt;char&gt;        &lt;char&gt; &lt;int&gt;      &lt;num&gt;       &lt;num&gt;\n1:     1    ilpd encode.classif.mlp            cv    10  0.2812683   0.9145299\n2:     2    ilpd      classif.rpart            cv    10  0.3035652   0.8301628\nHidden columns: resample_result"
  },
  {
    "objectID": "notebooks/4-optimizer-exercise.html",
    "href": "notebooks/4-optimizer-exercise.html",
    "title": "Optimizer Exercises",
    "section": "",
    "text": "Question 1\nIn this exercise, the task is to play around with the settings for the optimization of a neural network. We start by generating some (highly non-linear) synthetic data using the mlbench package.\n\nlibrary(torch)\ndata &lt;- mlbench::mlbench.friedman3(n = 3000, sd = 0.1)\nX &lt;- torch_tensor(data$x)\nX[1:2, ]\n\ntorch_tensor\n   40.8977   745.3378     0.3715     3.4246\n   88.3017  1148.7073     0.5288     6.5953\n[ CPUFloatType{2,4} ]\n\nY &lt;- torch_tensor(data$y)$unsqueeze(2)\nY[1:2, ]\n\ntorch_tensor\n 1.5238\n 1.3572\n[ CPUFloatType{2,1} ]\n\n\nThe associated machine learning task is to predict the output Y from the input X.\nNext, we create a dataset for it using the tensor_dataset() function.\n\nds &lt;- tensor_dataset(X, Y)\nds$.getitem(1)\n\n[[1]]\ntorch_tensor\n  40.8977\n 745.3378\n   0.3715\n   3.4246\n[ CPUFloatType{4} ]\n\n[[2]]\ntorch_tensor\n 1.5238\n[ CPUFloatType{1} ]\n\n\nWe can create two sub-datasets – for training and validation – using dataset_subset().\n\nids_train &lt;- sample(1000, 700)\nids_valid &lt;- setdiff(seq_len(1000), ids_train)\nds_train &lt;- dataset_subset(ds, ids_train)\nds_valid &lt;- dataset_subset(ds, ids_valid)\n\nThe network that we will be fitting is a simple MLP:\n\nnn_mlp &lt;- nn_module(\"nn_mlp\",\n  initialize = function() {\n    self$lin1 &lt;- nn_linear(4, 50)\n    self$lin2 &lt;- nn_linear(50, 50)\n    self$lin3 &lt;- nn_linear(50, 1)\n  },\n  forward = function(x) {\n    x |&gt;\n      self$lin1() |&gt;\n      nnf_relu() |&gt;\n      self$lin2() |&gt;\n      nnf_relu() |&gt;\n      self$lin3()\n  }\n)\n\nThe code to compare different optimizer configurations is provided through the (provided) compare_configs() function. It takes as arguments:\n\nepochs: The number of epochs to train for. Defaults to 30.\nbatch_size: The batch size to use for training. Defaults to 16.\nlr: The learning rate to use for training. Defaults to 0.01.\nweight_decay: The weight decay to use for training. Defaults to 0.01.\nbeta1: The momentum parameter to use for training. Defaults to 0.9.\nbeta2: The adaptive step size parameter to use for training. Defaults to 0.999.\n\nOne of the arguments (except for epochs) must be a list of values. The function will then run the same training configuration for each of the values in the list and visualize the results.\n\n\nImplementation of compare_configs\n\n\nlibrary(ggplot2)\ncompare_configs &lt;- function(epochs = 30, batch_size = 16, lr = 0.01, weight_decay = 0.01, beta1 = 0.9, beta2 = 0.999) {\n  # Identify which parameter is a list\n  args &lt;- list(batch_size = batch_size, lr = lr, weight_decay = weight_decay, beta1 = beta1, beta2 = beta2)\n  is_list &lt;- sapply(args, is.list)\n\n  if (sum(is_list) != 1) {\n    stop(\"One of the arguments must be a list\")\n  }\n\n  list_arg_name &lt;- names(args)[is_list]\n  list_args &lt;- args[[list_arg_name]]\n  other_args &lt;- args[!is_list]\n\n  # Run train_valid for each value in the list\n  results &lt;- lapply(list_args, function(arg) {\n    network &lt;- with_torch_manual_seed(seed = 123, nn_mlp())\n    other_args[[list_arg_name]] &lt;- arg\n    train_valid(network, ds_train = ds_train, ds_valid = ds_valid, epochs = epochs, batch_size = other_args$batch_size,\n      lr = other_args$lr, betas = c(other_args$beta1, other_args$beta2), weight_decay = other_args$weight_decay)\n  })\n\n  # Combine results into a single data frame\n  combined_results &lt;- do.call(rbind, lapply(seq_along(results), function(i) {\n    df &lt;- results[[i]]\n    df$config &lt;- paste(list_arg_name, \"=\", list_args[[i]])\n    df\n  }))\n\n  upper &lt;- if (max(combined_results$valid_loss) &gt; 10) quantile(combined_results$valid_loss, 0.98) else max(combined_results$valid_loss)\n\n  ggplot(combined_results, aes(x = epoch, y = valid_loss, color = config)) +\n    geom_line() +\n    theme_minimal() +\n    labs(x = \"Epoch\", y = \"Validation RMSE\", color = \"Configuration\") +\n    ylim(min(combined_results$valid_loss), upper)\n}\ntrain_loop &lt;- function(network, dl_train, opt) {\n  network$train()\n  coro::loop(for (batch in dl_train) {\n    opt$zero_grad()\n    Y_pred &lt;- network(batch[[1]])\n    loss &lt;- nnf_mse_loss(Y_pred, batch[[2]])\n    loss$backward()\n    opt$step()\n  })\n}\n\nvalid_loop &lt;- function(network, dl_valid) {\n  network$eval()\n  valid_loss &lt;- c()\n  coro::loop(for (batch in dl_valid) {\n    Y_pred &lt;- with_no_grad(network(batch[[1]]))\n    loss &lt;- sqrt(nnf_mse_loss(Y_pred, batch[[2]]))\n    valid_loss &lt;- c(valid_loss, loss$item())\n  })\n  mean(valid_loss)\n}\n\ntrain_valid &lt;- function(network, ds_train, ds_valid, epochs, batch_size, ...) {\n  opt &lt;- optim_ignite_adamw(network$parameters, ...)\n  train_losses &lt;- numeric(epochs)\n  valid_losses &lt;- numeric(epochs)\n  dl_train &lt;- dataloader(ds_train, batch_size = batch_size)\n  dl_valid &lt;- dataloader(ds_valid, batch_size = batch_size)\n  for (epoch in seq_len(epochs)) {\n    train_loop(network, dl_train, opt)\n    valid_losses[epoch] &lt;- valid_loop(network, dl_valid)\n  }\n  data.frame(epoch = seq_len(epochs), valid_loss = valid_losses)\n}\n\n\nYou can e.g. call the function like below:\n\ncompare_configs(epochs = 30, lr = list(0.1, 0.2), weight_decay = 0.02)\n\n\n\n\n\n\n\n\nExplore a few hyperparameter settings and make some observations as to how they affect the trajectory of the validation loss.\nQuestion 2: Optimization with Momentum\nIn this exercise, you will build a gradient descent optimizer with momentum. As a use case, we will minimize the Rosenbrock function. The function is defined as:\n\nrosenbrock &lt;- function(x, y) {\n  (1 - x)^2 + 2 * (y - x^2)^2\n}\nrosenbrock(torch_tensor(-1), torch_tensor(-1))\n\ntorch_tensor\n 12\n[ CPUFloatType{1} ]\n\n\nThe ‘parameters’ we will be optimizing is the position of a point (x, y), which will both be updated using gradient descent. The figure below shows the Rosenbrock function, where darker values indicate lower values.\n\n\n\n\n\n\n\n\n\nThe task is to implement the optim_step() function.\n\noptim_step &lt;- function(x, y, lr, x_momentum, y_momentum, beta) {\n  ...\n}\n\nIt will receive as arguments, the current values x and y, as well as the momentum values x_momentum and y_momentum (all scalar tensors). The function should then:\n\nPerform a forward pass by calling the rosenbrock() function.\nCall $backward() on the result of the forward pass.\nFirst multiply both momentum values in-place using $mul_() with beta, then add (1 - beta) times the gradient to the momentum values using $add_().\nUpdate the parameters using $sub_() with lr * x_momentum and lr * y_momentum. Perform this update within with_no_grad() as we don’t want to track the gradients for the parameters.\n\nTo perform in-place updates, you can use the $mul_() and $add_() methods.\nThe update rule is given exemplarily for x:\n\\[\n\\begin{aligned}\nv_{t + 1} &= \\beta v_t + (1 - \\beta) \\nabla_{x} f(x_t, y_t) \\\\\n\\end{aligned}\n\\]\n\\[\n\\begin{aligned}\nx_{t+1} &= x_t - \\eta v_{t + 1}\n\\end{aligned}\n\\]\nTo test your optimizer, you can use the following code:\n\noptimize_rosenbrock &lt;- function(steps, lr, beta) {\n  x &lt;- torch_tensor(-1, requires_grad = TRUE)\n  y &lt;- torch_tensor(2, requires_grad = TRUE)\n  momentum_x &lt;- torch_tensor(0)\n  momentum_y &lt;- torch_tensor(0)\n\n  trajectory &lt;- data.frame(\n    x = numeric(steps + 1),\n    y = numeric(steps + 1),\n    value = numeric(steps + 1)\n  )\n  for (step in seq_len(steps)){\n    optim_step(x, y, lr, momentum_x, momentum_y, beta)\n    x$grad$zero_()\n    y$grad$zero_()\n    trajectory$x[step] &lt;- x$item()\n    trajectory$y[step] &lt;- y$item()\n  }\n  trajectory$x[steps + 1] &lt;- x$item()\n  trajectory$y[steps + 1] &lt;- y$item()\n\n  plot_rosenbrock() +\n    geom_path(data = trajectory, aes(x = x, y = y, z = NULL), color = \"red\") +\n    labs(title = \"Optimization Path on Rosenbrock Function\", x = \"X-axis\", y = \"Y-axis\")\n}\n\nQuestion 3: Weight Decay\nIn exercise 2, we have optimized the Rosenbrock function. Does it make sense to also use weight decay for this optimization?"
  },
  {
    "objectID": "notebooks/4-optimizer-exercise-solution.html",
    "href": "notebooks/4-optimizer-exercise-solution.html",
    "title": "Optimizer Exercises",
    "section": "",
    "text": "Question 1\nIn this exercise, the task is to play around with the settings for the optimization of a neural network. We start by generating some (highly non-linear) synthetic data using the mlbench package.\n\nlibrary(torch)\ndata &lt;- mlbench::mlbench.friedman3(n = 3000, sd = 0.1)\nX &lt;- torch_tensor(data$x)\nX[1:2, ]\n\ntorch_tensor\n   40.8977   745.3378     0.3715     3.4246\n   88.3017  1148.7073     0.5288     6.5953\n[ CPUFloatType{2,4} ]\n\nY &lt;- torch_tensor(data$y)$unsqueeze(2)\nY[1:2, ]\n\ntorch_tensor\n 1.5238\n 1.3572\n[ CPUFloatType{2,1} ]\n\n\nThe associated machine learning task is to predict the output Y from the input X.\nNext, we create a dataset for it using the tensor_dataset() function.\n\nds &lt;- tensor_dataset(X, Y)\nds$.getitem(1)\n\n[[1]]\ntorch_tensor\n  40.8977\n 745.3378\n   0.3715\n   3.4246\n[ CPUFloatType{4} ]\n\n[[2]]\ntorch_tensor\n 1.5238\n[ CPUFloatType{1} ]\n\n\nWe can create two sub-datasets – for training and validation – using dataset_subset().\n\nids_train &lt;- sample(1000, 700)\nids_valid &lt;- setdiff(seq_len(1000), ids_train)\nds_train &lt;- dataset_subset(ds, ids_train)\nds_valid &lt;- dataset_subset(ds, ids_valid)\n\nThe network that we will be fitting is a simple MLP:\n\nnn_mlp &lt;- nn_module(\"nn_mlp\",\n  initialize = function() {\n    self$lin1 &lt;- nn_linear(4, 50)\n    self$lin2 &lt;- nn_linear(50, 50)\n    self$lin3 &lt;- nn_linear(50, 1)\n  },\n  forward = function(x) {\n    x |&gt;\n      self$lin1() |&gt;\n      nnf_relu() |&gt;\n      self$lin2() |&gt;\n      nnf_relu() |&gt;\n      self$lin3()\n  }\n)\n\nThe code to compare different optimizer configurations is provided through the (provided) compare_configs() function. It takes as arguments:\n\nepochs: The number of epochs to train for. Defaults to 30.\nbatch_size: The batch size to use for training. Defaults to 16.\nlr: The learning rate to use for training. Defaults to 0.01.\nweight_decay: The weight decay to use for training. Defaults to 0.01.\nbeta1: The momentum parameter to use for training. Defaults to 0.9.\nbeta2: The adaptive step size parameter to use for training. Defaults to 0.999.\n\nOne of the arguments (except for epochs) must be a list of values. The function will then run the same training configuration for each of the values in the list and visualize the results.\n\n\nImplementation of compare_configs\n\n\nlibrary(ggplot2)\ncompare_configs &lt;- function(epochs = 30, batch_size = 16, lr = 0.01, weight_decay = 0.01, beta1 = 0.9, beta2 = 0.999) {\n  # Identify which parameter is a list\n  args &lt;- list(batch_size = batch_size, lr = lr, weight_decay = weight_decay, beta1 = beta1, beta2 = beta2)\n  is_list &lt;- sapply(args, is.list)\n\n  if (sum(is_list) != 1) {\n    stop(\"One of the arguments must be a list\")\n  }\n\n  list_arg_name &lt;- names(args)[is_list]\n  list_args &lt;- args[[list_arg_name]]\n  other_args &lt;- args[!is_list]\n\n  # Run train_valid for each value in the list\n  results &lt;- lapply(list_args, function(arg) {\n    network &lt;- with_torch_manual_seed(seed = 123, nn_mlp())\n    other_args[[list_arg_name]] &lt;- arg\n    train_valid(network, ds_train = ds_train, ds_valid = ds_valid, epochs = epochs, batch_size = other_args$batch_size,\n      lr = other_args$lr, betas = c(other_args$beta1, other_args$beta2), weight_decay = other_args$weight_decay)\n  })\n\n  # Combine results into a single data frame\n  combined_results &lt;- do.call(rbind, lapply(seq_along(results), function(i) {\n    df &lt;- results[[i]]\n    df$config &lt;- paste(list_arg_name, \"=\", list_args[[i]])\n    df\n  }))\n\n  upper &lt;- if (max(combined_results$valid_loss) &gt; 10) quantile(combined_results$valid_loss, 0.98) else max(combined_results$valid_loss)\n\n  ggplot(combined_results, aes(x = epoch, y = valid_loss, color = config)) +\n    geom_line() +\n    theme_minimal() +\n    labs(x = \"Epoch\", y = \"Validation RMSE\", color = \"Configuration\") +\n    ylim(min(combined_results$valid_loss), upper)\n}\ntrain_loop &lt;- function(network, dl_train, opt) {\n  network$train()\n  coro::loop(for (batch in dl_train) {\n    opt$zero_grad()\n    Y_pred &lt;- network(batch[[1]])\n    loss &lt;- nnf_mse_loss(Y_pred, batch[[2]])\n    loss$backward()\n    opt$step()\n  })\n}\n\nvalid_loop &lt;- function(network, dl_valid) {\n  network$eval()\n  valid_loss &lt;- c()\n  coro::loop(for (batch in dl_valid) {\n    Y_pred &lt;- with_no_grad(network(batch[[1]]))\n    loss &lt;- sqrt(nnf_mse_loss(Y_pred, batch[[2]]))\n    valid_loss &lt;- c(valid_loss, loss$item())\n  })\n  mean(valid_loss)\n}\n\ntrain_valid &lt;- function(network, ds_train, ds_valid, epochs, batch_size, ...) {\n  opt &lt;- optim_ignite_adamw(network$parameters, ...)\n  train_losses &lt;- numeric(epochs)\n  valid_losses &lt;- numeric(epochs)\n  dl_train &lt;- dataloader(ds_train, batch_size = batch_size)\n  dl_valid &lt;- dataloader(ds_valid, batch_size = batch_size)\n  for (epoch in seq_len(epochs)) {\n    train_loop(network, dl_train, opt)\n    valid_losses[epoch] &lt;- valid_loop(network, dl_valid)\n  }\n  data.frame(epoch = seq_len(epochs), valid_loss = valid_losses)\n}\n\n\nYou can e.g. call the function like below:\n\ncompare_configs(epochs = 30, lr = list(0.1, 0.2), weight_decay = 0.02)\n\n\n\n\n\n\n\n\nExplore a few hyperparameter settings and make some observations as to how they affect the trajectory of the validation loss.\nSolution\nThere is not really a ‘solution’ to this exercise. We will go through some of the configurations and explain what is happening.\nLearning rate\n\ncompare_configs(epochs = 30, lr = list(1,  0.1, 0.01, 0.001))\n\n\n\n\n\n\n\n\nBatch Size\nFor this configuration, we can see that larger batch sizes lead to considerably better results as they allow for more updates for the given number of epochs.\n\ncompare_configs(epochs = 30, batch_size = list(2, 4, 8, 16, 32, 64))\n\nWarning: Removed 4 rows containing missing values or values outside the scale range (`geom_line()`).\n\n\n\n\n\n\n\n\n\nWeight Decay\nFor too large values of the weight decay, we see that the network struggles to get a good validation loss.\n\ncompare_configs(epochs = 30, weight_decay = list(0.2, 0.1, 0.05, 0.025, 0.0125))\n\n\n\n\n\n\n\n\nBeta 1\nFor the momentum parameter, we can observe that too large values for the momentum parameter lead to oscillations because the local information is not used enough.\n\ncompare_configs(epochs = 30, beta1 = list(0.5, 0.85, 0.9, 0.95, 0.99))\n\n\n\n\n\n\n\n\nBeta 2\nFor larger values of beta2, the loss trajectory is considerably smoother. While smaller values will – in this scenario – lead to more oscillations, they also lead to a better fit. Once the value of beta2 is getting too small, the performance will also deteriorate.\n\ncompare_configs(epochs = 30, beta2 = list(0.5, 0.6, 0.7, 0.8, 0.9, 0.99, 0.999, 0.9999))\n\n\n\n\n\n\n\n\nQuestion 2: Optimization with Momentum\nIn this exercise, you will build a gradient descent optimizer with momentum. As a use case, we will minimize the Rosenbrock function. The function is defined as:\n\nrosenbrock &lt;- function(x, y) {\n  (1 - x)^2 + 2 * (y - x^2)^2\n}\nrosenbrock(torch_tensor(-1), torch_tensor(-1))\n\ntorch_tensor\n 12\n[ CPUFloatType{1} ]\n\n\nThe ‘parameters’ we will be optimizing is the position of a point (x, y), which will both be updated using gradient descent. The figure below shows the Rosenbrock function, where darker values indicate lower values.\n\n\n\n\n\n\n\n\n\nThe task is to implement the optim_step() function.\n\noptim_step &lt;- function(x, y, lr, x_momentum, y_momentum, beta) {\n  ...\n}\n\nIt will receive as arguments, the current values x and y, as well as the momentum values x_momentum and y_momentum (all scalar tensors). The function should then:\n\nPerform a forward pass by calling the rosenbrock() function.\nCall $backward() on the result of the forward pass.\nFirst multiply both momentum values in-place using $mul_() with beta, then add (1 - beta) times the gradient to the momentum values using $add_().\nUpdate the parameters using $sub_() with lr * x_momentum and lr * y_momentum. Perform this update within with_no_grad() as we don’t want to track the gradients for the parameters.\n\nTo perform in-place updates, you can use the $mul_() and $add_() methods.\nThe update rule is given exemplarily for x:\n\\[\n\\begin{aligned}\nv_{t + 1} &= \\beta v_t + (1 - \\beta) \\nabla_{x} f(x_t, y_t) \\\\\n\\end{aligned}\n\\]\n\\[\n\\begin{aligned}\nx_{t+1} &= x_t - \\eta v_{t + 1}\n\\end{aligned}\n\\]\nTo test your optimizer, you can use the following code:\n\noptimize_rosenbrock &lt;- function(steps, lr, beta) {\n  x &lt;- torch_tensor(-1, requires_grad = TRUE)\n  y &lt;- torch_tensor(2, requires_grad = TRUE)\n  momentum_x &lt;- torch_tensor(0)\n  momentum_y &lt;- torch_tensor(0)\n\n  trajectory &lt;- data.frame(\n    x = numeric(steps + 1),\n    y = numeric(steps + 1),\n    value = numeric(steps + 1)\n  )\n  for (step in seq_len(steps)){\n    optim_step(x, y, lr, momentum_x, momentum_y, beta)\n    x$grad$zero_()\n    y$grad$zero_()\n    trajectory$x[step] &lt;- x$item()\n    trajectory$y[step] &lt;- y$item()\n  }\n  trajectory$x[steps + 1] &lt;- x$item()\n  trajectory$y[steps + 1] &lt;- y$item()\n\n  plot_rosenbrock() +\n    geom_path(data = trajectory, aes(x = x, y = y, z = NULL), color = \"red\") +\n    labs(title = \"Optimization Path on Rosenbrock Function\", x = \"X-axis\", y = \"Y-axis\")\n}\n\nSolution\n\noptim_step &lt;- function(x, y, lr, x_momentum, y_momentum, beta) {\n  value &lt;- rosenbrock(x, y)\n  value$backward()\n  x_momentum$mul_(beta)\n  x_momentum$add_((1 - beta) * x$grad)\n  y_momentum$mul_(beta)\n  y_momentum$add_((1 - beta) * y$grad)\n  with_no_grad({\n    x$sub_(lr * x_momentum)\n    y$sub_(lr * y_momentum)\n  })\n  value$item()\n}\noptimize_rosenbrock(steps = 500, lr = 0.01, beta = 0.9)\n\n\n\n\n\n\n\n\nQuestion 3: Weight Decay\nIn exercise 2, we have optimized the Rosenbrock function. Does it make sense to also use weight decay for this optimization?\nSolution No, it does not make sense to use weight decay for this optimization as it is intended to prevent overfitting which is not a problem in this case. This is, because there is no uncertainty in the function we are optimizing."
  },
  {
    "objectID": "notebooks/3-modules-data-exercise.html",
    "href": "notebooks/3-modules-data-exercise.html",
    "title": "It’s a Sin(us)",
    "section": "",
    "text": "Question 1: Create a torch::dataset that takes in arguments n, min, and max where:\n\nn is the total number of samples\nmin is the lower bound of the data\nmax is the upper bound of the data\n\nIn the initialize method, generate and store:\n\na 2D tensor x of n values drawn from a uniform distribution between min and max\na 2D tensor y that is defined as \\(sin(x) + \\epsilon\\) where \\(\\epsilon\\) is drawn from a normal distribution with mean 0 and standard deviation 0.1\n\nThe dataset should return a named list with values x and y.\nThen, create an instance of the dataset with n = 1000, min = 0, and max = 10.\nMake sure that the dataset is working by either calling its $.getitem() or $.getbatch() method depending on what you implemented. Also, check that the shapes of both tensors returned by the dataset are (n_batch, 1).\nQuestion 2: Create a torch::dataloader that takes in the dataset and returns batches of size 10. Create one tensor X and one tensor Y that contains the concatenated batches of x and y.\n\n\nHint\n\nThe functions coro::loop() and torch_cat() might be helpful.\n\nQuestion 3: Create a custom torch module that allows modeling the sinus data we have created. To test it, apply it to the tensor X we have created above and calculate its mean squared error with the tensor Y.\n\n\nHint\n\nYou can either use nn_module to create a custom module generically, or you can use nn_sequential() to create a custom module that is a sequence of layers.\n\nQuestion 4: Train the model on the task for different hyperparameters (lr or epochs) and visualize the results. Play around with the hyperparameters until you get a good fit. You can use the following code for that:\n\nlibrary(ggplot2)\npredict_network &lt;- function(net, dataloader) {\n  local_no_grad()\n  xs &lt;- list(x = numeric(), y = numeric(), pred = numeric())\n  i &lt;- 1\n  net$eval()\n  coro::loop(for (batch in dataloader) {\n    xs$x &lt;- c(xs$x, as.numeric(batch$x))\n    xs$y &lt;- c(xs$y, as.numeric(batch$y))\n    xs$pred &lt;- c(xs$pred, as.numeric(net(batch$x)))\n  })\n  as.data.frame(xs)\n}\ntrain_network &lt;- function(net, dataloader, epochs, lr) {\n  optimizer &lt;- optim_ignite_adamw(net$parameters, lr = lr)\n  net$train()\n  for (i in seq_len(epochs)) {\n    coro::loop(for (batch in dataloader) {\n      optimizer$zero_grad()\n      Y_pred &lt;- net(batch$x)\n      loss &lt;- nnf_mse_loss(Y_pred, batch$y)\n      loss$backward()\n      optimizer$step()\n    })\n  }\n  predict_network(net, dataloader)\n}\nplot_results &lt;- function(df) {\n  ggplot(data = df, aes(x = x)) +\n    geom_point(aes(y = y, color = \"true\")) +\n    geom_point(aes(y = pred, color = \"pred\")) +\n    theme_minimal()\n}\ntrain_and_plot &lt;- function(net, dataloader, epochs = 10, lr = 0.01) {\n  result &lt;- train_network(net, dataloader, epochs = epochs, lr = lr)\n  plot_results(result)\n}\n\n\n\n\n\n\n\nTip\n\n\n\nBeware of the reference semantics and make sure that you create a new instance of the network for each run.\n\n\nQuestion 5: Create a new instance from the sinus dataset class created earlier. Now, set the min and max values to 10 and 20 respectively and visualize the results. What do you observe? Can you explain why this is happening and can you fix the network architecture to make it work?\n\n\nHint\n\nThe sinus function has a phase of \\(2 \\pi\\)."
  },
  {
    "objectID": "notebooks/3-modules-data-exercise-solution.html",
    "href": "notebooks/3-modules-data-exercise-solution.html",
    "title": "It’s a Sin(us)",
    "section": "",
    "text": "Question 1: Create a torch::dataset that takes in arguments n, min, and max where:\n\nn is the total number of samples\nmin is the lower bound of the data\nmax is the upper bound of the data\n\nIn the initialize method, generate and store:\n\na 2D tensor x of n values drawn from a uniform distribution between min and max\na 2D tensor y that is defined as \\(sin(x) + \\epsilon\\) where \\(\\epsilon\\) is drawn from a normal distribution with mean 0 and standard deviation 0.1\n\nThe dataset should return a named list with values x and y.\nThen, create an instance of the dataset with n = 1000, min = 0, and max = 10.\nMake sure that the dataset is working by either calling its $.getitem() or $.getbatch() method depending on what you implemented. Also, check that the shapes of both tensors returned by the dataset are (n_batch, 1).\nQuestion 2: Create a torch::dataloader that takes in the dataset and returns batches of size 10. Create one tensor X and one tensor Y that contains the concatenated batches of x and y.\n\n\nHint\n\nThe functions coro::loop() and torch_cat() might be helpful.\n\nQuestion 3: Create a custom torch module that allows modeling the sinus data we have created. To test it, apply it to the tensor X we have created above and calculate its mean squared error with the tensor Y.\n\n\nHint\n\nYou can either use nn_module to create a custom module generically, or you can use nn_sequential() to create a custom module that is a sequence of layers.\n\nQuestion 4: Train the model on the task for different hyperparameters (lr or epochs) and visualize the results. Play around with the hyperparameters until you get a good fit. You can use the following code for that:\n\nlibrary(ggplot2)\npredict_network &lt;- function(net, dataloader) {\n  local_no_grad()\n  xs &lt;- list(x = numeric(), y = numeric(), pred = numeric())\n  i &lt;- 1\n  net$eval()\n  coro::loop(for (batch in dataloader) {\n    xs$x &lt;- c(xs$x, as.numeric(batch$x))\n    xs$y &lt;- c(xs$y, as.numeric(batch$y))\n    xs$pred &lt;- c(xs$pred, as.numeric(net(batch$x)))\n  })\n  as.data.frame(xs)\n}\ntrain_network &lt;- function(net, dataloader, epochs, lr) {\n  optimizer &lt;- optim_ignite_adamw(net$parameters, lr = lr)\n  net$train()\n  for (i in seq_len(epochs)) {\n    coro::loop(for (batch in dataloader) {\n      optimizer$zero_grad()\n      Y_pred &lt;- net(batch$x)\n      loss &lt;- nnf_mse_loss(Y_pred, batch$y)\n      loss$backward()\n      optimizer$step()\n    })\n  }\n  predict_network(net, dataloader)\n}\nplot_results &lt;- function(df) {\n  ggplot(data = df, aes(x = x)) +\n    geom_point(aes(y = y, color = \"true\")) +\n    geom_point(aes(y = pred, color = \"pred\")) +\n    theme_minimal()\n}\ntrain_and_plot &lt;- function(net, dataloader, epochs = 10, lr = 0.01) {\n  result &lt;- train_network(net, dataloader, epochs = epochs, lr = lr)\n  plot_results(result)\n}\n\n\n\n\n\n\n\nTip\n\n\n\nBeware of the reference semantics and make sure that you create a new instance of the network for each run.\n\n\nQuestion 5: Create a new instance from the sinus dataset class created earlier. Now, set the min and max values to 10 and 20 respectively and visualize the results. What do you observe? Can you explain why this is happening and can you fix the network architecture to make it work?\n\n\nHint\n\nThe sinus function has a phase of \\(2 \\pi\\)."
  },
  {
    "objectID": "notebooks/2-autograd-exercise.html",
    "href": "notebooks/2-autograd-exercise.html",
    "title": "Autograd",
    "section": "",
    "text": "Question 1: Appreciating autograd\nConsider the following function:\n\\[\nf(x) = x^2 + 3x + 2\n\\]\nAs well as the function \\(g(x) = f(f(f(x)))\\)\nCalculate the gradient of both functions at point \\(x = 2\\).\nQuestion 2: Approximating functions with gradients\nThe defining feature of the gradient is that it allows us to approximate the function locally by a linear function.\nI.e., for some value \\(x^*\\), we know for very small \\(\\delta\\), that\n\\[\nf(x^* + \\delta) \\approx f(x^*) + f'(x^*) \\cdot \\delta\n\\]\nPlot the function from earlier as well as the local linear approximation at \\(x = 2\\) using ggplot2.\n\nDefine a function f_approx that takes a value delta and returns the value of the function at x^* + delta using the linear approximation.\nCreate a sequence with 100 equidistant values between -4 to 4 using torch_linspace().\nCreate a data.frame with columns x, y_true, y_approx.\nUse ggplot2 to plot the function and its linear approximation.\n\nQuestion 3: Look ma, I made my own autograd function\nIn this exercise, we will build our own, custom autograd function. While you might rarely need this in practice, it still allows you to get a better understanding of how the autograd system works. There is also a tutorial on this on the torch website.\nTo construct our own autograd function, we need to define:\n\nThe forward pass:\n\nHow to calculate outputs from input\nWhat to save for the backward pass\n\nThe backward pass:\n\nHow to calculate the gradient of the output with respect to the input\n\n\nThe task is to re-create the ReLU activation function, which is a common activation function in neural networks and which is defined as:\n\\[\n\\text{ReLU}(x) = \\max(0, x)\n\\]\nNote that strictly speaking, the ReLU function is not differentiable at \\(x = 0\\) (but a subgradient can be used instead). The derivative/subgradient of the ReLU function is:\n\\[\n\\text{ReLU}'(x) = \\begin{cases}\n1 & \\text{if } x &gt; 0 \\\\\n0 & \\text{if } x \\leq 0 \\\\\n\\end{cases}\n\\]\nIn torch, a custom autograd function can be constructed using autograd_function() function and it accepts arguments forward and backward which are functions that define the forward and backward pass: They both take as first argument a ctx, which is a communication object that is used to save information during the forward pass to be able to compute the gradient in the backward pass. The return value of the backward pass should be a list of gradients with respect to the inputs. To check whether a gradient for an input is needed (has requires_grad = TRUE), you can use ctx$needs_input_grad which is a named list with boolean values for each input.\nThe backward function additionally takes a second argument grad_output, which is the gradient of the output: E.g., if our function is \\(f(x)\\) and we calculate the gradient of \\(g(x) = h(f(x))\\), then grad_output is the derivative of \\(g\\) with respect to its input, evaluated at \\(f(x)\\). This is essentially the chain rule: \\(\\frac{\\partial g}{\\partial x} = \\frac{\\partial g}{\\partial f} \\cdot \\frac{\\partial f}{\\partial x}\\).\nFill out the missing parts (...) in the code below.\n\nrelu &lt;- autograd_function(\n  forward = function(ctx, input) {\n    mask &lt;- ...\n    output &lt;- torch_where(mask, ...)\n    ctx$save_for_backward(mask)\n    output\n  },\n  backward = function(ctx, grad_output) {\n    grads &lt;- list(input = NULL)\n    if (ctx$needs_input_grad$input) {\n      mask &lt;- ctx$saved_variables[[1]]\n      grads$input &lt;- ...\n    }\n    grads\n  }\n)\n\nTo check that it’s working, use the code below (with your relu instead of nnf_relu) and check that the results are the same.\n\nx &lt;- torch_tensor(-1, requires_grad = TRUE)\n(nnf_relu(x)^2)$backward()\nx$grad\n\ntorch_tensor\n 0\n[ CPUFloatType{1} ]\n\nx$grad$zero_()\n\ntorch_tensor\n 0\n[ CPUFloatType{1} ]\n\nx &lt;- torch_tensor(3, requires_grad = TRUE)\n(nnf_relu(x)^2)$backward()\nx$grad\n\ntorch_tensor\n 6\n[ CPUFloatType{1} ]"
  },
  {
    "objectID": "notebooks/2-autograd-exercise-solution.html",
    "href": "notebooks/2-autograd-exercise-solution.html",
    "title": "Autograd",
    "section": "",
    "text": "Question 1: Appreciating autograd\nConsider the following function:\n\\[\nf(x) = x^2 + 3x + 2\n\\]\nAs well as the function \\(g(x) = f(f(f(x)))\\)\nCalculate the gradient of both functions at point \\(x = 2\\).\nSolution\n\nlibrary(torch)\nx &lt;- torch_tensor(2, requires_grad = TRUE)\nf &lt;- function(x) {\n  x^2 + 3 * x + 2\n}\ng &lt;- function(x) {\n  f(f(f(x)))\n}\n\n# Store the gradient, i.e., f'(2)\nf(x)$backward()\ngrad &lt;- x$grad$clone()\ngrad\n\ntorch_tensor\n 7\n[ CPUFloatType{1} ]\n\n# For another backward pass, we reset the gradient as they otherwise accumulate\nx$grad$zero_()\n\ntorch_tensor\n 0\n[ CPUFloatType{1} ]\n\n# Calculate the gradient of g(x)\ng(x)$backward()\n\n# Create a copy of the gradient\ngrad2 &lt;- x$grad$clone()\ngrad2\n\ntorch_tensor\n 69363\n[ CPUFloatType{1} ]\n\n# Zero gradients for good measure\nx$grad$zero_()\n\ntorch_tensor\n 0\n[ CPUFloatType{1} ]\n\n\nQuestion 2: Approximating functions with gradients\nThe defining feature of the gradient is that it allows us to approximate the function locally by a linear function.\nI.e., for some value \\(x^*\\), we know for very small \\(\\delta\\), that\n\\[\nf(x^* + \\delta) \\approx f(x^*) + f'(x^*) \\cdot \\delta\n\\]\nPlot the function from earlier as well as the local linear approximation at \\(x = 2\\) using ggplot2.\n\nDefine a function f_approx that takes a value delta and returns the value of the function at x^* + delta using the linear approximation.\nCreate a sequence with 100 equidistant values between -4 to 4 using torch_linspace().\nCreate a data.frame with columns x, y_true, y_approx.\nUse ggplot2 to plot the function and its linear approximation.\n\nSolution\n\nlibrary(ggplot2)\nx &lt;- x$detach() # No need to track gradients anymore\ndeltas &lt;- torch_linspace(-4, 4, 100)\ny_true &lt;- f(x + deltas)\ny_approx &lt;- f(x) + grad * deltas\n\nd &lt;- data.frame(x = as_array(deltas), y_true = as_array(y_true), y_approx = as_array(y_approx))\n\nggplot(d, aes(x = x)) +\n  geom_line(aes(y = y_true, color = \"True function\")) +\n  geom_line(aes(y = y_approx, color = \"Linear approximation\")) +\n  theme_minimal() +\n  labs(\n    title = \"Gradient as a local linear approximation\",\n    y = \"f(x)\",\n    x = \"x\",\n    colour = \"\"\n  )\n\n\n\n\n\n\n\n\nQuestion 3: Look ma, I made my own autograd function\nIn this exercise, we will build our own, custom autograd function. While you might rarely need this in practice, it still allows you to get a better understanding of how the autograd system works. There is also a tutorial on this on the torch website.\nTo construct our own autograd function, we need to define:\n\nThe forward pass:\n\nHow to calculate outputs from input\nWhat to save for the backward pass\n\nThe backward pass:\n\nHow to calculate the gradient of the output with respect to the input\n\n\nThe task is to re-create the ReLU activation function, which is a common activation function in neural networks and which is defined as:\n\\[\n\\text{ReLU}(x) = \\max(0, x)\n\\]\nNote that strictly speaking, the ReLU function is not differentiable at \\(x = 0\\) (but a subgradient can be used instead). The derivative/subgradient of the ReLU function is:\n\\[\n\\text{ReLU}'(x) = \\begin{cases}\n1 & \\text{if } x &gt; 0 \\\\\n0 & \\text{if } x \\leq 0 \\\\\n\\end{cases}\n\\]\nIn torch, a custom autograd function can be constructed using autograd_function() function and it accepts arguments forward and backward which are functions that define the forward and backward pass: They both take as first argument a ctx, which is a communication object that is used to save information during the forward pass to be able to compute the gradient in the backward pass. The return value of the backward pass should be a list of gradients with respect to the inputs. To check whether a gradient for an input is needed (has requires_grad = TRUE), you can use ctx$needs_input_grad which is a named list with boolean values for each input.\nThe backward function additionally takes a second argument grad_output, which is the gradient of the output: E.g., if our function is \\(f(x)\\) and we calculate the gradient of \\(g(x) = h(f(x))\\), then grad_output is the derivative of \\(g\\) with respect to its input, evaluated at \\(f(x)\\). This is essentially the chain rule: \\(\\frac{\\partial g}{\\partial x} = \\frac{\\partial g}{\\partial f} \\cdot \\frac{\\partial f}{\\partial x}\\).\nFill out the missing parts (...) in the code below.\n\nrelu &lt;- autograd_function(\n  forward = function(ctx, input) {\n    mask &lt;- ...\n    output &lt;- torch_where(mask, ...)\n    ctx$save_for_backward(mask)\n    output\n  },\n  backward = function(ctx, grad_output) {\n    grads &lt;- list(input = NULL)\n    if (ctx$needs_input_grad$input) {\n      mask &lt;- ctx$saved_variables[[1]]\n      grads$input &lt;- ...\n    }\n    grads\n  }\n)\n\nTo check that it’s working, use the code below (with your relu instead of nnf_relu) and check that the results are the same.\n\nx &lt;- torch_tensor(-1, requires_grad = TRUE)\n(nnf_relu(x)^2)$backward()\nx$grad\n\ntorch_tensor\n 0\n[ CPUFloatType{1} ]\n\nx$grad$zero_()\n\ntorch_tensor\n 0\n[ CPUFloatType{1} ]\n\nx &lt;- torch_tensor(3, requires_grad = TRUE)\n(nnf_relu(x)^2)$backward()\nx$grad\n\ntorch_tensor\n 6\n[ CPUFloatType{1} ]\n\n\nSolution\n\nrelu &lt;- autograd_function(\n  forward = function(ctx, input) {\n    mask &lt;- input &gt; 0\n    output &lt;- torch_where(mask, input, torch_tensor(0))\n    ctx$save_for_backward(mask)\n    output\n  },\n  backward = function(ctx, grad_output) {\n    grads &lt;- list(input = NULL)\n    if (ctx$needs_input_grad$input) {\n      mask &lt;- ctx$saved_variables[[1]]\n      grads$input &lt;- grad_output * mask\n    }\n    grads\n  }\n)\n\nx &lt;- torch_tensor(-1, requires_grad = TRUE)\n(relu(x)^2)$backward()\nx$grad\n\ntorch_tensor\n 0\n[ CPUFloatType{1} ]\n\nx$grad$zero_()\n\ntorch_tensor\n 0\n[ CPUFloatType{1} ]\n\nx &lt;- torch_tensor(3, requires_grad = TRUE)\n(relu(x)^2)$backward()\nx$grad\n\ntorch_tensor\n 6\n[ CPUFloatType{1} ]"
  },
  {
    "objectID": "notebooks/1-tensor-exercise.html",
    "href": "notebooks/1-tensor-exercise.html",
    "title": "Tensors",
    "section": "",
    "text": "Note\n\n\n\nTo solve these exercises, consulting the torch function reference can be helpful.\n\n\nQuestion 1: Tensor creation and manipulation\nRecreate this torch tensor:\n\n\ntorch_tensor\n 1  2  3\n 4  5  6\n[ CPULongType{2,3} ]\n\n\n\n\nHint\n\nFirst create an R matrix and then convert it using torch_tensor().\n\nNext, create a view of the tensor so it looks like this:\n\n\ntorch_tensor\n 1  2\n 3  4\n 5  6\n[ CPULongType{3,2} ]\n\n\n\n\nHint\n\nUse the $view() method and pass the desired shape as a vector.\n\nHow can you check programmatically that you successfully created a view, and not a copy (i.e. changing one does change the other)?\n\n\nHint\n\nSee what happens when you modify one of the tensors.\n\nQuestion 2: More complex reshaping\nConsider the following tensor:\n\nx &lt;- torch_tensor(1:6)\nx\n\ntorch_tensor\n 1\n 2\n 3\n 4\n 5\n 6\n[ CPULongType{6} ]\n\n\nReshape it so it looks like this.\n\n\ntorch_tensor\n 1  3  5\n 2  4  6\n[ CPULongType{2,3} ]\n\n\n\n\nHint\n\nFirst reshape to (2, 3) and then $permute() the two dimensions.\n\nQuestion 3: Broadcasting\nConsider the following vectors:\n\nx1 &lt;- torch_tensor(c(1, 2))\nx1\n\ntorch_tensor\n 1\n 2\n[ CPUFloatType{2} ]\n\nx2 &lt;- torch_tensor(c(3, 7))\nx2\n\ntorch_tensor\n 3\n 7\n[ CPUFloatType{2} ]\n\n\nPredict the result (shape and values) of the following operation by applying the broadcasting rules.\n\nx1 + x2$reshape(c(2, 1))\n\nQuestion 4: Handling Singleton dimensions\nA common operation in deep learning is to add or get rid of singleton dimensions, i.e., dimensions of size 1. As this is so common, torch offers a $squeeze() and $unsqueeze() method to add and remove singleton dimensions.\nUse these two functions to first remove the second dimension and then add one in the first position.\n\nx &lt;- torch_randn(2, 1)\nx\n\ntorch_tensor\n-0.1115\n 0.1204\n[ CPUFloatType{2,1} ]\n\n\nQuestion 5: Matrix multiplication\nGenerate a random matrix \\(A\\) of shape (10, 5) and a random matrix \\(B\\) of shape (10, 5) by sampling from a standard normal distribution.\n\n\nHint\n\nUse torch_randn(nrow, ncol) to generate random matrices.\n\nCan you multiply these two matrices with each other and if so, in which order? If not, generate two random matrices with compatible shapes and multiply them.\nQuestion 6: Uniform sampling\nGenerate 10 random variables from a uniform distribution (using only torch functions) in the interval \\([10, 20]\\). Use torch_rand() for this (which does not allow for min and max parameters).\n\n\nHint\n\nAdd the lower bound and multiply with the width of the interval.\n\nQuestion 7: Don’t touch this\nConsider the code below:\n\nf &lt;- function(x) {\n  x[1] &lt;- torch_tensor(-99)\n  return(x)\n}\nx &lt;- torch_tensor(1:3)\ny &lt;- f(x)\nx\n\ntorch_tensor\n-99\n  2\n  3\n[ CPULongType{3} ]\n\n\nImplement a new different version of this function that returns the same tensor but does not change the value of the input tensor in-place.\n\n\nHint\n\nThe $clone() method might be helpful."
  },
  {
    "objectID": "notebooks/1-tensor-exercise-solution.html",
    "href": "notebooks/1-tensor-exercise-solution.html",
    "title": "Tensors",
    "section": "",
    "text": "Note\n\n\n\nTo solve these exercises, consulting the torch function reference can be helpful.\n\n\nQuestion 1: Tensor creation and manipulation\nRecreate this torch tensor:\n\n\ntorch_tensor\n 1  2  3\n 4  5  6\n[ CPULongType{2,3} ]\n\n\n\n\nHint\n\nFirst create an R matrix and then convert it using torch_tensor().\n\nNext, create a view of the tensor so it looks like this:\n\n\ntorch_tensor\n 1  2\n 3  4\n 5  6\n[ CPULongType{3,2} ]\n\n\n\n\nHint\n\nUse the $view() method and pass the desired shape as a vector.\n\nHow can you check programmatically that you successfully created a view, and not a copy (i.e. changing one does change the other)?\n\n\nHint\n\nSee what happens when you modify one of the tensors.\n\nSolution\nWe start by creating the tensor:\n\nx &lt;- torch_tensor(matrix(1:6, byrow = TRUE, nrow = 2))\nx\n\ntorch_tensor\n 1  2  3\n 4  5  6\n[ CPULongType{2,3} ]\n\n\nThen, we create a view of the tensor:\n\ny &lt;- x$view(c(3, 2))\n\nTo check that we created a view, we can modify one of the tensors and see if the other one changes:\n\nx[1, 1] &lt;- 100\ny\n\ntorch_tensor\n 100    2\n   3    4\n   5    6\n[ CPULongType{3,2} ]\n\n\nQuestion 2: More complex reshaping\nConsider the following tensor:\n\nx &lt;- torch_tensor(1:6)\nx\n\ntorch_tensor\n 1\n 2\n 3\n 4\n 5\n 6\n[ CPULongType{6} ]\n\n\nReshape it so it looks like this.\n\n\ntorch_tensor\n 1  3  5\n 2  4  6\n[ CPULongType{2,3} ]\n\n\n\n\nHint\n\nFirst reshape to (2, 3) and then $permute() the two dimensions.\n\nSolution When reshaping, we start by filling up rows, then columns (and then higher dimensions). We therefore first reshape to (3, 2) and then permute the two dimensions to get the desired shape (2, 3).\n\nx &lt;- x$reshape(c(3, 2))\nx\n\ntorch_tensor\n 1  2\n 3  4\n 5  6\n[ CPULongType{3,2} ]\n\nx$permute(c(2, 1))\n\ntorch_tensor\n 1  3  5\n 2  4  6\n[ CPULongType{2,3} ]\n\n\nQuestion 3: Broadcasting\nConsider the following vectors:\n\nx1 &lt;- torch_tensor(c(1, 2))\nx1\n\ntorch_tensor\n 1\n 2\n[ CPUFloatType{2} ]\n\nx2 &lt;- torch_tensor(c(3, 7))\nx2\n\ntorch_tensor\n 3\n 7\n[ CPUFloatType{2} ]\n\n\nPredict the result (shape and values) of the following operation by applying the broadcasting rules.\n\nx1 + x2$reshape(c(2, 1))\n\nSolution\nThe result is the following tensor:\n\n\ntorch_tensor\n 4  5\n 8  9\n[ CPUFloatType{2,2} ]\n\n\nWe will now show how to arrive at this step by step. According to the broadcasting rules, we start by adding a singleton dimension to the first tensor:\n\nx1 &lt;- x1$reshape(c(1, 2))\n\nNow, we have a tensor of shape (1, 2) and a tensor of shape (2, 1). Next, we extend the first tensor along the first dimension to match the second tensor:\n\nx1 &lt;- x1$expand(c(2, 2))\n\nWe do this analogously for the second (reshaped) tensor:\n\nx2 &lt;- x2$reshape(c(2, 1))$expand(c(2, 2))\n\n\nx1 + x2\n\ntorch_tensor\n 4  5\n 8  9\n[ CPUFloatType{2,2} ]\n\n\nQuestion 4: Handling Singleton dimensions\nA common operation in deep learning is to add or get rid of singleton dimensions, i.e., dimensions of size 1. As this is so common, torch offers a $squeeze() and $unsqueeze() method to add and remove singleton dimensions.\nUse these two functions to first remove the second dimension and then add one in the first position.\n\nx &lt;- torch_randn(2, 1)\nx\n\ntorch_tensor\n-0.1115\n 0.1204\n[ CPUFloatType{2,1} ]\n\n\nSolution\n\nx$squeeze(2)$unsqueeze(1)\n\ntorch_tensor\n-0.1115  0.1204\n[ CPUFloatType{1,2} ]\n\n\nQuestion 5: Matrix multiplication\nGenerate a random matrix \\(A\\) of shape (10, 5) and a random matrix \\(B\\) of shape (10, 5) by sampling from a standard normal distribution.\n\n\nHint\n\nUse torch_randn(nrow, ncol) to generate random matrices.\n\nCan you multiply these two matrices with each other and if so, in which order? If not, generate two random matrices with compatible shapes and multiply them.\nSolution\nWe can only multiply a matrix of shape (n, k) with a matrix of shape (k, m) if the number of columns in the first matrix matches the number of rows in the second matrix.\nWe can therefore not multiply the two matrices with each other in either order. To generate two random matrices with compatible shapes, we can generate two random matrices with shape (10, 5) and (5, 10).\n\nA &lt;- torch_randn(10, 5)\nB &lt;- torch_randn(5, 10)\nA$matmul(B)\n\ntorch_tensor\n-1.4311  0.6090 -1.4795 -0.6977  2.4857 -0.7402  0.4060 -0.4299  2.9035  0.1459\n-4.0841  3.8794 -1.5376 -3.5270  4.8175 -0.7630  0.1188  3.0368  1.0634  0.0011\n-0.3880 -1.4639 -1.3191 -0.0589  3.1754 -3.1779  1.7006  0.0521  5.0765  0.0552\n 1.6030 -2.2295  1.1606  3.3083  3.3677  1.5567 -2.3565 -5.1759 -1.9122  5.1734\n 4.0126 -4.3978  0.5547  1.9958 -3.4347 -2.2880  2.1990  0.2017  2.6702 -1.7145\n 0.8548  3.0118 -2.0971 -3.3564 -8.1899  3.3494  1.5969  4.4134  0.4593 -6.8904\n 0.0597 -0.1650 -2.5737 -1.1190  6.1582 -0.6400  0.8576  0.2152  5.0070  1.6070\n 0.2675  2.4575 -2.6582 -3.1801 -3.0074  2.0887  1.4936  3.5447  2.3877 -4.3110\n-3.7894  1.8938  0.0528 -0.9525  0.3706 -1.8813  0.0365  0.2768  0.2025 -0.8839\n 2.7060 -2.1856  1.0679  2.6758 -6.8991  1.6866 -0.2875 -2.8479 -1.4630 -1.6319\n[ CPUFloatType{10,10} ]\n\n\nQuestion 6: Uniform sampling\nGenerate 10 random variables from a uniform distribution (using only torch functions) in the interval \\([10, 20]\\). Use torch_rand() for this (which does not allow for min and max parameters).\n\n\nHint\n\nAdd the lower bound and multiply with the width of the interval.\n\nSolution Because the uniform distribution of torch has no min and max parameters like runif(), we instead sample from a standard uniform distribution and then scale and shift it to the desired interval.\nThen, calculate the mean of the values that are larger than 15.\n\nn &lt;- 10\na &lt;- 10\nb &lt;- 20\nx &lt;- torch_rand(n) * (b - a) + a\n\n\nmean(x[x &gt; 15])\n\ntorch_tensor\n16.4606\n[ CPUFloatType{} ]\n\n\nQuestion 7: Don’t touch this\nConsider the code below:\n\nf &lt;- function(x) {\n  x[1] &lt;- torch_tensor(-99)\n  return(x)\n}\nx &lt;- torch_tensor(1:3)\ny &lt;- f(x)\nx\n\ntorch_tensor\n-99\n  2\n  3\n[ CPULongType{3} ]\n\n\nImplement a new different version of this function that returns the same tensor but does not change the value of the input tensor in-place.\n\n\nHint\n\nThe $clone() method might be helpful.\n\nSolution\nWe need to $clone() the tensor before we modify it.\n\ng &lt;- function(x) {\n  x &lt;- x$clone()\n  x[1] &lt;- torch_tensor(-99)\n  x\n}\nx &lt;- torch_tensor(1:3)\ny &lt;- g(x)\nx\n\ntorch_tensor\n 1\n 2\n 3\n[ CPULongType{3} ]"
  },
  {
    "objectID": "notebooks/0-exercise-quide.html",
    "href": "notebooks/0-exercise-quide.html",
    "title": "Exercise Guide",
    "section": "",
    "text": "Tutorial and Exercise\nMost of the exercises are similar to the examples from the tutorial.\nAI Autocompletion\nIf you have AI autocompletion, we recommend disabling it, as GPT-4 already knows the torch API.\nHints\nHints give additional information that can help you solve the exercise. Don’t hesitate to open them when you are stuck.\nCPU vs. CUDA\nAll exercises can be solved with a CPU only.\nUncertainty\nTo make the exercises run fast, we are often using datasets with few observations and resamplings without too many iterations. This means that there is often uncertainty in the results that we mostly ignore, but which would have to be taken account in a proper analysis."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Deep Learning with (mlr3)torch in R 🔥",
    "section": "",
    "text": "This is a course, containing six tutorials and corresponding exercises (with solutions). The topics are:\n\nTorch Tensors\nAutograd\nModules and Data\nOptimizers\nIntro to mlr3torch (and mlr3 recap)\nTraining Efficiency\nUse Case\n\n\n\n\nIn order to be able to successfully run the notebooks and solve the exercises, make sure that you have the right environment set up.\n\n\nThese notebooks were developed using R 4.4.2, so ideally you should use the same version. You can check your current R version by running R --version in your terminal. If you have the same R version, you can skip the rest of this section.\nA convenient way to simultaneously maintain different versions of R and to easily switch between them is to use rig, the R installation manager. It’s documentation contains instructions on how to install it on Windows, macOS, and Linux.\nAfter having installed rig, you can install R 4.4.2 and make it the default by running the following command in your terminal:\nrig add 4.4.2\nrig default 4.4.2\nVerify that the R version is 4.4.2 by running R --version in your terminal.\n\n\n\nFor managing libraries, we use the renv package. If you don’t have renv installed, you can install it by running the following command:\ninstall.packages(\"renv\")\nNext, initialize the renv environment by running the following command:\nrenv::init()\nFinally, restore the renv environment by running the following command. This might take some time as it downloads all the required libraries.\nrenv::restore()\nTo be able to use the torch package, you need to also run this additional command:\ntorch::install_torch()\nIf this completes successfully, you are ready to go! 🚀\nTo optionally check whether you have GPU support installed (this is not necessary for the exercises), run the following command:\ntorch::cuda_is_available()\n\n\n\n\nSome of the content is based on the book Deep Learning and Scientific Computing with R torch by Sigrid Keydana."
  },
  {
    "objectID": "index.html#overview",
    "href": "index.html#overview",
    "title": "Deep Learning with (mlr3)torch in R 🔥",
    "section": "",
    "text": "This is a course, containing six tutorials and corresponding exercises (with solutions). The topics are:\n\nTorch Tensors\nAutograd\nModules and Data\nOptimizers\nIntro to mlr3torch (and mlr3 recap)\nTraining Efficiency\nUse Case"
  },
  {
    "objectID": "index.html#setup",
    "href": "index.html#setup",
    "title": "Deep Learning with (mlr3)torch in R 🔥",
    "section": "",
    "text": "In order to be able to successfully run the notebooks and solve the exercises, make sure that you have the right environment set up.\n\n\nThese notebooks were developed using R 4.4.2, so ideally you should use the same version. You can check your current R version by running R --version in your terminal. If you have the same R version, you can skip the rest of this section.\nA convenient way to simultaneously maintain different versions of R and to easily switch between them is to use rig, the R installation manager. It’s documentation contains instructions on how to install it on Windows, macOS, and Linux.\nAfter having installed rig, you can install R 4.4.2 and make it the default by running the following command in your terminal:\nrig add 4.4.2\nrig default 4.4.2\nVerify that the R version is 4.4.2 by running R --version in your terminal.\n\n\n\nFor managing libraries, we use the renv package. If you don’t have renv installed, you can install it by running the following command:\ninstall.packages(\"renv\")\nNext, initialize the renv environment by running the following command:\nrenv::init()\nFinally, restore the renv environment by running the following command. This might take some time as it downloads all the required libraries.\nrenv::restore()\nTo be able to use the torch package, you need to also run this additional command:\ntorch::install_torch()\nIf this completes successfully, you are ready to go! 🚀\nTo optionally check whether you have GPU support installed (this is not necessary for the exercises), run the following command:\ntorch::cuda_is_available()"
  },
  {
    "objectID": "index.html#credit",
    "href": "index.html#credit",
    "title": "Deep Learning with (mlr3)torch in R 🔥",
    "section": "",
    "text": "Some of the content is based on the book Deep Learning and Scientific Computing with R torch by Sigrid Keydana."
  },
  {
    "objectID": "notebooks/1-tensor-exercise-task.html",
    "href": "notebooks/1-tensor-exercise-task.html",
    "title": "Tensors",
    "section": "",
    "text": "Note\n\n\n\nTo solve these exercises, consulting the torch function reference can be helpful.\n\n\nQuestion 1: Tensor creation and manipulation\nRecreate this torch tensor:\n\n\ntorch_tensor\n 1  2  3\n 4  5  6\n[ CPULongType{2,3} ]\n\n\n\n\nHint\n\nFirst create an R matrix and then convert it using torch_tensor().\n\nNext, create a view of the tensor so it looks like this:\n\n\ntorch_tensor\n 1  2\n 3  4\n 5  6\n[ CPULongType{3,2} ]\n\n\n\n\nHint\n\nUse the $view() method and pass the desired shape as a vector.\n\nHow can you check programmatically that you successfully created a view, and not a copy (i.e. changing one does change the other)?\n\n\nHint\n\nSee what happens when you modify one of the tensors.\n\nQuestion 2: More complex reshaping\nConsider the following tensor:\n\nx &lt;- torch_tensor(1:6)\nx\n\ntorch_tensor\n 1\n 2\n 3\n 4\n 5\n 6\n[ CPULongType{6} ]\n\n\nReshape it so it looks like this.\n\n\ntorch_tensor\n 1  3  5\n 2  4  6\n[ CPULongType{2,3} ]\n\n\n\n\nHint\n\nFirst reshape to (2, 3) and then $permute() the two dimensions.\n\nQuestion 3: Broadcasting\nConsider the following vectors:\n\nx1 &lt;- torch_tensor(c(1, 2))\nx1\n\ntorch_tensor\n 1\n 2\n[ CPUFloatType{2} ]\n\nx2 &lt;- torch_tensor(c(3, 7))\nx2\n\ntorch_tensor\n 3\n 7\n[ CPUFloatType{2} ]\n\n\nPredict the result (shape and values) of the following operation by applying the broadcasting rules.\n\nx1 + x2$reshape(c(2, 1))\n\nQuestion 4: Handling Singleton dimensions\nA common operation in deep learning is to add or get rid of singleton dimensions, i.e., dimensions of size 1. As this is so common, torch offers a $squeeze() and $unsqueeze() method to add and remove singleton dimensions.\nUse these two functions to first remove the second dimension and then add one in the first position.\n\nx &lt;- torch_randn(2, 1)\nx\n\ntorch_tensor\n-0.1115\n 0.1204\n[ CPUFloatType{2,1} ]\n\n\nQuestion 5: Matrix multiplication\nGenerate a random matrix \\(A\\) of shape (10, 5) and a random matrix \\(B\\) of shape (10, 5) by sampling from a standard normal distribution.\n\n\nHint\n\nUse torch_randn(nrow, ncol) to generate random matrices.\n\nCan you multiply these two matrices with each other and if so, in which order? If not, generate two random matrices with compatible shapes and multiply them.\nQuestion 6: Uniform sampling\nGenerate 10 random variables from a uniform distribution (using only torch functions) in the interval \\([10, 20]\\). Use torch_rand() for this (which does not allow for min and max parameters).\n\n\nHint\n\nAdd the lower bound and multiply with the width of the interval.\n\nQuestion 7: Don’t touch this\nConsider the code below:\n\nf &lt;- function(x) {\n  x[1] &lt;- torch_tensor(-99)\n  return(x)\n}\nx &lt;- torch_tensor(1:3)\ny &lt;- f(x)\nx\n\ntorch_tensor\n-99\n  2\n  3\n[ CPULongType{3} ]\n\n\nImplement a new different version of this function that returns the same tensor but does not change the value of the input tensor in-place.\n\n\nHint\n\nThe $clone() method might be helpful."
  },
  {
    "objectID": "notebooks/1-tensor.html",
    "href": "notebooks/1-tensor.html",
    "title": "Torch Tensors",
    "section": "",
    "text": "Torch Website: https://torch.mlverse.org/\nAPI Docs: https://torch.mlverse.org/docs/\nBook by Sigrid Keydana: Deep Learning and Scientific Computing with R torch on which part of this is based.\nSince torch mimics PyTorch and the latter has a larger community, you can often learn from the PyTorch documentation."
  },
  {
    "objectID": "notebooks/1-tensor.html#creating-tensors",
    "href": "notebooks/1-tensor.html#creating-tensors",
    "title": "Torch Tensors",
    "section": "Creating Tensors",
    "text": "Creating Tensors\n\n# From R matrices\nx_matrix &lt;- matrix(1:6, nrow = 2, ncol = 3)\ntensor_x &lt;- torch_tensor(x_matrix)\nprint(tensor_x)\n\ntorch_tensor\n 1  3  5\n 2  4  6\n[ CPULongType{2,3} ]\n\nzeros_tensor &lt;- torch_zeros(2, 3)      # Creates a tensor of zeros\nones_tensor &lt;- torch_ones(2, 3)        # Creates a tensor of ones\nlike_tensor &lt;- torch_zeros_like(ones_tensor)  # Creates a zeros tensor with the same shape as ones_tensor\n\n\nRandom Sampling\nYou can also randomly sample torch tensors:\n\nnormal_tensor &lt;- torch_randn(2, 3)    # Samples from N(0,1)\nuniform_tensor &lt;- torch_rand(2, 3)    # Samples from U(0,1)\n\n\n\n\n\n\n\nRandom Seeds in torch\n\n\n\ntorch maintains its own random number generator, separate from R’s.\nSetting R’s random seed with set.seed() does not affect torch’s random operations. Instead, use torch_manual_seed() to control the reproducibility of torch operations.\n\n\n\n\nMissing Values\n\n\n\n\n\n\nQuiz: NaN vs NA\n\n\n\nQuestion 1: What is the difference between NaN and NA in R?\n\n\nClick for answer\n\nNaN is a floating-point value that represents an undefined or unrepresentable value (such as 0 / 0).\nNA is a missing value indicator used in vectors, matrices, and data frames to represent unknown or missing data.\n\n\n\nTorch tensors do not have a native representation for R’s NA values. When converting R vectors containing NAs to torch tensors, you need to be cautious:\n\nDouble: NA_real_ becomes NaN\n\ntorch_tensor(NA_real_)\n\ntorch_tensor\nnan\n[ CPUFloatType{1} ]\n\n\nInteger: NA_integer_ becomes the smallest negative value:\n\ntorch_tensor(NA_integer_)\n\ntorch_tensor\n-2.1475e+09\n[ CPULongType{1} ]\n\n\nLogical: NA becomes TRUE:\n\ntorch_tensor(NA)\n\ntorch_tensor\n 1\n[ CPUBoolType{1} ]\n\n\n\nYou should handle missing values carefully before converting them to torch tensors."
  },
  {
    "objectID": "notebooks/1-tensor.html#tensor-properties",
    "href": "notebooks/1-tensor.html#tensor-properties",
    "title": "Torch Tensors",
    "section": "Tensor Properties",
    "text": "Tensor Properties\n\nShape\nLike R arrays, each tensor has a shape and a dimension:\n\nprint(tensor_x$shape)\n\n[1] 2 3\n\nprint(tensor_x$dim()) # dim(tensor_x) also works\n\n[1] 2\n\n\n\n\nData Type\nFurthermore, each tensor has a datatype. Unlike base R, where typically there is one integer type (32 bits) and one floating-point type (double, 64 bits), torch differentiates between different precisions:\n\nFloating point:\n\nfloat32_tensor &lt;- torch_ones(2, 3, dtype = torch_float32())  # Default float\nfloat64_tensor &lt;- torch_ones(2, 3, dtype = torch_float64())  # Double precision\nfloat16_tensor &lt;- torch_ones(2, 3, dtype = torch_float16())  # Half precision\n\nUsually, you work with 32-bit floats.\nInteger:\n\nint32_tensor &lt;- torch_ones(2, 3, dtype = torch_int32())\nint64_tensor &lt;- torch_ones(2, 3, dtype = torch_int64())  # Long\nint16_tensor &lt;- torch_ones(2, 3, dtype = torch_int16())  # Short\nint8_tensor  &lt;- torch_ones(2, 3, dtype = torch_int8())    # Byte\nuint8_tensor &lt;- torch_ones(2, 3, dtype = torch_uint8())  # Unsigned byte\n\nBoolean:\n\nbool_tensor &lt;- torch_ones(2, 3, dtype = torch_bool())\n\n\nYou can convert between datatypes using the $to() method:\n\n# Converting between datatypes\nx &lt;- torch_ones(2, 3)  # Default float32\nx_int &lt;- x$to(dtype = torch_int32())\n\nNote that floats are converted to integers by truncating, not by rounding.\n\ntorch_tensor(2.999)$to(dtype = torch_int())\n\ntorch_tensor\n 2\n[ CPUIntType{1} ]\n\ntorch_tensor(-2.999)$to(dtype = torch_int())\n\ntorch_tensor\n-2\n[ CPUIntType{1} ]\n\n\n\n\nDevice\nEach tensor lives on a “device”, where common options are:\n\ncpu for CPU, which is available everywhere\ncuda for NVIDIA GPUs\nmps for Apple Silicon (M1/M2/M3) GPUs on macOS\n\n\n# Create a tensor and move it to CUDA if available\nx &lt;- torch_randn(2, 3)\nif (cuda_is_available()) {\n  x &lt;- x$to(device = torch_device(\"cuda\"))\n  # x &lt;- x$cuda() also works\n} else {\n  print(\"CUDA not available; tensor remains on CPU\")\n}\n\n[1] \"CUDA not available; tensor remains on CPU\"\n\nprint(x$device)\n\ntorch_device(type='cpu') \n\nx &lt;- x$to(device = \"cpu\")\n# x &lt;- x$cpu() also works\nprint(x$device)\n\ntorch_device(type='cpu') \n\n\nGPU acceleration enables massive parallelization of tensor operations, often providing 10-100x speedups compared to CPU processing for large-scale computations.\n\n\n\n\n\n\nDevice Compatibility\n\n\n\nTensors must reside on the same device to perform operations between them."
  },
  {
    "objectID": "notebooks/1-tensor.html#converting-tensors-back-to-r",
    "href": "notebooks/1-tensor.html#converting-tensors-back-to-r",
    "title": "Torch Tensors",
    "section": "Converting Tensors Back to R",
    "text": "Converting Tensors Back to R\nYou can easily convert torch tensors back to R using as_array(), as.matrix(), or $item():\n\n0-dimensional tensors (scalars) are converted to R vectors with length 1:\ntorch_scalar_tensor(1)$item() # as_array() also works\n1-dimensional tensors are converted to R vectors:\nas_array(torch_randn(3))\n\\(&gt;1\\)-dimensional tensors are converted to R arrays:\n\nas_array(torch_randn(2, 2))\n\n           [,1]      [,2]\n[1,] -1.4168206 0.8429176\n[2,] -0.6306752 1.2340047"
  },
  {
    "objectID": "notebooks/1-tensor.html#basic-tensor-operations",
    "href": "notebooks/1-tensor.html#basic-tensor-operations",
    "title": "Torch Tensors",
    "section": "Basic Tensor Operations",
    "text": "Basic Tensor Operations\nTorch provides two main syntaxes for tensor operations: function-style (torch_*()) and method-style (using $).\nHere’s an example with matrix multiplication:\n\n# Create example tensors\na &lt;- torch_tensor(matrix(1:6, nrow=2, ncol=3))\nb &lt;- torch_tensor(matrix(7:12, nrow=3, ncol=2))\n\n# Matrix multiplication - two equivalent ways\nc1 &lt;- torch_matmul(a, b)  # Function style\nc2 &lt;- a$matmul(b)         # Method style\n\ntorch_equal(c1, c2)\n\n[1] TRUE\n\n\nBelow, there is another example using addition:\n\n# Addition - two equivalent ways\nx &lt;- torch_ones(2, 2)\ny &lt;- torch_ones(2, 2)\nz1 &lt;- torch_add(x, y)  # Function style\nz2 &lt;- x$add(y)         # Method style\n\n\n\n\n\n\n\nIn-place Operations\n\n\n\nOperations that modify the tensor directly are marked with an underscore suffix (_). These operations are more memory efficient as they do not allocate a new tensor:\n\nx &lt;- torch_ones(2, 2)\nx$add_(1)  # Adds 1 to all elements in place\n\ntorch_tensor\n 2  2\n 2  2\n[ CPUFloatType{2,2} ]\n\nx\n\ntorch_tensor\n 2  2\n 2  2\n[ CPUFloatType{2,2} ]\n\n\n\n\nYou can also apply common summary functions to torch tensors:\n\nx = torch_randn(1000)\nmean(x)\n\ntorch_tensor\n-0.0239668\n[ CPUFloatType{} ]\n\nmax(x)\n\ntorch_tensor\n3.89537\n[ CPUFloatType{} ]\n\nsd(x)\n\n[1] 1.028835\n\n\nAccessing elements from a tensor is also similar to R arrays and matrices, i.e., it is 1-based.\n\nx &lt;- matrix(1:6, nrow = 3)\nxt &lt;- torch_tensor(x)\nx[1:2, 1]\n\n[1] 1 2\n\nxt[1:2, 1]\n\ntorch_tensor\n 1\n 2\n[ CPULongType{2} ]\n\n\nOne difference between indexing torch vectors and standard R vectors is the behavior regarding negative indices. While R vectors remove the element at the specified index, torch vectors return elements from the beginning.\n\nx[-1, 1]\n\n[1] 2 3\n\nxt[-1, 1]\n\ntorch_tensor\n3\n[ CPULongType{} ]\n\n\n\n\n\n\n\n\nWarning\n\n\n\nWhile (R) torch is 1-based, PyTorch is 0-based. When translating PyTorch code to R, you need to be careful with this difference.\n\n\nAnother convenient feature in torch is the .. syntax for indexing:\n\narr &lt;- array(1:24, dim = c(4, 3, 2))\narr[1:2, , ] # works\n\n, , 1\n\n     [,1] [,2] [,3]\n[1,]    1    5    9\n[2,]    2    6   10\n\n, , 2\n\n     [,1] [,2] [,3]\n[1,]   13   17   21\n[2,]   14   18   22\n\narr[1:2, ]    # does not work\n\nError in arr[1:2, ]: incorrect number of dimensions\n\n\nIn torch, you can achieve the same result as follows:\n\ntensor &lt;- torch_tensor(arr)\ntensor[1:2, ..]\n\ntorch_tensor\n(1,.,.) = \n   1  13\n   5  17\n   9  21\n\n(2,.,.) = \n   2  14\n   6  18\n  10  22\n[ CPULongType{2,3,2} ]\n\n\nYou can also specify indices after the .. operator:\n\ntensor[.., 1]\n\ntorch_tensor\n  1   5   9\n  2   6  10\n  3   7  11\n  4   8  12\n[ CPULongType{4,3} ]\n\n\nNote that when you select a single element from a dimension, the dimension is removed:\n\ndim(tensor[.., 1])\n\n[1] 4 3\n\ndim(tensor[.., 1, drop = FALSE])\n\n[1] 4 3 1\n\n\nTensors also support indexing by boolean masks, which will result in a 1-dimensional tensor:\n\ntensor[tensor &gt; 15]\n\ntorch_tensor\n 17\n 21\n 18\n 22\n 19\n 23\n 16\n 20\n 24\n[ CPULongType{9} ]\n\n\nWe can also extract the first two rows and columns of the tensor from the first index of the third dimension:\n\ntensor[1:2, 1:2, 1]\n\ntorch_tensor\n 1  5\n 2  6\n[ CPULongType{2,2} ]"
  },
  {
    "objectID": "notebooks/1-tensor.html#broadcasting-rules",
    "href": "notebooks/1-tensor.html#broadcasting-rules",
    "title": "Torch Tensors",
    "section": "Broadcasting Rules",
    "text": "Broadcasting Rules\nAnother difference between R arrays and torch tensors is how operations on tensors with different shapes are handled. For example, in R, we cannot add a matrix with shape (1, 2) to a matrix with shape (2, 3):\n\nm1 &lt;- matrix(1:4, nrow = 2)\nm2 &lt;- matrix(1:2, nrow = 2)\nm1 + m2\n\nError in m1 + m2: non-conformable arrays\n\n\nBroadcasting (similar to “recycling” in R) allows torch to perform operations between tensors of different shapes.\n\nt1 &lt;- torch_tensor(m1)\nt2 &lt;- torch_tensor(m2)\nt1 + t2\n\ntorch_tensor\n 2  4\n 4  6\n[ CPULongType{2,2} ]\n\n\nThere are strict rules that define when two shapes are compatible:\n\nIf tensors have a different number of dimensions, prepend 1’s to the shape of the lower-dimensional tensor until they match.\nTwo dimensions are compatible when:\n\nThey are equal, or\nOne of them is 1 (which will be stretched to match the other)\n\nIf any dimension pair is incompatible, broadcasting fails.\n\n\n\n\n\n\n\nQuiz: Broadcasting Rules\n\n\n\nQuestion 1: What would be the resulting shape when broadcasting a tensor of shape (2, 1, 3) with a tensor of shape (4, 3)?\n\n\nClick for answer\n\nThe resulting shape would be (2, 4, 3). Here’s why:\n\nPrepend one to the rank of the second tensor to get (1, 4, 3).\nGoing dimension by dimension:\n\nFirst: 2 vs 1 -&gt; Compatible, expand to 2\nSecond: 1 vs 4 -&gt; Compatible, expand to 4\nThird: 3 vs 3 -&gt; Compatible, remains 3\n\nAll pairs are compatible, so broadcasting succeeds.\n\n\nQuestion 2: Would broadcasting work between tensors of shape (2, 3) and (3, 2)?\n\n\nClick for answer\n\nNo, broadcasting would fail in this case. Here’s why:\n\nBoth tensors have the same rank (2), so no prepending is needed.\nGoing dimension by dimension:\n\nFirst: 2 vs 3 -&gt; Incompatible (neither is 1)\nSecond: 3 vs 2 -&gt; Incompatible (neither is 1)\n\nSince both dimension pairs are incompatible, broadcasting fails."
  },
  {
    "objectID": "notebooks/1-tensor.html#reshaping-tensors",
    "href": "notebooks/1-tensor.html#reshaping-tensors",
    "title": "Torch Tensors",
    "section": "Reshaping Tensors",
    "text": "Reshaping Tensors\nTorch provides several ways to reshape tensors while preserving their data:\n\n# Create a sample tensor\nx &lt;- torch_tensor(0:15)\nprint(x)\n\ntorch_tensor\n  0\n  1\n  2\n  3\n  4\n  5\n  6\n  7\n  8\n  9\n 10\n 11\n 12\n 13\n 14\n 15\n[ CPULongType{16} ]\n\n\nWe can reshape this tensor with shape (16) to a tensor with shape (4, 4).\n\ny &lt;- x$reshape(c(4, 4))\ny\n\ntorch_tensor\n  0   1   2   3\n  4   5   6   7\n  8   9  10  11\n 12  13  14  15\n[ CPULongType{4,4} ]\n\n\nWhen x is reshaped to y, we can imagine it as initializing a new tensor of the desired shape and then filling up the rows and columns of the new tensor by iterating over the rows and columns of the old tensor:\n\ny2 &lt;- torch_zeros(4, 4)\nfor (j in 1:4) { # columns\n  for (i in 1:4) { # rows\n    y2[i, j] &lt;- y[i, j]\n  }\n}\nsum(abs(y - y2))\n\ntorch_tensor\n0\n[ CPUFloatType{} ]\n\n\nInternally, this type of reshaping is (in many cases) implemented by changing the stride of the tensor without altering the underlying data.\n\nx$stride()\n\n[1] 1\n\ny$stride()\n\n[1] 4 1\n\n\nThe value of the stride indicates how many elements to skip to get to the next element along each dimension: If we move from element x[1] (1) to element x[2] (2), we move one index along the columns of y. If we move from x[1] to x[5] (5), i.e., 4 steps, we move one index along the rows of y.\nThis means, for example, that reshaping torch tensors can be considerably more efficient than permuting R arrays, as the latter will always allocate a new, reordered vector, while the former just changes the strides.\nThe functionality of strides is illustrated in the image below.\n Source: How to Represent a Tensor or ndarray\n\n\n\n\n\n\nQuiz: Strides\n\n\n\nQuestion 1: How do you need to change the strides from a matrix with strides (4, 1) to transpose it?\n\n\nClick for answer\n\nThe matrix can be transposed by changing the strides from (4, 1) to (1, 4).\n\ny$t()$stride()\n\n[1] 1 4\n\n\n\n\n\nWhen reshaping tensors, you can also infer a dimension by setting it to -1:\n\nx$reshape(c(-1, 4))$shape\n\n[1] 4 4\n\n\nOf course, not all reshaping operations are valid. The number of elements in the original tensor and the reshaped tensor must be the same:\n\nx$reshape(6)\n\nError in (function (self, shape) : shape '[6]' is invalid for input of size 16"
  },
  {
    "objectID": "notebooks/1-tensor.html#reference-semantics",
    "href": "notebooks/1-tensor.html#reference-semantics",
    "title": "Torch Tensors",
    "section": "Reference Semantics",
    "text": "Reference Semantics\nOne key property of torch tensors is that they have reference semantics. This is different from R, where objects usually have value semantics.\n\nx &lt;- torch_ones(2)\ny &lt;- x\ny[1] &lt;- 5\nx # was modified\n\ntorch_tensor\n 5\n 1\n[ CPUFloatType{2} ]\n\n\nThis differs from R, where objects typically have value semantics:\n\nx &lt;- c(1, 1)\ny &lt;- x\ny[1] &lt;- 5\nx # was not modified\n\n[1] 1 1\n\n\n\n\n\n\n\n\nNote\n\n\n\nAnother notable exception to values semantics are R6 classes, which are used in the mlr3 ecosystem.\n\n\nWhen one tensor (y) shares underlying data with another tensor (x), this is called a view. It is also possible to obtain a view on a subset of a tensor, e.g., via slicing:\n\nx &lt;- torch_arange(1, 10)\ny &lt;- x[1:3]\ny[1] &lt;- 100\nx[1]\n\ntorch_tensor\n100\n[ CPUFloatType{} ]\n\n\nUnfortunately, similar operations might sometimes create a view and sometimes allocate a new tensor. In the example below, we create a subset that is a non-contiguous sequence, and hence a new tensor is allocated:\n\nx &lt;- torch_arange(1, 10)\ny &lt;- x[c(1, 3, 5)]\ny[1] &lt;- 100\nx[1]\n\ntorch_tensor\n1\n[ CPUFloatType{} ]\n\n\nIf it is important to create a copy of a vector, you can call the $clone() method:\n\nx &lt;- torch_arange(1, 3)\ny &lt;- x$clone()\ny[1] &lt;- 10\nx[1] # is still 1\n\ntorch_tensor\n1\n[ CPUFloatType{} ]\n\n\n\n\n\n\n\n\nWarning\n\n\n\nThis is also the case for the $reshape() methods from the last section, which will in some cases create a view and in other cases allocate a new tensor with the desired shape. If you want to ensure that you create a view on a tensor, you can use the $view() method, which will fail if the required view is not possible.\n\n\n\n\n\n\n\n\nQuiz: Contiguous Data\n\n\n\nQuestion 1: Reshaping a 2D Tensor\nConsider the tensor below:\n\nx1 &lt;- torch_tensor(matrix(1:6, nrow = 2, byrow = FALSE))\nx1\n\ntorch_tensor\n 1  3  5\n 2  4  6\n[ CPULongType{2,3} ]\n\n\nWhat is the result of x1$reshape(6), i.e., what are the first, second, …, sixth elements?\n\n\nClick for answer\n\nThis will result in (1, 3, 5, 2, 4, 6) because we (imagine that) first iterate over the rows and then the columns when “creating” the new tensor."
  },
  {
    "objectID": "notebooks/2-autograd-exercise-task.html",
    "href": "notebooks/2-autograd-exercise-task.html",
    "title": "Autograd",
    "section": "",
    "text": "Question 1: Appreciating autograd\nConsider the following function:\n\\[\nf(x) = x^2 + 3x + 2\n\\]\nAs well as the function \\(g(x) = f(f(f(x)))\\)\nCalculate the gradient of both functions at point \\(x = 2\\).\nQuestion 2: Approximating functions with gradients\nThe defining feature of the gradient is that it allows us to approximate the function locally by a linear function.\nI.e., for some value \\(x^*\\), we know for very small \\(\\delta\\), that\n\\[\nf(x^* + \\delta) \\approx f(x^*) + f'(x^*) \\cdot \\delta\n\\]\nPlot the function from earlier as well as the local linear approximation at \\(x = 2\\) using ggplot2.\n\nDefine a function f_approx that takes a value delta and returns the value of the function at x^* + delta using the linear approximation.\nCreate a sequence with 100 equidistant values between -4 to 4 using torch_linspace().\nCreate a data.frame with columns x, y_true, y_approx.\nUse ggplot2 to plot the function and its linear approximation.\n\nQuestion 3: Look ma, I made my own autograd function\nIn this exercise, we will build our own, custom autograd function. While you might rarely need this in practice, it still allows you to get a better understanding of how the autograd system works. There is also a tutorial on this on the torch website.\nTo construct our own autograd function, we need to define:\n\nThe forward pass:\n\nHow to calculate outputs from input\nWhat to save for the backward pass\n\nThe backward pass:\n\nHow to calculate the gradient of the output with respect to the input\n\n\nThe task is to re-create the ReLU activation function, which is a common activation function in neural networks and which is defined as:\n\\[\n\\text{ReLU}(x) = \\max(0, x)\n\\]\nNote that strictly speaking, the ReLU function is not differentiable at \\(x = 0\\) (but a subgradient can be used instead). The derivative/subgradient of the ReLU function is:\n\\[\n\\text{ReLU}'(x) = \\begin{cases}\n1 & \\text{if } x &gt; 0 \\\\\n0 & \\text{if } x \\leq 0 \\\\\n\\end{cases}\n\\]\nIn torch, a custom autograd function can be constructed using autograd_function() function and it accepts arguments forward and backward which are functions that define the forward and backward pass: They both take as first argument a ctx, which is a communication object that is used to save information during the forward pass to be able to compute the gradient in the backward pass. The return value of the backward pass should be a list of gradients with respect to the inputs. To check whether a gradient for an input is needed (has requires_grad = TRUE), you can use ctx$needs_input_grad which is a named list with boolean values for each input.\nThe backward function additionally takes a second argument grad_output, which is the gradient of the output: E.g., if our function is \\(f(x)\\) and we calculate the gradient of \\(g(x) = h(f(x))\\), then grad_output is the derivative of \\(g\\) with respect to its input, evaluated at \\(f(x)\\). This is essentially the chain rule: \\(\\frac{\\partial g}{\\partial x} = \\frac{\\partial g}{\\partial f} \\cdot \\frac{\\partial f}{\\partial x}\\).\nFill out the missing parts (...) in the code below.\n\nrelu &lt;- autograd_function(\n  forward = function(ctx, input) {\n    mask &lt;- ...\n    output &lt;- torch_where(mask, ...)\n    ctx$save_for_backward(mask)\n    output\n  },\n  backward = function(ctx, grad_output) {\n    grads &lt;- list(input = NULL)\n    if (ctx$needs_input_grad$input) {\n      mask &lt;- ctx$saved_variables[[1]]\n      grads$input &lt;- ...\n    }\n    grads\n  }\n)\n\nTo check that it’s working, use the code below (with your relu instead of nnf_relu) and check that the results are the same.\n\nx &lt;- torch_tensor(-1, requires_grad = TRUE)\n(nnf_relu(x)^2)$backward()\nx$grad\n\ntorch_tensor\n 0\n[ CPUFloatType{1} ]\n\nx$grad$zero_()\n\ntorch_tensor\n 0\n[ CPUFloatType{1} ]\n\nx &lt;- torch_tensor(3, requires_grad = TRUE)\n(nnf_relu(x)^2)$backward()\nx$grad\n\ntorch_tensor\n 6\n[ CPUFloatType{1} ]"
  },
  {
    "objectID": "notebooks/2-autograd.html",
    "href": "notebooks/2-autograd.html",
    "title": "Autograd",
    "section": "",
    "text": "Automatic differentiation (autograd) is one of torch’s key features, enabling the automatic computation of gradients for optimization tasks like training neural networks. Unlike numerical differentiation, which approximates gradients using finite differences, autograd computes exact gradients by tracking operations as they are performed and automatically applying the chain rule of calculus. This makes it possible to efficiently compute gradients of complex functions with respect to many parameters—a critical requirement for training modern neural networks.\nAutograd works by building a dynamic computational graph of operations, where each node represents a tensor and each edge represents a mathematical operation.\nWhy do we need automatic differentiation?\nIn deep learning, training a model requires iteratively updating parameters to minimize a loss function, which measures the difference between predictions and actual data. These updates depend on calculating gradients of the loss with respect to model parameters, information used by optimization algorithms like stochastic gradient descent (SGD). Automatic Differentiation eliminates the need to manually derive these gradients, which is error-prone.\n\n\nTo use autograd, tensors must have their requires_grad field set to TRUE. This can either be set during tensor construction or changed afterward using the in-place modifier $requires_grad_(TRUE). In the context of deep learning, we track the gradients of the weights of a neural network. The simplest “neural network” is a linear model with slope \\(a\\) and bias \\(b\\) and a single input \\(x\\).\nThe forward pass is defined as:\n\\[\\hat{y} = a \\times x + b\\]\nWe might be interested in how the prediction \\(\\hat{y}\\) changes for the given \\(x\\) when we change the weight \\(a\\) or the bias \\(b\\). We will later use this to adjust the weights \\(a\\) and \\(b\\) to improve predictions, i.e., to perform gradient-based optimization. To write down the gradients, let \\(u = a \\times x\\) denote the intermediate tensor from the linear predictor.\n\nWeight \\(a\\):\nThis is expressed by the gradient \\(\\frac{\\partial \\hat{y}}{\\partial a}\\). We can compute the derivative using the chain rule as:\n\\[\\frac{\\partial \\hat{y}}{\\partial a} = \\frac{\\partial \\hat{y}}{\\partial u} \\cdot \\frac{\\partial u}{\\partial a} = 1 \\cdot x = x\\]\nBias \\(b\\):\n\\[\\frac{\\partial \\hat{y}}{\\partial b} = 1\\]\n\n\nlibrary(torch)\n\na &lt;- torch_tensor(2, requires_grad = TRUE)\na$requires_grad\n\n[1] TRUE\n\nb &lt;- torch_tensor(1, requires_grad = TRUE)\nx &lt;- torch_tensor(3)\n\nWe can use the weights and input to perform a forward pass:\n\nu &lt;- a * x\ny &lt;- u + b\n\nWhen you perform operations on tensors with gradient tracking, torch builds a computational graph on the fly. In the figure below:\n\nBlue tensors are those for which we want to calculate gradients.\nThe violet node is an intermediate tensor.\nThe yellow boxes are differentiable functions.\nThe green node is the final tensor with respect to which we want to calculate gradients.\n\n\n\n\n\n\ngraph TD\n    a[a] --&gt; mul[Multiply]\n    x[x] --&gt; mul\n    mul --&gt; u[u]\n    u --&gt; add[Add]\n    b[b] --&gt; add\n    add --&gt; y[y]\n\n    %% Gradient flow\n    y_grad[dy/du = 1, dy/db = 1] -.-&gt; y\n    u_grad[du/da = x] -.-&gt; u\n    a_grad[dy/da = x] -.-&gt; a\n    b_grad[dy/db = 1] -.-&gt; b\n\n    %% Styling\n    classDef input fill:#a8d5ff,stroke:#333\n    classDef op fill:#ffe5a8,stroke:#333\n    classDef output fill:#a8ffb6,stroke:#333\n    classDef grad fill:#ffa8a8,stroke:#333,stroke-dasharray:5,5\n    classDef intermediate fill:#d5a8ff,stroke:#333\n    classDef nograd fill:#e8e8e8,stroke:#333\n\n    class a,b input\n    class mul,add op\n    class y output\n    class u intermediate\n    class y_grad,u_grad,a_grad,b_grad grad\n    class x nograd\n\n\n\n\n\n\nEach intermediate tensor knows how to calculate gradients with respect to its inputs.\n\ny$grad_fn\n\nAddBackward0\n\nu$grad_fn\n\nMulBackward0\n\n\nTo calculate the gradients \\(\\frac{\\partial y}{\\partial a}\\) and \\(\\frac{\\partial y}{\\partial b}\\), we can traverse this computational graph backward, invoke the differentiation functions, and multiply the individual derivatives according to the chain rule. In torch, this is done by calling $backward() on y. Note that $backward() can only be called on scalar tensors. Afterwards, the gradients are accessible in the $grad field of the tensors a and b:\n\n# Compute gradients\ny$backward()\n\n# Access gradients\nprint(a$grad)  # dy/da = x = 3\n\ntorch_tensor\n 3\n[ CPUFloatType{1} ]\n\nprint(b$grad)  # dy/db = 1\n\ntorch_tensor\n 1\n[ CPUFloatType{1} ]\n\n\nNote that only tensors with $requires_grad set to TRUE store their gradients. For the intermediate value u, no gradient is stored.\n\n\n\n\n\n\nTip\n\n\n\nWhen you want to perform an operation on tensors that require gradients without tracking this specific operation, you can use with_no_grad(...).\n\n\nIn the next section, we will show how we can use gradients to train a simple linear model.\n\n\n\nWe can use autograd to fit a simple linear regression model. Let’s first generate some synthetic data:\n\nlibrary(ggplot2)\n\n# Set random seed for reproducibility\ntorch_manual_seed(42)\n\n# Generate synthetic data\nn &lt;- 100\na_true &lt;- 2.5\nb_true &lt;- 1.0\n\n# Create input X and add noise to output Y\nX &lt;- torch_randn(n)\nnoise &lt;- torch_randn(n) * 0.5\nY &lt;- X * a_true + b_true + noise\n\n\n\n\n\n\n\n\n\n\nFirst, we randomly initialize our parameters a and b.\n\n# Initialize parameters with random values\na &lt;- torch_randn(1, requires_grad = TRUE)\nb &lt;- torch_randn(1, requires_grad = TRUE)\n\nTo optimize the parameters \\(a\\) and \\(b\\), we need to define the Loss Function that quantifies the discrepancy between our predictions \\(\\hat{y}\\) and the observed values \\(Y\\). The standard loss for linear regression is the L2 loss:\n\\[ L(y, \\hat{y}) = (y - \\hat{y})^2\\]\nThe graphic below visualizes the relationship between the parameters \\(a\\) and \\(b\\) with the average L2 loss over all datapoints, i.e., the Mean Squared Error (MSE). For parameters \\(a\\) and \\(b\\) that are on the same contour line, the same loss is observed. The color gradient indicates the magnitude of the MSE. In this case, lighter values mark areas with higher loss, and darker values mark areas with lower loss. The red point marks the minimum loss, while the blue point shows the starting values of the parameters.\n\n\n\n\n\n\n\n\n\nWe can optimize the parameters \\(a\\) and \\(b\\) to converge to the minimum by using gradient descent. Gradient descent is a fundamental optimization algorithm that helps us find the minimum of a function by iteratively moving in the direction of steepest descent.\n\n\n\nThe gradient of a function points in the direction of the steepest increase—like pointing uphill on mountainous terrain. Therefore, the negative gradient points in the direction of the steepest decrease—like pointing downhill.\nGradient descent uses this property to iteratively:\n\nCalculate the gradient at the current position.\nTake a small step in the opposite direction of the gradient.\nRepeat until we reach a minimum.\n\nNote that the gradient only tells us in which direction we have to go, not how far. The length of the step should not be:\n\nToo large because the gradient approximation only holds in a small neighborhood.\nToo small as otherwise the convergence will be slow.\n\nThe general update formula for the weights \\(a\\) and \\(b\\) is:\n\\[a_{t+1} = a_t - \\eta \\frac{\\partial L}{\\partial a_t}\\] \\[b_{t+1} = b_t - \\eta \\frac{\\partial L}{\\partial b_t}\\]\nwhere \\(\\eta\\) is the learning rate, and \\(L\\) is the loss function.\nIn practice, when dealing with large datasets, computing the gradient over the entire dataset can be computationally expensive. Instead, we often use Stochastic Gradient Descent (SGD), where the gradient is estimated using only a few observations (a so called ‘batch’), but more on that later.\nWe start by implementing a single gradient step. Note that if we repeatedly call loss$backward(), the gradients in a and b would accumulate, so we set them to 0 before performing the update. The return value of the update will be the parameter values and the loss so we can plot them later. Also, note that we mutate the parameters a and b in-place (suffix _).\n\nupdate_params &lt;- function(X_batch, Y_batch, lr, a, b) {\n  # Perform forward pass, calculate loss\n  Y_hat &lt;- X_batch * a + b\n  loss &lt;- mean((Y_hat - Y_batch)^2)\n\n  # Calculate gradients\n  loss$backward()\n\n  # We don't want to track gradients when we update the parameters.\n  with_no_grad({\n    a$sub_(lr * a$grad)\n    b$sub_(lr * b$grad)\n  })\n\n  # Ensure gradients are zero\n  a$grad$zero_()\n  b$grad$zero_()\n\n  list(\n    a = a$item(),\n    b = b$item(),\n    loss = loss$item()\n  )\n}\n\n\nlibrary(data.table)\n\n# Hyperparameters\nlr &lt;- 0.02\nepochs &lt;- 10\nbatch_size &lt;- 10\n\n# Split data into 10 batches of size 10\nbatches &lt;- split(sample(1:100), rep(seq_len(batch_size), length.out = 100))\nhistory &lt;- list()\nfor (epoch in seq_len(epochs)) {\n  for (step in 1:10) {\n    result &lt;- update_params(X[batches[[step]]], Y[batches[[step]]], lr, a, b)\n    history &lt;- append(history, list(as.data.table(result)))\n  }\n}\n\nhistory = rbindlist(history)\n\nThis example demonstrates how we can use torch’s autograd to implement gradient descent for fitting a simple linear regression model. The dashed red lines show the progression of the model during training, with increasing opacity for later steps. The blue line represents the true relationship.\n\n\n\n\n\n\n\n\n\nWe can also visualize the parameter updates over time:\n\n\n\n\n\n\n\n\n\nOf course, better solutions exist for estimating a simple linear model, but this example demonstrates how we can utilize an autograd system to estimate the parameters of a model."
  },
  {
    "objectID": "notebooks/2-autograd.html#enabling-gradient-tracking",
    "href": "notebooks/2-autograd.html#enabling-gradient-tracking",
    "title": "Autograd",
    "section": "",
    "text": "To use autograd, tensors must have their requires_grad field set to TRUE. This can either be set during tensor construction or changed afterward using the in-place modifier $requires_grad_(TRUE). In the context of deep learning, we track the gradients of the weights of a neural network. The simplest “neural network” is a linear model with slope \\(a\\) and bias \\(b\\) and a single input \\(x\\).\nThe forward pass is defined as:\n\\[\\hat{y} = a \\times x + b\\]\nWe might be interested in how the prediction \\(\\hat{y}\\) changes for the given \\(x\\) when we change the weight \\(a\\) or the bias \\(b\\). We will later use this to adjust the weights \\(a\\) and \\(b\\) to improve predictions, i.e., to perform gradient-based optimization. To write down the gradients, let \\(u = a \\times x\\) denote the intermediate tensor from the linear predictor.\n\nWeight \\(a\\):\nThis is expressed by the gradient \\(\\frac{\\partial \\hat{y}}{\\partial a}\\). We can compute the derivative using the chain rule as:\n\\[\\frac{\\partial \\hat{y}}{\\partial a} = \\frac{\\partial \\hat{y}}{\\partial u} \\cdot \\frac{\\partial u}{\\partial a} = 1 \\cdot x = x\\]\nBias \\(b\\):\n\\[\\frac{\\partial \\hat{y}}{\\partial b} = 1\\]\n\n\nlibrary(torch)\n\na &lt;- torch_tensor(2, requires_grad = TRUE)\na$requires_grad\n\n[1] TRUE\n\nb &lt;- torch_tensor(1, requires_grad = TRUE)\nx &lt;- torch_tensor(3)\n\nWe can use the weights and input to perform a forward pass:\n\nu &lt;- a * x\ny &lt;- u + b\n\nWhen you perform operations on tensors with gradient tracking, torch builds a computational graph on the fly. In the figure below:\n\nBlue tensors are those for which we want to calculate gradients.\nThe violet node is an intermediate tensor.\nThe yellow boxes are differentiable functions.\nThe green node is the final tensor with respect to which we want to calculate gradients.\n\n\n\n\n\n\ngraph TD\n    a[a] --&gt; mul[Multiply]\n    x[x] --&gt; mul\n    mul --&gt; u[u]\n    u --&gt; add[Add]\n    b[b] --&gt; add\n    add --&gt; y[y]\n\n    %% Gradient flow\n    y_grad[dy/du = 1, dy/db = 1] -.-&gt; y\n    u_grad[du/da = x] -.-&gt; u\n    a_grad[dy/da = x] -.-&gt; a\n    b_grad[dy/db = 1] -.-&gt; b\n\n    %% Styling\n    classDef input fill:#a8d5ff,stroke:#333\n    classDef op fill:#ffe5a8,stroke:#333\n    classDef output fill:#a8ffb6,stroke:#333\n    classDef grad fill:#ffa8a8,stroke:#333,stroke-dasharray:5,5\n    classDef intermediate fill:#d5a8ff,stroke:#333\n    classDef nograd fill:#e8e8e8,stroke:#333\n\n    class a,b input\n    class mul,add op\n    class y output\n    class u intermediate\n    class y_grad,u_grad,a_grad,b_grad grad\n    class x nograd\n\n\n\n\n\n\nEach intermediate tensor knows how to calculate gradients with respect to its inputs.\n\ny$grad_fn\n\nAddBackward0\n\nu$grad_fn\n\nMulBackward0\n\n\nTo calculate the gradients \\(\\frac{\\partial y}{\\partial a}\\) and \\(\\frac{\\partial y}{\\partial b}\\), we can traverse this computational graph backward, invoke the differentiation functions, and multiply the individual derivatives according to the chain rule. In torch, this is done by calling $backward() on y. Note that $backward() can only be called on scalar tensors. Afterwards, the gradients are accessible in the $grad field of the tensors a and b:\n\n# Compute gradients\ny$backward()\n\n# Access gradients\nprint(a$grad)  # dy/da = x = 3\n\ntorch_tensor\n 3\n[ CPUFloatType{1} ]\n\nprint(b$grad)  # dy/db = 1\n\ntorch_tensor\n 1\n[ CPUFloatType{1} ]\n\n\nNote that only tensors with $requires_grad set to TRUE store their gradients. For the intermediate value u, no gradient is stored.\n\n\n\n\n\n\nTip\n\n\n\nWhen you want to perform an operation on tensors that require gradients without tracking this specific operation, you can use with_no_grad(...).\n\n\nIn the next section, we will show how we can use gradients to train a simple linear model."
  },
  {
    "objectID": "notebooks/2-autograd.html#a-simple-linear-model",
    "href": "notebooks/2-autograd.html#a-simple-linear-model",
    "title": "Autograd",
    "section": "",
    "text": "We can use autograd to fit a simple linear regression model. Let’s first generate some synthetic data:\n\nlibrary(ggplot2)\n\n# Set random seed for reproducibility\ntorch_manual_seed(42)\n\n# Generate synthetic data\nn &lt;- 100\na_true &lt;- 2.5\nb_true &lt;- 1.0\n\n# Create input X and add noise to output Y\nX &lt;- torch_randn(n)\nnoise &lt;- torch_randn(n) * 0.5\nY &lt;- X * a_true + b_true + noise\n\n\n\n\n\n\n\n\n\n\nFirst, we randomly initialize our parameters a and b.\n\n# Initialize parameters with random values\na &lt;- torch_randn(1, requires_grad = TRUE)\nb &lt;- torch_randn(1, requires_grad = TRUE)\n\nTo optimize the parameters \\(a\\) and \\(b\\), we need to define the Loss Function that quantifies the discrepancy between our predictions \\(\\hat{y}\\) and the observed values \\(Y\\). The standard loss for linear regression is the L2 loss:\n\\[ L(y, \\hat{y}) = (y - \\hat{y})^2\\]\nThe graphic below visualizes the relationship between the parameters \\(a\\) and \\(b\\) with the average L2 loss over all datapoints, i.e., the Mean Squared Error (MSE). For parameters \\(a\\) and \\(b\\) that are on the same contour line, the same loss is observed. The color gradient indicates the magnitude of the MSE. In this case, lighter values mark areas with higher loss, and darker values mark areas with lower loss. The red point marks the minimum loss, while the blue point shows the starting values of the parameters.\n\n\n\n\n\n\n\n\n\nWe can optimize the parameters \\(a\\) and \\(b\\) to converge to the minimum by using gradient descent. Gradient descent is a fundamental optimization algorithm that helps us find the minimum of a function by iteratively moving in the direction of steepest descent."
  },
  {
    "objectID": "notebooks/2-autograd.html#understanding-gradient-descent",
    "href": "notebooks/2-autograd.html#understanding-gradient-descent",
    "title": "Autograd",
    "section": "",
    "text": "The gradient of a function points in the direction of the steepest increase—like pointing uphill on mountainous terrain. Therefore, the negative gradient points in the direction of the steepest decrease—like pointing downhill.\nGradient descent uses this property to iteratively:\n\nCalculate the gradient at the current position.\nTake a small step in the opposite direction of the gradient.\nRepeat until we reach a minimum.\n\nNote that the gradient only tells us in which direction we have to go, not how far. The length of the step should not be:\n\nToo large because the gradient approximation only holds in a small neighborhood.\nToo small as otherwise the convergence will be slow.\n\nThe general update formula for the weights \\(a\\) and \\(b\\) is:\n\\[a_{t+1} = a_t - \\eta \\frac{\\partial L}{\\partial a_t}\\] \\[b_{t+1} = b_t - \\eta \\frac{\\partial L}{\\partial b_t}\\]\nwhere \\(\\eta\\) is the learning rate, and \\(L\\) is the loss function.\nIn practice, when dealing with large datasets, computing the gradient over the entire dataset can be computationally expensive. Instead, we often use Stochastic Gradient Descent (SGD), where the gradient is estimated using only a few observations (a so called ‘batch’), but more on that later.\nWe start by implementing a single gradient step. Note that if we repeatedly call loss$backward(), the gradients in a and b would accumulate, so we set them to 0 before performing the update. The return value of the update will be the parameter values and the loss so we can plot them later. Also, note that we mutate the parameters a and b in-place (suffix _).\n\nupdate_params &lt;- function(X_batch, Y_batch, lr, a, b) {\n  # Perform forward pass, calculate loss\n  Y_hat &lt;- X_batch * a + b\n  loss &lt;- mean((Y_hat - Y_batch)^2)\n\n  # Calculate gradients\n  loss$backward()\n\n  # We don't want to track gradients when we update the parameters.\n  with_no_grad({\n    a$sub_(lr * a$grad)\n    b$sub_(lr * b$grad)\n  })\n\n  # Ensure gradients are zero\n  a$grad$zero_()\n  b$grad$zero_()\n\n  list(\n    a = a$item(),\n    b = b$item(),\n    loss = loss$item()\n  )\n}\n\n\nlibrary(data.table)\n\n# Hyperparameters\nlr &lt;- 0.02\nepochs &lt;- 10\nbatch_size &lt;- 10\n\n# Split data into 10 batches of size 10\nbatches &lt;- split(sample(1:100), rep(seq_len(batch_size), length.out = 100))\nhistory &lt;- list()\nfor (epoch in seq_len(epochs)) {\n  for (step in 1:10) {\n    result &lt;- update_params(X[batches[[step]]], Y[batches[[step]]], lr, a, b)\n    history &lt;- append(history, list(as.data.table(result)))\n  }\n}\n\nhistory = rbindlist(history)\n\nThis example demonstrates how we can use torch’s autograd to implement gradient descent for fitting a simple linear regression model. The dashed red lines show the progression of the model during training, with increasing opacity for later steps. The blue line represents the true relationship.\n\n\n\n\n\n\n\n\n\nWe can also visualize the parameter updates over time:\n\n\n\n\n\n\n\n\n\nOf course, better solutions exist for estimating a simple linear model, but this example demonstrates how we can utilize an autograd system to estimate the parameters of a model."
  },
  {
    "objectID": "notebooks/3-modules-data-exercise-task.html",
    "href": "notebooks/3-modules-data-exercise-task.html",
    "title": "It’s a Sin(us)",
    "section": "",
    "text": "Question 1: Create a torch::dataset that takes in arguments n, min, and max where:\n\nn is the total number of samples\nmin is the lower bound of the data\nmax is the upper bound of the data\n\nIn the initialize method, generate and store:\n\na 2D tensor x of n values drawn from a uniform distribution between min and max\na 2D tensor y that is defined as \\(sin(x) + \\epsilon\\) where \\(\\epsilon\\) is drawn from a normal distribution with mean 0 and standard deviation 0.1\n\nThe dataset should return a named list with values x and y.\nThen, create an instance of the dataset with n = 1000, min = 0, and max = 10.\nMake sure that the dataset is working by either calling its $.getitem() or $.getbatch() method depending on what you implemented. Also, check that the shapes of both tensors returned by the dataset are (n_batch, 1).\nQuestion 2: Create a torch::dataloader that takes in the dataset and returns batches of size 10. Create one tensor X and one tensor Y that contains the concatenated batches of x and y.\n\n\nHint\n\nThe functions coro::loop() and torch_cat() might be helpful.\n\nQuestion 3: Create a custom torch module that allows modeling the sinus data we have created. To test it, apply it to the tensor X we have created above and calculate its mean squared error with the tensor Y.\n\n\nHint\n\nYou can either use nn_module to create a custom module generically, or you can use nn_sequential() to create a custom module that is a sequence of layers.\n\nQuestion 4: Train the model on the task for different hyperparameters (lr or epochs) and visualize the results. Play around with the hyperparameters until you get a good fit. You can use the following code for that:\n\nlibrary(ggplot2)\npredict_network &lt;- function(net, dataloader) {\n  local_no_grad()\n  xs &lt;- list(x = numeric(), y = numeric(), pred = numeric())\n  i &lt;- 1\n  net$eval()\n  coro::loop(for (batch in dataloader) {\n    xs$x &lt;- c(xs$x, as.numeric(batch$x))\n    xs$y &lt;- c(xs$y, as.numeric(batch$y))\n    xs$pred &lt;- c(xs$pred, as.numeric(net(batch$x)))\n  })\n  as.data.frame(xs)\n}\ntrain_network &lt;- function(net, dataloader, epochs, lr) {\n  optimizer &lt;- optim_ignite_adamw(net$parameters, lr = lr)\n  net$train()\n  for (i in seq_len(epochs)) {\n    coro::loop(for (batch in dataloader) {\n      optimizer$zero_grad()\n      Y_pred &lt;- net(batch$x)\n      loss &lt;- nnf_mse_loss(Y_pred, batch$y)\n      loss$backward()\n      optimizer$step()\n    })\n  }\n  predict_network(net, dataloader)\n}\nplot_results &lt;- function(df) {\n  ggplot(data = df, aes(x = x)) +\n    geom_point(aes(y = y, color = \"true\")) +\n    geom_point(aes(y = pred, color = \"pred\")) +\n    theme_minimal()\n}\ntrain_and_plot &lt;- function(net, dataloader, epochs = 10, lr = 0.01) {\n  result &lt;- train_network(net, dataloader, epochs = epochs, lr = lr)\n  plot_results(result)\n}\n\n\n\n\n\n\n\nTip\n\n\n\nBeware of the reference semantics and make sure that you create a new instance of the network for each run.\n\n\nQuestion 5: Create a new instance from the sinus dataset class created earlier. Now, set the min and max values to 10 and 20 respectively and visualize the results. What do you observe? Can you explain why this is happening and can you fix the network architecture to make it work?\n\n\nHint\n\nThe sinus function has a phase of \\(2 \\pi\\)."
  },
  {
    "objectID": "notebooks/3-modules-data.html",
    "href": "notebooks/3-modules-data.html",
    "title": "Modules and Data",
    "section": "",
    "text": "In the previous notebook, we explored how to use torch’s autograd system to fit simple linear models. We manually:\n\nManaged the weights.\nDefined the forward path for the model.\nComputed gradients and updated parameters using a simple update rule: a$sub_(lr * a$grad)\n\nFor more complex models, this approach becomes cumbersome. torch offers several high-level abstractions that simplify building and training neural networks:\n\nnn_module: A class to organize model parameters and define the forward pass.\ndataset and dataloader: Classes to handle data loading and batching, replacing our manual data handling.\noptim: Classes that implement various optimization algorithms, replacing our manual gradient updates.\n\nLet’s explore how these components work together by building a neural network to classify spiral data. Note that we only briefly touch on optimizers here and dedicate an additional notebook to them."
  },
  {
    "objectID": "notebooks/3-modules-data.html#the-world-is-not-linear",
    "href": "notebooks/3-modules-data.html#the-world-is-not-linear",
    "title": "Modules and Data",
    "section": "The World is Not Linear",
    "text": "The World is Not Linear\nWhile we have so far explained much of torch’s functionality using simple linear networks, the main idea of deep learning is to model complex, non-linear relationships. Below, we generate some non-linear synthetic spiral data for binary classification:\n\nlibrary(torch)\nlibrary(ggplot2)\nlibrary(mlbench)\n\n# Generate spiral data\nset.seed(123)\nn &lt;- 500\nspiral &lt;- mlbench.spirals(n, sd = 0.1)\n\n# Convert to data frame\nspiral_data &lt;- data.frame(\n  x1 = spiral$x[,1],\n  x2 = spiral$x[,2],\n  label = as.factor(spiral$classes)\n)\n\nThe data looks like this:\n\n\n\n\n\n\n\n\n\nWhile linear models are often useful and have helped us explain the torch API, they are limited in capturing the complex, non-linear patterns commonly present in real-world data, especially unstructured types like images, text, audio, and video. Deep neural networks typically consist of many different layers (hence the name “deep”) and combine linear and non-linear layers with various other components, allowing them to represent highly complex functions. Traditional machine learning and statistics rely on manual feature engineering to transform raw inputs, whereas deep neural networks have revolutionized this process by automatically learning hierarchical features directly from the data.\nOne challenging problem is defining a neural network architecture for a given task. While neural networks with a single hidden layer can theoretically approximate any continuous function, the practical challenge lies in finding these solutions efficiently. This is where architectural choices and their associated inductive biases become crucial.\nAn inductive bias represents the set of assumptions that a learning algorithm uses to predict outputs for inputs it hasn’t encountered during training. These biases help the model generalize beyond its training data by favoring certain solutions over others.\nSome examples of inductive biases in different neural network architectures:\n\nConvolutional Neural Networks (CNNs)\nThe central component of a CNN is the convolutional layer:\n\n\n\nConvolutional Layer\n\n\nCNNs encode several strong inductive biases about visual data:\n\nLocality: Nearby pixels are more likely to be related than distant ones.\nTranslation Invariance: Features should be detected regardless of their position.\nHierarchical Composition: Complex patterns are built from simpler ones.\n\nThese biases make CNNs particularly effective for image-related tasks because they align with our understanding of how visual information is structured.\nTo create a convolutional layer for a 2D image, we can use the nn_conv2d function.\n\nstr(image)\n\nFloat [1:1, 1:28, 1:28]\n\nconv_layer &lt;- nn_conv2d(in_channels = 1, out_channels = 1, kernel_size = 3, padding = 1)\nstr(conv_layer(image))\n\nFloat [1:1, 1:28, 1:28]\n\n\nBecause we have encoded more information about the structural relationship between the input tensor and the output tensor (the same filter is applied to the entire image), the convolutional layer has far fewer parameters than a fully connected layer.\n\nconv_layer$parameters\n\n$weight\ntorch_tensor\n(1,1,.,.) = \n -0.1232  0.1247 -0.2829\n -0.2022 -0.1224 -0.0655\n -0.2543  0.2183 -0.0786\n[ CPUFloatType{1,1,3,3} ][ requires_grad = TRUE ]\n\n$bias\ntorch_tensor\n 0.1070\n[ CPUFloatType{1} ][ requires_grad = TRUE ]\n\n\n\n\n\n\n\n\nWeights of a Fully Connected Layer\n\n\n\nQuestion 1: How many parameters does a fully connected layer with the same number of inputs and outputs have?\n\n\nAnswer\n\nThe input has \\(28 \\times 28 = 784\\) pixels and the output as well. The weights of the fully connected layer are a \\(784 \\times 784\\) matrix and the bias also has 784 elements, so the number of parameters is \\(784 \\times 784 + 784 = 615440\\), much more than our simple convolutional kernel.\n\n\n\nBelow, we show the output of the first convolutional layer from a (trained) ResNet18 model.\n\n\n\n\n\n\n\n\n\n\n\nTransformers\nWhile there are many variations of transformer architectures, the main idea is the (self-)attention mechanism:\n Source: https://medium.com/@ramendrakumar/self-attention-d8196b9e9143\nTransformer architectures, which power language models like GPT-4 and are commonly used in natural language processing, have different inductive biases:\n\nNon-locality: Any token can directly interact with any other token (this is why training transformers is so expensive).\nPosition Awareness: Sequential order matters but is explicitly encoded.\nAttention-based Relationships: Important connections between elements are learned dynamically.\n\nThese biases make Transformers well-suited for tasks where long-range dependencies are important, such as understanding language or analyzing sequences.\nIn torch, the nn_multihead_attention module implements the attention mechanism. We demonstrate how to use it with random data, a single output head, and self-attention for simplicity.\n\nlibrary(torch)\nlibrary(ggplot2)\nlibrary(tidyr)\nlibrary(zeallot)\n\nembed_dim &lt;- 16\nseq_length &lt;- 10\nbatch_size &lt;- 1\n\n# Initialize multihead attention module\nattention &lt;- nn_multihead_attention(\n  embed_dim = embed_dim,\n  num_heads = 1\n)\n\n# Create random input embedding\ninput_embedding &lt;- torch_randn(seq_length, batch_size, embed_dim)\n\n# For self-attention, the query, key, and value are the same\nquery &lt;- key &lt;- value &lt;- input_embedding\n\n# Forward pass, keep the attention weights, not only new embeddings\noutput &lt;- attention(query, key, value, need_weights = TRUE)\nattn_output &lt;- output[[1L]]\nattn_weights &lt;- output[[2L]]\n\nBelow, we print the attention weights between the random embeddings and weights.\n\n\n\n\n\n\n\n\n\nBelow, we show a visualization of the mechanism:\n\nSource: https://data-science-blog.com/blog/2021/04/07/multi-head-attention-mechanism/\n\n\nMLPs (like our Spiral Network)\nThe different layers in a Multi-Layer Perceptron (MLP) consist mainly of an affine-linear transformation followed by a non-linear function, such as a ReLU activation function:\n\nSource: https://scikit-learn.org/1.5/modules/neural_networks_supervised.html\nOur simple multi-layer perceptron has minimal inductive biases:\n\nContinuity: Similar inputs should produce similar outputs.\nHierarchical Feature Learning: Each layer builds increasingly abstract representations.\n\nThis flexibility makes MLPs general-purpose learners, but they may require more data or parameters to learn patterns that specialized architectures can discover more efficiently.\nFor our problem, we will use a simple MLP with three hidden layers:\n\nnn_spiral_net &lt;- nn_module(\"nn_spiral_net\",\n  initialize = function(input_size, hidden_size, output_size) {\n    self$fc1 &lt;- nn_linear(input_size, hidden_size)\n    self$fc2 &lt;- nn_linear(hidden_size, hidden_size)\n    self$fc3 &lt;- nn_linear(hidden_size, hidden_size)\n    self$fc4 &lt;- nn_linear(hidden_size, output_size)\n    self$relu = nn_relu()\n  },\n\n  forward = function(x) {\n    x |&gt;\n      self$fc1() |&gt;\n      self$relu() |&gt;\n      self$fc2() |&gt;\n      self$relu() |&gt;\n      self$fc3() |&gt;\n      self$relu() |&gt;\n      self$fc4()\n  }\n)\n\n\n\n\n\n\n\nTip\n\n\n\nInstead of creating an nn_relu() during network initialization, we could have used the nnf_relu function directly in the forward pass. This is possible for activation functions as they have no trainable weights.\nIn general, nn_ functions create module instances that can maintain state (like trainable weights or running statistics), while nnf_ functions provide the same operations as pure functions without any state.\nFurthermore, for simple sequential networks, we could have used nn_sequential to define the network instead of nn_module. This allows you to chain layers together in a linear fashion without explicitly defining the forward pass.\n\n\nThe image below visualizes the general structure of this network:\n\nSource: https://tikz.net/neural_networks/\nWe can create a concrete network by calling the resulting nn_module_generator and specifying the required parameters.\n\n# Create model instance\nmodel &lt;- nn_spiral_net(\n  input_size = 2,\n  hidden_size = 64,\n  output_size = 2\n)\n\nprint(model)\n\nAn `nn_module` containing 8,642 parameters.\n\n── Modules ─────────────────────────────────────────────────────────────────────────────────────────────────────────────\n• fc1: &lt;nn_linear&gt; #192 parameters\n• fc2: &lt;nn_linear&gt; #4,160 parameters\n• fc3: &lt;nn_linear&gt; #4,160 parameters\n• fc4: &lt;nn_linear&gt; #130 parameters\n• relu: &lt;nn_relu&gt; #0 parameters\n\n\nAt this point, let’s briefly discuss the output—the ‘head’ of the network, as well as loss functions.\nClassification\nThe output dimension of a classification network is usually the number of classes, which is 2 in our case. However, the output is not probabilities but logit scores. To convert a vector of scores to probabilities, we apply the softmax function:\n\\[ \\text{softmax}(x) = \\frac{\\exp(x)}{\\sum_i \\exp(x_i)} \\]\nIn torch, we can apply the softmax function using nnf_softmax(), specifying the dimension along which to apply softmax.\n\nlogits &lt;- model(torch_randn(2, 2))\nprint(logits)\n\ntorch_tensor\n0.01 *\n 9.6994  3.6723\n  9.8986  2.9452\n[ CPUFloatType{2,2} ][ grad_fn = &lt;AddmmBackward0&gt; ]\n\n# dim = 2 applies softmax along the class dimension (columns)\nnnf_softmax(logits, dim = 2)\n\ntorch_tensor\n 0.5151  0.4849\n 0.5174  0.4826\n[ CPUFloatType{2,2} ][ grad_fn = &lt;SoftmaxBackward0&gt; ]\n\n\nThe most commonly used loss function is cross-entropy. For a true probability vector \\(p\\) and a predicted probability vector \\(q\\), the cross-entropy is defined as:\n\\[ \\text{CE}(p, q) = - \\sum_i p_i \\log(q_i) \\]\nNote that when the true probability \\(p\\) is 1 for the true class and 0 for all other classes, the cross-entropy simplifies to:\n\\[ \\text{CE}(p, q) = - \\log(q_{y}) \\]\nwhere \\(y\\) is the true class and \\(q_y\\) is its predicted probability.\nTo calculate the cross-entropy loss, we need to pass the predicted scores and the true class indices to the loss function. The classes should be labeled from 1 to C for a total of C classes.\n\ny_true &lt;- torch_tensor(c(1, 2), dtype = torch_long())\ndim(logits)\n\n[1] 2 2\n\ndim(y_true)\n\n[1] 2\n\nnnf_cross_entropy(input = logits, target = y_true)\n\ntorch_tensor\n0.695992\n[ CPUFloatType{} ][ grad_fn = &lt;NllLossBackward0&gt; ]\n\n\nRegression\nFor regression tasks, the final layer is almost always a simple linear layer with a single output. We can construct a version of the spiral network for regression by changing the final layer to a linear layer with a single output:\n\nmodel_regr &lt;- nn_spiral_net(input_size = 2, hidden_size = 64, output_size = 1)\nx &lt;- torch_randn(1, 2)\ny_hat &lt;- model_regr(x)\ny &lt;- torch_randn(1)\n\nThe loss function typically used is the mean squared error, defined as:\n\\[ \\text{MSE}(y, \\hat{y}) = \\frac{1}{n} \\sum_{i=1}^n (y_i - \\hat{y}_i)^2 \\]\nIn torch, we can apply the mean squared error loss using nnf_mse_loss(), or construct an MSE module:\n\nmse &lt;- nn_mse_loss()\nmse(y_hat, y)\n\nWarning: Using a target size (1) that is different to the input size (1,1).\n\n\ntorch_tensor\n0.559677\n[ CPUFloatType{} ][ grad_fn = &lt;MseLossBackward0&gt; ]\n\nnnf_mse_loss(y_hat, y)\n\nWarning: Using a target size (1) that is different to the input size (1,1).\n\n\ntorch_tensor\n0.559677\n[ CPUFloatType{} ][ grad_fn = &lt;MseLossBackward0&gt; ]\n\n\n\n\n\n\n\n\nNote\n\n\n\nFinally, it’s important to note that there is nothing inherently ‘magical’ about nn_modules. We could have equally implemented the same network manually ourselves:\n\nmake_weights &lt;- function(input_size, hidden_size, output_size) {\n  list(\n    list(\n      w = torch_randn(input_size, hidden_size),\n      b = torch_randn(hidden_size)\n    ),\n    list(\n      w = torch_randn(hidden_size, hidden_size),\n      b = torch_randn(hidden_size)\n    ),\n    list(\n      w = torch_randn(hidden_size, hidden_size),\n      b = torch_randn(hidden_size)\n    ),\n    list(\n      w = torch_randn(hidden_size, output_size),\n      b = torch_randn(output_size)\n    )\n  )\n}\n\nforward &lt;- function(input, weights) {\n  for (layer in weights) {\n    input &lt;- nnf_relu(input$matmul(layer$w) + layer$b)\n  }\n  input\n}\nparams &lt;- make_weights(2, 64, 2)\nforward(x, params)\n\ntorch_tensor\n 0  0\n[ CPUFloatType{1,2} ]"
  },
  {
    "objectID": "notebooks/4-optimizer-exercise-task.html",
    "href": "notebooks/4-optimizer-exercise-task.html",
    "title": "Optimizer Exercises",
    "section": "",
    "text": "Question 1\nIn this exercise, the task is to play around with the settings for the optimization of a neural network. We start by generating some (highly non-linear) synthetic data using the mlbench package.\n\nlibrary(torch)\ndata &lt;- mlbench::mlbench.friedman3(n = 3000, sd = 0.1)\nX &lt;- torch_tensor(data$x)\nX[1:2, ]\n\ntorch_tensor\n   40.8977   745.3378     0.3715     3.4246\n   88.3017  1148.7073     0.5288     6.5953\n[ CPUFloatType{2,4} ]\n\nY &lt;- torch_tensor(data$y)$unsqueeze(2)\nY[1:2, ]\n\ntorch_tensor\n 1.5238\n 1.3572\n[ CPUFloatType{2,1} ]\n\n\nThe associated machine learning task is to predict the output Y from the input X.\nNext, we create a dataset for it using the tensor_dataset() function.\n\nds &lt;- tensor_dataset(X, Y)\nds$.getitem(1)\n\n[[1]]\ntorch_tensor\n  40.8977\n 745.3378\n   0.3715\n   3.4246\n[ CPUFloatType{4} ]\n\n[[2]]\ntorch_tensor\n 1.5238\n[ CPUFloatType{1} ]\n\n\nWe can create two sub-datasets – for training and validation – using dataset_subset().\n\nids_train &lt;- sample(1000, 700)\nids_valid &lt;- setdiff(seq_len(1000), ids_train)\nds_train &lt;- dataset_subset(ds, ids_train)\nds_valid &lt;- dataset_subset(ds, ids_valid)\n\nThe network that we will be fitting is a simple MLP:\n\nnn_mlp &lt;- nn_module(\"nn_mlp\",\n  initialize = function() {\n    self$lin1 &lt;- nn_linear(4, 50)\n    self$lin2 &lt;- nn_linear(50, 50)\n    self$lin3 &lt;- nn_linear(50, 1)\n  },\n  forward = function(x) {\n    x |&gt;\n      self$lin1() |&gt;\n      nnf_relu() |&gt;\n      self$lin2() |&gt;\n      nnf_relu() |&gt;\n      self$lin3()\n  }\n)\n\nThe code to compare different optimizer configurations is provided through the (provided) compare_configs() function. It takes as arguments:\n\nepochs: The number of epochs to train for. Defaults to 30.\nbatch_size: The batch size to use for training. Defaults to 16.\nlr: The learning rate to use for training. Defaults to 0.01.\nweight_decay: The weight decay to use for training. Defaults to 0.01.\nbeta1: The momentum parameter to use for training. Defaults to 0.9.\nbeta2: The adaptive step size parameter to use for training. Defaults to 0.999.\n\nOne of the arguments (except for epochs) must be a list of values. The function will then run the same training configuration for each of the values in the list and visualize the results.\n\n\nImplementation of compare_configs\n\n\nlibrary(ggplot2)\ncompare_configs &lt;- function(epochs = 30, batch_size = 16, lr = 0.01, weight_decay = 0.01, beta1 = 0.9, beta2 = 0.999) {\n  # Identify which parameter is a list\n  args &lt;- list(batch_size = batch_size, lr = lr, weight_decay = weight_decay, beta1 = beta1, beta2 = beta2)\n  is_list &lt;- sapply(args, is.list)\n\n  if (sum(is_list) != 1) {\n    stop(\"One of the arguments must be a list\")\n  }\n\n  list_arg_name &lt;- names(args)[is_list]\n  list_args &lt;- args[[list_arg_name]]\n  other_args &lt;- args[!is_list]\n\n  # Run train_valid for each value in the list\n  results &lt;- lapply(list_args, function(arg) {\n    network &lt;- with_torch_manual_seed(seed = 123, nn_mlp())\n    other_args[[list_arg_name]] &lt;- arg\n    train_valid(network, ds_train = ds_train, ds_valid = ds_valid, epochs = epochs, batch_size = other_args$batch_size,\n      lr = other_args$lr, betas = c(other_args$beta1, other_args$beta2), weight_decay = other_args$weight_decay)\n  })\n\n  # Combine results into a single data frame\n  combined_results &lt;- do.call(rbind, lapply(seq_along(results), function(i) {\n    df &lt;- results[[i]]\n    df$config &lt;- paste(list_arg_name, \"=\", list_args[[i]])\n    df\n  }))\n\n  upper &lt;- if (max(combined_results$valid_loss) &gt; 10) quantile(combined_results$valid_loss, 0.98) else max(combined_results$valid_loss)\n\n  ggplot(combined_results, aes(x = epoch, y = valid_loss, color = config)) +\n    geom_line() +\n    theme_minimal() +\n    labs(x = \"Epoch\", y = \"Validation RMSE\", color = \"Configuration\") +\n    ylim(min(combined_results$valid_loss), upper)\n}\ntrain_loop &lt;- function(network, dl_train, opt) {\n  network$train()\n  coro::loop(for (batch in dl_train) {\n    opt$zero_grad()\n    Y_pred &lt;- network(batch[[1]])\n    loss &lt;- nnf_mse_loss(Y_pred, batch[[2]])\n    loss$backward()\n    opt$step()\n  })\n}\n\nvalid_loop &lt;- function(network, dl_valid) {\n  network$eval()\n  valid_loss &lt;- c()\n  coro::loop(for (batch in dl_valid) {\n    Y_pred &lt;- with_no_grad(network(batch[[1]]))\n    loss &lt;- sqrt(nnf_mse_loss(Y_pred, batch[[2]]))\n    valid_loss &lt;- c(valid_loss, loss$item())\n  })\n  mean(valid_loss)\n}\n\ntrain_valid &lt;- function(network, ds_train, ds_valid, epochs, batch_size, ...) {\n  opt &lt;- optim_ignite_adamw(network$parameters, ...)\n  train_losses &lt;- numeric(epochs)\n  valid_losses &lt;- numeric(epochs)\n  dl_train &lt;- dataloader(ds_train, batch_size = batch_size)\n  dl_valid &lt;- dataloader(ds_valid, batch_size = batch_size)\n  for (epoch in seq_len(epochs)) {\n    train_loop(network, dl_train, opt)\n    valid_losses[epoch] &lt;- valid_loop(network, dl_valid)\n  }\n  data.frame(epoch = seq_len(epochs), valid_loss = valid_losses)\n}\n\n\nYou can e.g. call the function like below:\n\ncompare_configs(epochs = 30, lr = list(0.1, 0.2), weight_decay = 0.02)\n\n\n\n\n\n\n\n\nExplore a few hyperparameter settings and make some observations as to how they affect the trajectory of the validation loss.\nQuestion 2: Optimization with Momentum\nIn this exercise, you will build a gradient descent optimizer with momentum. As a use case, we will minimize the Rosenbrock function. The function is defined as:\n\nrosenbrock &lt;- function(x, y) {\n  (1 - x)^2 + 2 * (y - x^2)^2\n}\nrosenbrock(torch_tensor(-1), torch_tensor(-1))\n\ntorch_tensor\n 12\n[ CPUFloatType{1} ]\n\n\nThe ‘parameters’ we will be optimizing is the position of a point (x, y), which will both be updated using gradient descent. The figure below shows the Rosenbrock function, where darker values indicate lower values.\n\n\n\n\n\n\n\n\n\nThe task is to implement the optim_step() function.\n\noptim_step &lt;- function(x, y, lr, x_momentum, y_momentum, beta) {\n  ...\n}\n\nIt will receive as arguments, the current values x and y, as well as the momentum values x_momentum and y_momentum (all scalar tensors). The function should then:\n\nPerform a forward pass by calling the rosenbrock() function.\nCall $backward() on the result of the forward pass.\nFirst multiply both momentum values in-place using $mul_() with beta, then add (1 - beta) times the gradient to the momentum values using $add_().\nUpdate the parameters using $sub_() with lr * x_momentum and lr * y_momentum. Perform this update within with_no_grad() as we don’t want to track the gradients for the parameters.\n\nTo perform in-place updates, you can use the $mul_() and $add_() methods.\nThe update rule is given exemplarily for x:\n\\[\n\\begin{aligned}\nv_{t + 1} &= \\beta v_t + (1 - \\beta) \\nabla_{x} f(x_t, y_t) \\\\\n\\end{aligned}\n\\]\n\\[\n\\begin{aligned}\nx_{t+1} &= x_t - \\eta v_{t + 1}\n\\end{aligned}\n\\]\nTo test your optimizer, you can use the following code:\n\noptimize_rosenbrock &lt;- function(steps, lr, beta) {\n  x &lt;- torch_tensor(-1, requires_grad = TRUE)\n  y &lt;- torch_tensor(2, requires_grad = TRUE)\n  momentum_x &lt;- torch_tensor(0)\n  momentum_y &lt;- torch_tensor(0)\n\n  trajectory &lt;- data.frame(\n    x = numeric(steps + 1),\n    y = numeric(steps + 1),\n    value = numeric(steps + 1)\n  )\n  for (step in seq_len(steps)){\n    optim_step(x, y, lr, momentum_x, momentum_y, beta)\n    x$grad$zero_()\n    y$grad$zero_()\n    trajectory$x[step] &lt;- x$item()\n    trajectory$y[step] &lt;- y$item()\n  }\n  trajectory$x[steps + 1] &lt;- x$item()\n  trajectory$y[steps + 1] &lt;- y$item()\n\n  plot_rosenbrock() +\n    geom_path(data = trajectory, aes(x = x, y = y, z = NULL), color = \"red\") +\n    labs(title = \"Optimization Path on Rosenbrock Function\", x = \"X-axis\", y = \"Y-axis\")\n}\n\nQuestion 3: Weight Decay\nIn exercise 2, we have optimized the Rosenbrock function. Does it make sense to also use weight decay for this optimization?"
  },
  {
    "objectID": "notebooks/4-optimizer.html",
    "href": "notebooks/4-optimizer.html",
    "title": "Optimizers",
    "section": "",
    "text": "In this notebook, we will cover the optimization aspect of deep learning and how to work with optimizers in torch. Optimizers are algorithms that iteratively adjust the parameters of a neural network to minimize the loss function during training. They define how the networks learn from the data.\nLet’s denote with \\(L(\\theta)\\) the loss function, which assigns the empirical risk given data \\(\\{(x_i, y_i)\\}_{i = 1}^n\\) to a parameter vector \\(\\theta\\): \\[L(\\theta) = \\sum_{i=1}^n L(f_\\theta(x_i), y_i)\\] Here, \\(f_\\theta\\) is the model’s prediction function, \\(x_i\\) is the \\(i\\)-th sample in the training data, and \\(y_i\\) is the corresponding target value.\nThe goal of the optimizer is to find the parameter vector \\(\\theta^*\\) that minimizes the loss function \\(L(\\theta)\\): \\[\\theta^* = \\arg \\min_\\theta L(\\theta)\\]\nThis is done by iteratively updating the parameter vector \\(\\theta\\) using the gradient of the loss function with respect to the parameter vector. The simplified update formula for a parameter \\(\\theta\\) at time step \\(t\\) is given by:\n\\[\\theta_{t+1} = \\theta_t - \\eta \\frac{\\partial L}{\\partial \\theta_t}\\]\nWhere:\n\n\\(\\theta_t\\) is the current value of the parameter vector at time step \\(t\\).\n\\(\\theta_{t+1}\\) is the new value of the parameter after the update.\n\\(\\eta\\) (eta) is the learning rate, which controls how big of a step we take.\n\\(\\frac{\\partial L}{\\partial \\theta_t}\\) is the derivative of the loss function \\(L\\) with respect to parameter \\(\\theta\\), i.e., the gradient.\n\n\n\n\n\n\n\nQuiz: Learning Rate\n\n\n\nQuestion 1: Can you explain what happens when the learning rate is too high? What happens when it is too low?\n\n\nClick for answer\n\nA too high learning rate will cause the parameters to overshoot the minimum and diverge. A too low learning rate will cause the parameters to converge slowly.  Source: https://stackoverflow.com/questions/62690725/small-learning-rate-vs-big-learning-rate\n\n\n\nThe optimizers used in practice differ from the above formula, as:\n\nThe gradient is estimated from a batch rather than the entire training dataset.\nThe simplistic update formula is extended with:\n\nWeight decay\nMomentum\nAdaptive learning rates\n\n\nBefore we cover these more advanced approaches (specifically their implementation in AdamW), we will first focus on the vanilla version of Stochastic Gradient Descent (SGD)."
  },
  {
    "objectID": "notebooks/4-optimizer.html#overview",
    "href": "notebooks/4-optimizer.html#overview",
    "title": "Optimizers",
    "section": "",
    "text": "In this notebook, we will cover the optimization aspect of deep learning and how to work with optimizers in torch. Optimizers are algorithms that iteratively adjust the parameters of a neural network to minimize the loss function during training. They define how the networks learn from the data.\nLet’s denote with \\(L(\\theta)\\) the loss function, which assigns the empirical risk given data \\(\\{(x_i, y_i)\\}_{i = 1}^n\\) to a parameter vector \\(\\theta\\): \\[L(\\theta) = \\sum_{i=1}^n L(f_\\theta(x_i), y_i)\\] Here, \\(f_\\theta\\) is the model’s prediction function, \\(x_i\\) is the \\(i\\)-th sample in the training data, and \\(y_i\\) is the corresponding target value.\nThe goal of the optimizer is to find the parameter vector \\(\\theta^*\\) that minimizes the loss function \\(L(\\theta)\\): \\[\\theta^* = \\arg \\min_\\theta L(\\theta)\\]\nThis is done by iteratively updating the parameter vector \\(\\theta\\) using the gradient of the loss function with respect to the parameter vector. The simplified update formula for a parameter \\(\\theta\\) at time step \\(t\\) is given by:\n\\[\\theta_{t+1} = \\theta_t - \\eta \\frac{\\partial L}{\\partial \\theta_t}\\]\nWhere:\n\n\\(\\theta_t\\) is the current value of the parameter vector at time step \\(t\\).\n\\(\\theta_{t+1}\\) is the new value of the parameter after the update.\n\\(\\eta\\) (eta) is the learning rate, which controls how big of a step we take.\n\\(\\frac{\\partial L}{\\partial \\theta_t}\\) is the derivative of the loss function \\(L\\) with respect to parameter \\(\\theta\\), i.e., the gradient.\n\n\n\n\n\n\n\nQuiz: Learning Rate\n\n\n\nQuestion 1: Can you explain what happens when the learning rate is too high? What happens when it is too low?\n\n\nClick for answer\n\nA too high learning rate will cause the parameters to overshoot the minimum and diverge. A too low learning rate will cause the parameters to converge slowly.  Source: https://stackoverflow.com/questions/62690725/small-learning-rate-vs-big-learning-rate\n\n\n\nThe optimizers used in practice differ from the above formula, as:\n\nThe gradient is estimated from a batch rather than the entire training dataset.\nThe simplistic update formula is extended with:\n\nWeight decay\nMomentum\nAdaptive learning rates\n\n\nBefore we cover these more advanced approaches (specifically their implementation in AdamW), we will first focus on the vanilla version of Stochastic Gradient Descent (SGD)."
  },
  {
    "objectID": "notebooks/4-optimizer.html#mini-batch-effects-in-sgd",
    "href": "notebooks/4-optimizer.html#mini-batch-effects-in-sgd",
    "title": "Optimizers",
    "section": "Mini-Batch Effects in SGD",
    "text": "Mini-Batch Effects in SGD\nWhen using mini-batches, the gradient becomes a noisy estimate of the gradient over the full dataset. With \\(\\nabla L^i_t := \\frac{\\partial L^i}{\\partial \\theta_t}\\) being the gradient of the loss function with respect to the entire parameter vector estimated using \\((x_i, y_i)\\), the mini-batch gradient is given by:\n\\[\\nabla L^B_t = \\frac{1}{|B|} \\sum_{i \\in B} \\nabla L^i_t\\]\nwhere \\(B\\) is the batch of samples and \\(|B|\\) is the batch size.\nThe update formula for SGD is then given by:\n\\[\\theta_{t+1} = \\theta_t - \\eta \\nabla L^B_t\\]\nThis is visualized in the image below:\n\n\n\n\n\n\n\nQuiz: Vanilla SGD\n\n\n\nQuestion 1: What happens when the batch size is too small or too large?\n\n\nClick for answer\n\nTrade-offs with Batch Size:\n\nLarger batches provide more accurate gradient estimates.\nSmaller batches introduce more noise but allow more frequent parameter updates. \n\n\nQuestion 2: The mini-batch gradient is an approximation of the gradient over the full dataset. Does the latter also approximate something? If so, what?\n\n\nClick for answer\n\nIn machine learning, we assume that the data is drawn from a distribution \\(P\\). The gradient over the full dataset is an expectation over this distribution:\n\\[\\nabla L = \\mathbb{E}_{x \\sim P} \\nabla L(f_\\theta(x), y)\\]\nThe mini-batch gradient is an empirical estimate of this gradient, i.e., the expectation over a finite sample from the distribution.\n\n\n\nBecause deep learning models can have many parameters and computing gradients is expensive, understanding the effects of different batch sizes and convergence is important. The computational cost (which we define as the time it takes to perform one optimization step) of a gradient update using a batch size \\(b\\) consists of:\n\nLoading the batch into memory (if the data does not fit into RAM).\nThe forward pass of the model.\nThe backward pass of the model.\nThe update of the parameters.\n\nWe will discuss point 1 later, and point 4 does not depend on the batch size, so we can ignore it.\n\n\n\n\n\n\nQuiz: Bang for Your Buck\n\n\n\nQuestion 1: True or False: The cost of performing a gradient update using a batch size of \\(2\\) is twice the cost of a batch size of \\(1\\).\n\n\nClick for answer\n\nFalse. Because GPUs can perform many operations simultaneously, the cost of performing a gradient update using a batch size of \\(2\\) is not twice the cost of a batch size of \\(1\\).\n\nQuestion 2: The standard error of the mini-batch gradient estimate (which characterizes the precision of the gradient estimate) can be written as:\n\\[\\text{SE}_{\\nabla L^B_t} = \\frac{\\sigma_{\\nabla L_t}}{\\sqrt{|B|}}\\]\nwhere \\(\\sigma_{\\nabla L_t}\\) is the standard deviation of the gradient estimate relative to the batch size.\nDescribe the dynamics of the standard error when increasing the batch size: How do you need to increase a batch size from \\(1\\) to achieve half the standard error? What about increasing a batch size from \\(100\\)?\n\n\nClick for answer\n\nThe standard error decreases as the batch size increases, but with diminishing returns. To halve the standard error:\n\nIncrease the batch size from \\(1\\) to \\(4\\).\nIncrease the batch size from \\(100\\) to \\(400\\).\n\nThis is because the standard error is inversely proportional to the square root of the batch size."
  },
  {
    "objectID": "notebooks/4-optimizer.html#mini-batch-gradient-descent-its-not-all-about-runtime",
    "href": "notebooks/4-optimizer.html#mini-batch-gradient-descent-its-not-all-about-runtime",
    "title": "Optimizers",
    "section": "Mini-Batch Gradient Descent: It’s not all about runtime",
    "text": "Mini-Batch Gradient Descent: It’s not all about runtime\nAs we have now covered some of the dynamics of a simple gradient-based optimizer, we can examine the final parameter vector \\(\\theta^*\\) that the optimizer converges to. When using a gradient-based optimizer, the updates will stop once the gradient is close to zero. We will now discuss the type of solutions where this is true and their properties.\nWe need to distinguish saddle points from local minima from global minima:\n\nIn deep learning, where high-dimensional parameter spaces are common, saddle points are more likely to occur than local minima [@dauphin2014identifying]. However, due to the stochastic nature of SGD, optimizers will find local minima instead of saddle points @pmlr-v80-daneshmand18a.\n\n\n\n\n\n\nQuiz: Local vs. Global Minima, Generalization\n\n\n\nQuestion 1: Do you believe SGD will find local or global minima? Explain your reasoning.\n\n\nClick for answer\n\nBecause the gradient only has local information about the loss function, SGD finds local minima.\n\nQuestion 2: Assuming we have found a \\(\\theta^*\\) that has low training loss, does this ensure that we have found a good model?\n\n\nClick for answer\n\nNo, because we only know that the model has low training loss, but not the test loss.\n\n\n\nSGD has been empirically shown to find solutions that generalize well to unseen data. This phenomenon is attributed to the implicit regularization effects of SGD, where the noise introduced by mini-batch sampling helps guide the optimizer towards broader minima with smaller L2 norms. These broader minima are typically associated with better generalization performance compared to sharp minima.\n\nSource: https://www.researchgate.net/figure/Flat-minima-results-in-better-generalization-compared-to-sharp-minima-Pruning-neural_fig2_353068686\nThese properties are also known as implicit regularization of SGD. Regularization generally refers to techniques that prevent overfitting and improve generalization. There are also explicit regularization techniques, which we will cover next.\n\nWeight Decay\nBecause weight decay in SGD is equivalent to adding a regularization penalty term to the loss function, we can draw a parallel to the regularization techniques used in statistics such as ridge regression. Regularization in machine learning/statistics is used to prevent overfitting by adding a penalty term to the model’s loss function, which discourages overly complex models that might fit noise in the training data. It helps improve generalization to unseen data. For example, in ridge regression, the regularization term penalizes large coefficients by adding the squared magnitude of the coefficients to the loss function:\n\\[\n\\mathcal{L}(y, \\hat{y}) = \\sum_{i=1}^n \\left(y_i - \\hat{y}_i\\right)^2 + \\lambda \\sum_{j=1}^p \\beta_j^2\n\\]\nThis will make the model prefer less complex solutions, where complexity is measured by the L2 norm of the coefficients.\n\n\n\n\n\n\nNote\n\n\n\nFor more complex optimizers such as Adam, weight decay is not equivalent to adding a regularization penalty term to the loss function. However, the main idea of both approaches is still to shrink the weights to \\(0\\) during training.\n\n\nHere, \\(\\lambda\\) controls the strength of the regularization, \\(y_i\\) are the observed values, \\(\\hat{y}_i\\) are the predicted values, and \\(w_i\\) are the model coefficients.\nIf we integrate weight decay into the gradient update formula, we get the following:\n\\[\\theta_{t+1} = \\theta_t - \\eta \\big(\\frac{\\partial L}{\\partial \\theta_t} - \\lambda \\theta_t\\big)\\]\nThis formula shows that the weight decay term (\\(- \\lambda \\theta_t\\)) effectively shrinks the weights during each update, helping to prevent overfitting."
  },
  {
    "objectID": "notebooks/4-optimizer.html#momentum",
    "href": "notebooks/4-optimizer.html#momentum",
    "title": "Optimizers",
    "section": "Momentum",
    "text": "Momentum\nMomentum is a technique that helps accelerate gradient descent by using an exponential moving average of past gradients. Like a ball rolling down a hill, momentum helps the optimizer:\n\nMove faster through areas of consistent gradient direction.\nPush through sharp local minima and saddle points.\nDampen oscillations in areas where the gradient frequently changes direction.\n\nThe exponential moving momentum update can be expressed mathematically as:\n\\[\n(1 - \\beta) \\sum_{\\tau=1}^{t} \\beta^{t-\\tau} \\nabla_{\\theta} \\mathcal{L}(\\theta_{\\tau-1})\n\\]\nIn order to avoid having to keep track of all the gradients, we can calculate the update in two steps as follows:\n\\[\nv_t = \\beta_1 v_{t-1} + (1 - \\beta_1) \\nabla_\\theta L(\\theta_t)\n\\]\n\\[\n\\theta_{t+1} = \\theta_t - \\eta \\frac{v_t}{1 - \\beta_1^t}\n\\]\nThe hyperparameter \\(\\beta_1\\) is the momentum decay rate (typically 0.9), \\(v_t\\) is the exponential moving average of gradients, and \\(\\eta\\) is the learning rate as before. Note that dividing by \\(1 - \\beta_1^t\\) counteracts a bias because \\(v_0\\) is initialized to \\(0\\)."
  },
  {
    "objectID": "notebooks/4-optimizer.html#adaptive-learning-rates",
    "href": "notebooks/4-optimizer.html#adaptive-learning-rates",
    "title": "Optimizers",
    "section": "Adaptive Learning Rates",
    "text": "Adaptive Learning Rates\nAdaptive learning rate methods automatically adjust the learning rate for each parameter during training. This is particularly useful because:\n\nDifferent parameters may require different learning rates.\nThe optimal learning rate often changes during training.\n\nBefore, we had one global learning rate \\(\\eta\\) for all parameters. However, learning rates are now allowed to:\n\nChange over time.\nBe different for different parameters.\n\nOur vanilla SGD update formula is now generalized to handle adaptive learning rates:\n\\[\\theta_{t+1} = \\theta_t - \\eta_t \\cdot \\frac{\\nabla_\\theta L(\\theta_t)}{\\sqrt{v_t} + \\epsilon}\\]\nHere, \\(\\eta_t\\) is now not a scalar learning rate, but a vector of learning rates for each parameter, and ‘\\(\\cdot\\)’ denotes the element-wise multiplication. Further, \\(\\epsilon\\) is a small constant for numerical stability.\nIn AdamW, the adaptive learning rate is controlled by the second moment estimate (squared gradients):\n\\[v_t = \\beta_2 v_{t-1} + (1-\\beta_2)(g_t)^2\\] \\[\\hat{\\eta}_t = \\eta \\frac{1}{\\sqrt{v_t + \\epsilon}}\\]\nIn words, this means: In steep directions where the gradient is large, the learning rate is small and vice versa. The parameters \\(\\beta_2\\) and \\(\\epsilon\\) are hyperparameters that control the decay rate and numerical stability of the second moment estimate.\n\nWhen combining weight decay, adaptive learning rates, and momentum, we get the AdamW optimizer. It therefore has parameters:\n\nlr: The learning rate.\nweight_decay: The weight decay parameter.\nbetas: The momentum parameters (\\(\\beta_1\\) and \\(\\beta_2\\)).\neps: The numerical stability parameter.\n\nNote that AdamW also has another configuration parameter amsgrad, which is disabled by default in torch, but which can help with convergence."
  },
  {
    "objectID": "notebooks/4-optimizer.html#learning-rate-schedules",
    "href": "notebooks/4-optimizer.html#learning-rate-schedules",
    "title": "Optimizers",
    "section": "Learning Rate Schedules",
    "text": "Learning Rate Schedules\nWhile we have already covered dynamic learning rates, it can still be beneficial to use a learning rate scheduler to further improve convergence. There, the learning rate is not a constant scalar, but a function of the current epoch. The update formula for the simple SGD optimizer is now:\n\\[\\theta_{t+1} = \\theta_t - \\eta_t \\cdot \\frac{\\nabla_\\theta L(\\theta_t)}{\\sqrt{v_t} + \\epsilon}\\]\nDecaying learning rates:\nThis includes gradient decay, cosine annealing, and cyclical learning rates. The general idea is to start with a high learning rate and then gradually decrease it over time.\nWarmup:\nWarmup is a technique that gradually increases the learning rate from a small value to a larger value over a specified number of epochs. For an explanation of why warmup is beneficial, see @kalra2024warmup.\nCyclical Learning Rates:\nCyclical learning rates are a technique that involves periodically increasing and decreasing the learning rate. This can help the optimizer to traverse saddle points faster and find better solutions.\n\n\n\n\n\n\n\n\n\nIn torch, learning rate schedulers are prefixed by lr_, such as the simple lr_step, where the learning rate is multiplied by a factor of gamma every step_size epochs. In order to use them, we need to pass the optimizer to the scheduler and specify additional arguments.\n\nscheduler = lr_step(opt, step_size = 2, gamma = 0.1)\n\nThe main API of a learning rate scheduler is the $step() method, which updates the learning rate. For some schedulers, this needs to be called after each optimization step, for others after each epoch. You can find this out by consulting the documentation of the specific scheduler.\n\nopt$param_groups[[1L]]$lr\n\n[1] 0.2\n\nscheduler$step()\nopt$param_groups[[1L]]$lr\n\n[1] 0.2\n\nscheduler$step()\nopt$param_groups[[1L]]$lr\n\n[1] 0.02"
  },
  {
    "objectID": "notebooks/4-optimizer.html#setting-the-learning-rate",
    "href": "notebooks/4-optimizer.html#setting-the-learning-rate",
    "title": "Optimizers",
    "section": "Setting the Learning Rate",
    "text": "Setting the Learning Rate\nArguably the most important hyperparameter is the learning rate. While we have now discussed the dynamics of the optimizer hyperparameters, the primary practical concern is how to set them. As a start, one can see whether good results can be achieved with the default hyperparameters. Alternatively, one can look at how others (e.g., in scientific papers) have set the hyperparameters for similar architectures and tasks.\nWhen setting the learning rate, it is a good idea to then inspect the loss over time to see whether the learning rate is too high (instability) or too low (slow convergence). Below, we show the learning curve for two different learning rates.\n\nlibrary(mlr3torch)\n\nLoading required package: mlr3\n\n\nLoading required package: mlr3pipelines\n\nepochs = 40\nl1 = lrn(\"classif.mlp\", batch_size = 32, epochs = epochs, opt.lr = 0.01, callbacks = t_clbk(\"history\"), measures_train = msr(\"classif.logloss\"), predict_type = \"prob\", neurons = 100)\nl2 = lrn(\"classif.mlp\", batch_size = 32, epochs = epochs, opt.lr = 0.001, callbacks = t_clbk(\"history\"), measures_train = msr(\"classif.logloss\"), predict_type = \"prob\", neurons = 100)\ntask = tsk(\"spam\")\nl1$train(task)\nl2$train(task)\n\nd = data.frame(\n  epoch = rep(1:epochs, times = 2),\n  logloss = c(l1$model$callbacks$history$train.classif.logloss, l2$model$callbacks$history$train.classif.logloss),\n  lr = rep(c(\"0.01\", \"0.001\"), each = epochs)\n)\n\n\nggplot(d, aes(x = epoch, y = logloss, color = lr)) +\n  geom_line() +\n  theme_minimal()\n\n\n\n\n\n\n\n\nWhen no good results can easily be achieved with defaults or with learning rates from the literature, one can employ hyperparameter optimization to find good learning rates. It is recommended to tune the learning rate on a logarithmic scale."
  },
  {
    "objectID": "notebooks/4-optimizer.html#saving-an-optimizer",
    "href": "notebooks/4-optimizer.html#saving-an-optimizer",
    "title": "Optimizers",
    "section": "Saving an Optimizer",
    "text": "Saving an Optimizer\nIn order to resume training at a later stage, we can save the optimizer’s state using $state_dict().\n\nstate_dict = opt$state_dict()\n\nThis state dictionary contains:\n\nThe $param_groups which contains the parameters and their associated hyperparameters.\nThe $state which contains the optimizer’s internal state, such as the momentum and second moment estimates.\n\n\nstate_dict$param_groups[[1L]]\n\n$params\n[1] 1 2\n\n$lr\n[1] 0.02\n\n$betas\n[1] 0.900 0.999\n\n$eps\n[1] 1e-08\n\n$weight_decay\n[1] 0.01\n\n$amsgrad\n[1] FALSE\n\n$initial_lr\n[1] 0.2\n\n\n\n\n\n\n\n\nNote\n\n\n\nIt is possible to set different parameters (such as learning rate) for different parameter groups.\n\no2 = optim_adamw(list(\n  list(params = torch_tensor(1), lr = 1),\n  list(params = torch_tensor(2), lr = 2)\n))\no2$param_groups[[1L]]$lr\n\n[1] 1\n\no2$param_groups[[2L]]$lr\n\n[1] 2\n\n\n\n\nThe $state field contains the state for each parameter, which is currently empty as we have not performed any updates yet.\n\nstate_dict$state\n\n$`1`\n$`1`$step\ntorch_tensor\n1\n[ CPUFloatType{} ]\n\n$`1`$exp_avg\ntorch_tensor\n0.01 *\n 5.9926\n[ CPUFloatType{1,1} ]\n\n$`1`$exp_avg_sq\ntorch_tensor\n0.0001 *\n 3.5912\n[ CPUFloatType{1,1} ]\n\n\n$`2`\n$`2`$step\ntorch_tensor\n1\n[ CPUFloatType{} ]\n\n$`2`$exp_avg\ntorch_tensor\n0.01 *\n 2.1779\n[ CPUFloatType{1} ]\n\n$`2`$exp_avg_sq\ntorch_tensor\n1e-05 *\n 4.7433\n[ CPUFloatType{1} ]\n\n\n\nstep = function() {\n  opt$zero_grad()\n  ((model(torch_tensor(1)) - torch_tensor(1))^2)$backward()\n  opt$step()\n}\nreplicate(step(), n = 2)\n\nAfter performing two steps, the state dictionary contains the state for each parameter:\n\nopt$state_dict()$state[[\"1\"]]\n\n$step\ntorch_tensor\n3\n[ CPUFloatType{} ]\n\n$exp_avg\ntorch_tensor\n-0.6202\n[ CPUFloatType{1,1} ]\n\n$exp_avg_sq\ntorch_tensor\n0.01 *\n 2.5144\n[ CPUFloatType{1,1} ]\n\n\nJust like for the nn_module, we can save the optimizer state using torch_save().\n\npth = tempfile(fileext = \".pth\")\ntorch_save(state_dict, pth)\n\n\n\n\n\n\n\nWarning\n\n\n\nGenerally, we don’t want to save the whole optimizer, as this also contains the weight tensors of the model that one usually wants to save separately.\n\n\n\nstate_dict2 = torch_load(pth)\nopt2 &lt;- optim_adamw(model$parameters, lr = 0.2)\nopt2$load_state_dict(state_dict2)"
  },
  {
    "objectID": "notebooks/5-mlr3torch-exercise-task.html",
    "href": "notebooks/5-mlr3torch-exercise-task.html",
    "title": "Training Neural Networks with mlr3torch",
    "section": "",
    "text": "Question 1: Hello World!\nIn this exercise, you will train your first neural network with mlr3torch.\nAs a task, we will use the ‘Indian Liver Patient’ dataset where the goal is to predict whether a patient has liver disease or not.\n\nlibrary(mlr3verse)\n\nLoading required package: mlr3\n\nlibrary(mlr3torch)\n\nLoading required package: mlr3pipelines\n\n\nLoading required package: torch\n\nilpd &lt;- tsk(\"ilpd\")\nilpd\n\n&lt;TaskClassif:ilpd&gt; (583 x 11): Indian Liver Patient Data\n* Target: diseased\n* Properties: twoclass\n* Features (10):\n  - dbl (5): albumin, albumin_globulin_ratio, direct_bilirubin, total_bilirubin, total_protein\n  - int (4): age, alanine_transaminase, alkaline_phosphatase, aspartate_transaminase\n  - fct (1): gender\n\nautoplot(ilpd)\n\n\n\n\n\n\n\n\nWe remove the gender column from the task, so we need to deal with numeric features for now.\n\nilpd_num = ilpd$clone(deep = TRUE)\nilpd_num$select(setdiff(ilpd_num$feature_names, \"gender\"))\n\nTrain a simple multi layer perceptron (lrn(\"classif.mlp\")) with 2 hidden layers with 100 neurons each. Set the batch size to 32, the learning rate to 0.001 and the number of epochs to 20. Then, resample the learner on the task with a cross-validation with 5 folds and evaluate the results using classification error and false positive rate (FPR). Is the result good?\n\n\nHint\n\n\nThe parameter for the learning rate is opt.lr\nProbability predictions are made by setting the predict_type field to \"prob\".\n\n\nQuestion 2: Preprocessing\nIn the previous task, we have operated on the ilpd_num task where we excluded the categorical gender column. This was done because the MLP learner operates on numeric features only. We will now create a more complex GraphLearner that also incudes one-hot encoding of the gender column. Resample this learner on the original ilpd task and evaluate the results using the same measures as before.\n\n\nHint\n\nConcatenate po(\"encode\") with a lrn(\"classif.mlp\") using %&gt;&gt;% to create the GraphLearner. For available options on the encoding, see po(\"encode\")$help().\n\nQuestion 3: Benchmarking\nInstead of resampling a single learner, we now want to compare the performance of our MLP with a simple classification tree Create a benchmark design and compare the performance of the two learners.\n\n\nHint\n\nCreate a classification tree via lrn(\"classif.rpart\"). A benchmark design can be created via benchmark_grid(). To run a benchmark, pass the design to benchmark()."
  },
  {
    "objectID": "notebooks/5-mlr3torch.html",
    "href": "notebooks/5-mlr3torch.html",
    "title": "Training Neural Networks with mlr3torch",
    "section": "",
    "text": "mlr3torch is a package that extends the mlr3 framework with deep learning capabilities, allowing the application of deep learning techniques to both tabular and non-tabular data. The package implements many routines common in deep learning and allows users to focus on the actual problem at hand. Some advantages of using mlr3torch over ‘only’ torch are:\n\nLess Code: Avoid writing repetitive boilerplate code by utilizing predefined network architectures or easily building custom ones tailored to your specific needs.\nmlr3 Integration: Especially for users with experience in the mlr3 framework, working with mlr3torch should feel familiar. Due to the integration into the mlr3 framework, many mlr3 features like hyperparameter tuning, preprocessing, and resampling are readily available for mlr3torch.\n\nHowever, as mlr3torch is a framework, it is less flexible than torch itself, so knowing both is recommended. Another helpful R package that provides many useful functions to train neural networks is luz."
  },
  {
    "objectID": "notebooks/5-mlr3torch.html#why-use-mlr3torch",
    "href": "notebooks/5-mlr3torch.html#why-use-mlr3torch",
    "title": "Training Neural Networks with mlr3torch",
    "section": "",
    "text": "mlr3torch is a package that extends the mlr3 framework with deep learning capabilities, allowing the application of deep learning techniques to both tabular and non-tabular data. The package implements many routines common in deep learning and allows users to focus on the actual problem at hand. Some advantages of using mlr3torch over ‘only’ torch are:\n\nLess Code: Avoid writing repetitive boilerplate code by utilizing predefined network architectures or easily building custom ones tailored to your specific needs.\nmlr3 Integration: Especially for users with experience in the mlr3 framework, working with mlr3torch should feel familiar. Due to the integration into the mlr3 framework, many mlr3 features like hyperparameter tuning, preprocessing, and resampling are readily available for mlr3torch.\n\nHowever, as mlr3torch is a framework, it is less flexible than torch itself, so knowing both is recommended. Another helpful R package that provides many useful functions to train neural networks is luz."
  },
  {
    "objectID": "notebooks/5-mlr3torch.html#mlr3-recap",
    "href": "notebooks/5-mlr3torch.html#mlr3-recap",
    "title": "Training Neural Networks with mlr3torch",
    "section": "mlr3 Recap",
    "text": "mlr3 Recap\nBefore diving into mlr3torch, we will briefly review the core building blocks of the mlr3 machine learning framework. For reference, we recommend the mlr3 book that explains the mlr3 framework in more detail. Additionally, the mlr3 website contains more tutorials and overviews.\n\nTask\nA task is a machine learning problem on a dataset. It consists of the data itself and some metadata such as the features or the target variable. To create an example task that comes with mlr3, we can use the tsk() function:\n\nlibrary(mlr3)\ntsk(\"iris\")\n\n&lt;TaskClassif:iris&gt; (150 x 5): Iris Flowers\n* Target: Species\n* Properties: multiclass\n* Features (4):\n  - dbl (4): Petal.Length, Petal.Width, Sepal.Length, Sepal.Width\n\n\nTo create a custom Task from a data.frame, we can use the as_task_&lt;type&gt; converters:\n\nhead(iris)\n\n  Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n1          5.1         3.5          1.4         0.2  setosa\n2          4.9         3.0          1.4         0.2  setosa\n3          4.7         3.2          1.3         0.2  setosa\n4          4.6         3.1          1.5         0.2  setosa\n5          5.0         3.6          1.4         0.2  setosa\n6          5.4         3.9          1.7         0.4  setosa\n\ntsk_iris &lt;- as_task_classif(iris, id = \"iris\", target = \"Species\")\ntsk_iris\n\n&lt;TaskClassif:iris&gt; (150 x 5)\n* Target: Species\n* Properties: multiclass\n* Features (4):\n  - dbl (4): Petal.Length, Petal.Width, Sepal.Length, Sepal.Width\n\n\n\n\n\n\n\n\nTip\n\n\n\nTo get the help page for an mlr3 object, you can call tsk_iris$help().\n\n\nYou can access the data of a task using the $data() method, which accepts arguments rows and cols to select specific rows and columns.\n\ntsk_iris$data(rows = 1:5, cols = c(\"Sepal.Length\", \"Sepal.Width\"))\n\n   Sepal.Length Sepal.Width\n          &lt;num&gt;       &lt;num&gt;\n1:          5.1         3.5\n2:          4.9         3.0\n3:          4.7         3.2\n4:          4.6         3.1\n5:          5.0         3.6\n\n\nUsing the mlr3viz extension, we can get an overview of the task:\n\nlibrary(mlr3viz)\nautoplot(tsk_iris)\n\n\n\n\n\n\n\n\n\n\nLearner\nA learner is a machine learning algorithm that can be $train()ed on a Task and $predict()ed on a Task. An overview of all learners is shown on our website. We can construct one by passing the name of the learner to the lrn() function.\n\nlrn_tree &lt;- lrn(\"classif.rpart\")\n\nNext, we need to split the data into a training and test set and apply the learner on the former.\n\nsplit &lt;- partition(tsk_iris, ratio = 0.8)\nlrn_tree$train(tsk_iris, row_ids = split$train)\n\nThe trained model can be accessed via the $model slot of the learner:\n\nprint(lrn_tree$model)\n\nn= 120 \n\nnode), split, n, loss, yval, (yprob)\n      * denotes terminal node\n\n1) root 120 75 virginica (0.33333333 0.29166667 0.37500000)  \n  2) Petal.Length&lt; 2.45 40  0 setosa (1.00000000 0.00000000 0.00000000) *\n  3) Petal.Length&gt;=2.45 80 35 virginica (0.00000000 0.43750000 0.56250000)  \n    6) Petal.Length&lt; 4.75 32  1 versicolor (0.00000000 0.96875000 0.03125000) *\n    7) Petal.Length&gt;=4.75 48  4 virginica (0.00000000 0.08333333 0.91666667) *\n\n\nTo make predictions on the test set, we can use the $predict() method of the learner:\n\npredictions &lt;- lrn_tree$predict(tsk_iris, row_ids = split$test)\n\nTo make predictions on data.frames, we can use the $predict_newdata() method of the learner:\n\nnew_data &lt;- iris[1:2, ]\nlrn_tree$predict_newdata(new_data)\n\n&lt;PredictionClassif&gt; for 2 observations:\n row_ids  truth response\n       1 setosa   setosa\n       2 setosa   setosa\n\n\n\n\nPerformance Evaluation\nTo assess the quality of the predictions, we can use a Measure. mlr3 comes with many predefined measures, and we can construct them by passing the name of the measure to the msr() function. Below, we construct the mean classification accuracy measure – which can only be applied to classification tasks – and use it to evaluate the predictions.\n\nacc &lt;- msr(\"classif.acc\")\npredictions$score(acc)\n\nclassif.acc \n  0.9333333 \n\n\nFor more elaborate evaluation strategies, we can use rsmp() to define a Resampling strategy that can be executed using resample().\n\nrsmp_cv &lt;- rsmp(\"cv\", folds = 3)\n\nrr &lt;- resample(\n  task       = tsk_iris,\n  learner    = lrn_tree,\n  resampling = rsmp_cv\n)\n\n# Aggregate the results\nrr$aggregate(msr(\"classif.acc\"))\n\nclassif.acc \n  0.9466667 \n\n\n\n\nHyperparameter Tuning\nHyperparameter tuning is an essential process in machine learning to optimize the performance of models by selecting the best combination of hyperparameters. In the mlr3 framework, hyperparameter tuning is facilitated by the mlr3tuning extension, which provides a flexible and powerful interface for defining, searching, and evaluating hyperparameters.\n\nKey Concepts\n\nHyperparameters: Configurable settings provided to the learning algorithm before training begins, such as learning rate, number of trees in a random forest, or regularization parameters.\nSearch Space: The range of values or distributions from which hyperparameters are sampled during the tuning process.\nResampling Strategy: A method to evaluate the performance of a hyperparameter configuration, commonly using techniques like cross-validation or bootstrapping.\nTuner: An algorithm that explores the search space to find the optimal hyperparameters. Common tuners include grid search, random search, and Bayesian optimization.\n\n\n\nWorkflow\n\nDefine the Search Space: Specify the range and distribution of hyperparameters to explore.\n\nlibrary(mlr3tuning)\n\nLoading required package: paradox\n\nlrn_tree$configure(\n  cp = to_tune(lower = 0.001, upper = 0.1),\n  maxdepth = to_tune(lower = 1, upper = 30)\n)\n\nChoose a Resampling Strategy: Determine how to evaluate each hyperparameter configuration’s performance.\n\nrsmp_tune &lt;- rsmp(\"cv\", folds = 3)\n\nSelect a Tuner: Decide on the algorithm that will search through the hyperparameter space.\n\ntuner &lt;- tnr(\"random_search\")\n\nSelect a Measure: Define the metric to optimize during tuning.\n\nmsr_tune &lt;- msr(\"classif.acc\")\n\nExecute Tuning: Run the tuning process to find the optimal hyperparameters. Here we also specify our budget of 10 evaluations.\n\ntune_result &lt;- tune(\n  task = tsk_iris,\n  learner = lrn_tree,\n  resampling = rsmp_tune,\n  measure = msr_tune,\n  tuner = tuner,\n  term_evals = 10L\n)\n\nApply the Best Hyperparameters: Update the learner with the best-found hyperparameters and retrain the model.\n\nlrn_tree$param_set$values &lt;- tune_result$result_learner_param_vals\nlrn_tree$train(tsk_iris)\n\n\n\n\n\n\n\n\nQuiz: Tuning Performance\n\n\n\nQuestion 1: Estimating the performance of a tuned model:\nThrough the tuning archive, we can access the performance of the best-found hyperparameter configuration.\n\ntune_result$archive$data[order(classif.acc, decreasing = TRUE), ][1, classif.acc]\n\n[1] 0.94\n\n\nDo you think this is a good estimate for the performance of the final model? Explain your answer.\n\n\nClick for answer\n\nOne reason why we would expect the performance of the final model to be worse than the performance of the best-found hyperparameter configuration is due to optimization bias: We choose the model configuration with the highest validation performance. This selection process biases the result since the chosen model is the best among several trials. To illustrate this, imagine that we take the maximum of 10 random numbers drawn from a normal distribution with mean 0. The maximum over those numbers is larger than \\(0\\), even though this is the mean of the generating distribution.\n\n\n\n\n\n\n\n\n\n\n\n\nThese two steps can also be encapsulated in the AutoTuner class, which first finds the best hyperparameters and then trains the model with them.\n\nat &lt;- auto_tuner(\n  learner = lrn_tree,\n  resampling = rsmp_tune,\n  measure = msr_tune,\n  term_evals = 10L,\n  tuner = tuner\n)\n\nThe AutoTuner can be used just like any other Learner. To get a valid performance estimate of the tuning process, we can resample() it on the task. This is called nested resampling: the outer resampling is for evaluation and the inner resampling is for tuning.\n\nrr &lt;- resample(tsk_iris, at, rsmp_tune)\nrr$aggregate()\n\nclassif.ce \n0.05333333 \n\n\n\n\n\nLearning Pipelines\nIn many cases, we don’t only fit a single learner but a whole learning pipeline. Common use cases include the preprocessing of the data, e.g., for imputing missing values, scaling the data, or encoding categorical features, but many other operations are possible. The mlr3 extension mlr3pipelines is a toolbox for defining such learning pipelines. Its core building block is the PipeOp that can be constructed using the po() function.\n\nlibrary(mlr3pipelines)\npca &lt;- po(\"pca\")\n\nJust like a learner, it has a $train() and $predict() method, and we can apply it to a Task using these methods.\n\npca$train(list(tsk_iris))\n\n$output\n&lt;TaskClassif:iris&gt; (150 x 5)\n* Target: Species\n* Properties: multiclass\n* Features (4):\n  - dbl (4): PC1, PC2, PC3, PC4\n\npca$predict(list(tsk_iris))[[1L]]\n\n&lt;TaskClassif:iris&gt; (150 x 5)\n* Target: Species\n* Properties: multiclass\n* Features (4):\n  - dbl (4): PC1, PC2, PC3, PC4\n\n\nUsually, such PipeOps are combined with a Learner into a full learning Graph. This is possible using the %&gt;&gt;% chain operator.\n\nlibrary(mlr3pipelines)\ngraph &lt;- po(\"pca\") %&gt;&gt;% lrn(\"classif.rpart\")\nprint(graph)\n\nGraph with 2 PipeOps:\n            ID         State      sccssors prdcssors\n        &lt;char&gt;        &lt;char&gt;        &lt;char&gt;    &lt;char&gt;\n           pca &lt;&lt;UNTRAINED&gt;&gt; classif.rpart          \n classif.rpart &lt;&lt;UNTRAINED&gt;&gt;                     pca\n\ngraph$plot(horizontal = TRUE)\n\n\n\n\n\n\n\n\nThe resulting Graph can be converted back into a Learner using the as_learner() function and used just like any other Learner.\n\nglrn &lt;- as_learner(graph)\nglrn$train(tsk_iris)"
  },
  {
    "objectID": "notebooks/5-mlr3torch.html#brief-introduction-to-mlr3torch",
    "href": "notebooks/5-mlr3torch.html#brief-introduction-to-mlr3torch",
    "title": "Training Neural Networks with mlr3torch",
    "section": "Brief Introduction to mlr3torch",
    "text": "Brief Introduction to mlr3torch\nmlr3torch builds upon the same components as mlr3, only that we use Deep Learners, and can also work on non-tabular data. A simple example learner is the lrn(\"classif.mlp\") learner, which is a Multi-Layer Perceptron (MLP) for classification tasks.\n\nUsing a Predefined Torch Learner\n\nlibrary(mlr3torch)\nlrn_mlp &lt;- lrn(\"classif.mlp\",\n  neurons = c(50, 50), # Two hidden layers with 50 neurons each\n  batch_size = 256, # Number of samples per gradient update\n  epochs = 30, # Number of training epochs\n  device = \"auto\", # Uses GPU if available, otherwise CPU\n  shuffle = TRUE, # because iris is sorted\n  optimizer = t_opt(\"adam\") # Adam optimizer\n)\n\nThis multi-layer perceptron can be used just like the classification tree above.\n\nlrn_mlp$train(tsk_iris, row_ids = split$train)\n\nThe trained nn_module can be accessed via the $model slot of the learner:\n\nlrn_mlp$model$network\n\nAn `nn_module` containing 2,953 parameters.\n\n── Modules ─────────────────────────────────────────────────────────────────────────────────────────────────────────────\n• 0: &lt;nn_linear&gt; #250 parameters\n• 1: &lt;nn_relu&gt; #0 parameters\n• 2: &lt;nn_dropout&gt; #0 parameters\n• 3: &lt;nn_linear&gt; #2,550 parameters\n• 4: &lt;nn_relu&gt; #0 parameters\n• 5: &lt;nn_dropout&gt; #0 parameters\n• 6: &lt;nn_linear&gt; #153 parameters\n\n\nBesides the trained network, the $model of the learner also contains the $state_dict() of the optimizer and other information.\nHaving trained the neural network on the training set, we can now make predictions on the test set:\n\npredictions &lt;- lrn_mlp$predict(tsk_iris, row_ids = split$test)\npredictions$score(msr(\"classif.acc\"))\n\nclassif.acc \n        0.5 \n\n\nUsing the benchmarking facilities of mlr3, we can also easily compare the classification tree with our deep learning learner:\n\n# Define the resampling strategy\nrsmp_cv &lt;- rsmp(\"cv\", folds = 3)\n\n# Create a benchmark grid to compare both learners\nbenchmark_grid &lt;- benchmark_grid(\n  tasks = tsk_iris,\n  learners = list(lrn_tree, lrn_mlp),\n  resampling = rsmp_cv\n)\n\n# Run the benchmark\nrr_benchmark &lt;- benchmark(benchmark_grid)\n\n# Aggregate the results\nresults_benchmark &lt;- rr_benchmark$aggregate(msr(\"classif.acc\"))\n\n# Print the results\nprint(results_benchmark)\n\n      nr task_id    learner_id resampling_id iters classif.acc\n   &lt;int&gt;  &lt;char&gt;        &lt;char&gt;        &lt;char&gt; &lt;int&gt;       &lt;num&gt;\n1:     1    iris classif.rpart            cv     3        0.94\n2:     2    iris   classif.mlp            cv     3        0.64\nHidden columns: resample_result\n\n\n\n\nValidation Performance\nTracking validation performance is crucial for understanding how well your neural network is learning and to detect issues such as overfitting. In the mlr3 machine learning framework, this can be easily done by specifying the $validate field of a Learner. Note that this is not possible for all Learners, but only for those with the \"validation\" property. This includes boosting algorithms such as XGBoost or CatBoost, and also the mlr3torch learners.\nBelow, we set the validation ratio to 30% of the training data, specify the measures to record, and set the callbacks of the learner to record the history of the training process.\n\nlrn_mlp$configure(\n  validate = 0.3,\n  callbacks = t_clbk(\"history\"),\n  predict_type = \"prob\",\n  measures_valid = msr(\"classif.logloss\"),\n  measures_train = msr(\"classif.logloss\")\n)\n\n\n\n\n\n\n\nTip\n\n\n\nThe $configure() method of a Learner allows you to simultaneously set fields and hyperparameters of a learner.\n\n\nWhen we now train the learner, 30% of the training data is used for validation, and the loss is recorded in the history of the learner.\n\nlrn_mlp$train(tsk_iris)\n\nAfter training, the results of the callback can be accessed via the model.\n\nhead(lrn_mlp$model$callbacks$history)\n\nKey: &lt;epoch&gt;\n   epoch train.classif.logloss valid.classif.logloss\n   &lt;num&gt;                 &lt;num&gt;                 &lt;num&gt;\n1:     1              1.216219             1.0216911\n2:     2              1.090373             1.0141709\n3:     3              1.103897             1.0070298\n4:     4              1.163823             1.0011182\n5:     5              1.100077             0.9970761\n6:     6              1.111388             0.9937620\n\n\nAdditionally, the final validation scores can be accessed via the $internal_valid_scores field of the learner.\n\nlrn_mlp$internal_valid_scores\n\n$classif.logloss\n[1] 0.8368236\n\n\n\n\nDefining a Custom Torch Learner\nmlr3torch also allows defining custom architectures by assembling special PipeOps in a Graph. As a starting point in the graph, we need to mark the entry of the Neural Network using an ingress pipeop. Because we are working with a task with only one numeric feature, we can use po(\"torch_ingress_num\"). There also exist inputs for categorical features (po(\"torch_ingress_cat\")) and generic tensors (po(\"torch_ingress_ltnsr\")).\n\ningress &lt;- po(\"torch_ingress_num\")\n\nThe next steps in the graph are the actual layers of the neural network.\n\narchitecture &lt;- po(\"nn_linear_1\", out_features = 100) %&gt;&gt;%\n  po(\"nn_relu_1\") %&gt;&gt;%\n  po(\"nn_linear_2\", out_features = 100) %&gt;&gt;%\n  po(\"nn_relu_2\") %&gt;&gt;%\n  po(\"nn_head\")\n\narchitecture$plot(horizontal = TRUE)\n\n\n\n\n\n\n\n\nAfter specifying the architecture, we need to set the remaining parts for the learner, which are the loss, optimizer, and the remaining training configuration such as the epochs, device, or the batch size.\n\ngraph &lt;- ingress %&gt;&gt;% architecture %&gt;&gt;%\n  po(\"torch_loss\", loss = \"cross_entropy\") %&gt;&gt;%\n  po(\"torch_optimizer\", optimizer = t_opt(\"adam\")) %&gt;&gt;%\n  po(\"torch_model_classif\", epochs = 10, batch_size = 256)\n\nJust like before, we can convert the graph into a Learner using as_learner() and train it on the task:\n\nglrn &lt;- as_learner(graph)\nglrn$train(tsk_iris, row_ids = split$train)\n\n\n\nWorking with Non-Tabular Data\nIn the mlr3 ecosystem, the data of a task is always stored in a data.frame or data.table. To work with non-tabular data, the mlr3torch package offers a custom datatype, the lazy_tensor, which can be stored in a data.table.\nAs an example to showcase this, we can use the CIFAR-10 dataset, which is a dataset of 60,000 32x32 color images in 10 classes, with 6,000 images per class.\n\ntsk_cifar &lt;- tsk(\"cifar10\")\ntsk_cifar\n\n&lt;TaskClassif:cifar10&gt; (60000 x 2): CIFAR-10 Classification\n* Target: class\n* Properties: multiclass\n* Features (1):\n  - lt (1): image\n\n\nThe image below shows some examples from the dataset:\n\n\n\n\n\n\n\nTip\n\n\n\nTo avoid having to re-download the dataset every time, you can set the mlr3torch.cache option to TRUE.\n\noptions(mlr3torch.cache = TRUE)\n\n\n\nWhen accessing the data, only the images are represented as lazy_tensors, the labels are still stored as a factor column:\n\ntsk_cifar$head()\n\n        class           image\n       &lt;fctr&gt;   &lt;lazy_tensor&gt;\n1:       frog &lt;tnsr[3x32x32]&gt;\n2:      truck &lt;tnsr[3x32x32]&gt;\n3:      truck &lt;tnsr[3x32x32]&gt;\n4:       deer &lt;tnsr[3x32x32]&gt;\n5: automobile &lt;tnsr[3x32x32]&gt;\n6: automobile &lt;tnsr[3x32x32]&gt;\n\n\nA lazy_tensor column can be compared to the torch::dataset class that we have seen earlier. This means it does not necessarily store the data in memory, but only stores the information on how to load the data.\n\nimage_vec &lt;- tsk_cifar$data(cols = \"image\")[[1L]]\nhead(image_vec)\n\n&lt;ltnsr[6]&gt;\n[1] &lt;tnsr[3x32x32]&gt; &lt;tnsr[3x32x32]&gt; &lt;tnsr[3x32x32]&gt; &lt;tnsr[3x32x32]&gt; &lt;tnsr[3x32x32]&gt; &lt;tnsr[3x32x32]&gt;\n\n\nTo access the data as torch_tensors, we can call the materialize() function:\n\nimage_tensor &lt;- materialize(image_vec)[[1L]]\n\nTo construct the CIFAR-10 task ourselves, we need to first:\n\nConstruct a torch::dataset that returns the images as torch_tensors.\nCreate a factor() vector that contains the labels.\n\nWe can access the dataset via the torchvision package. For simplicity, we will only load the training data.\n\ncifar10_original &lt;- torchvision::cifar10_dataset(root = \"data/cifar10\", train = TRUE, download = TRUE)\nimage_array &lt;- cifar10_original$x\nstr(image_array)\n\n int [1:50000, 1:32, 1:32, 1:3] 59 154 255 28 170 159 164 28 134 125 ...\n\n\nThe array contains 50,000 images (rows) of shape 32x32x3. The last dimension contains the channels, i.e., the RGB values of the pixels. We reshape this so the channel dimension is the first dimension, which is the standard format for images in torch.\n\nimage_array &lt;- aperm(image_array, c(1, 4, 2, 3))\ndim(image_array)\n\n[1] 50000     3    32    32\n\n\nNext, we create a torch::dataset() that loads individual images as a torch_tensor. To convert this to a lazy_tensor later, the $.getitem() method needs to return a named list.\n\ndataset_cifar10 &lt;- dataset(\"cifar10\",\n  initialize = function(x) {\n    self$x &lt;- x\n  },\n  .getitem = function(i) {\n    list(image = torch_tensor(self$x[i, , , ]))\n  },\n  .length = function() {\n    nrow(self$x)\n  }\n)\n\nThe above object is not yet a dataset, but a dataset constructor, so we create the actual dataset by calling it with the image array.\n\ncifar10 &lt;- dataset_cifar10(image_array)\n\nWe can check that this works correctly by accessing the first image.\n\nstr(cifar10$.getitem(1))\n\nList of 1\n $ image:Long [1:3, 1:32, 1:32]\n\n\nTo convert this to a lazy_tensor, we can use the as_lazy_tensor() function. The only thing we need to specify is the output shapes of the tensors, which we set to c(NA, 3, 32, 32). The NA is used to indicate that the first dimension (batch dimension) can be of any size.\n\ncifar10_lt &lt;- as_lazy_tensor(cifar10, dataset_shapes = list(image = c(NA, 3, 32, 32)))\nhead(cifar10_lt)\n\n&lt;ltnsr[6]&gt;\n[1] &lt;tnsr[3x32x32]&gt; &lt;tnsr[3x32x32]&gt; &lt;tnsr[3x32x32]&gt; &lt;tnsr[3x32x32]&gt; &lt;tnsr[3x32x32]&gt; &lt;tnsr[3x32x32]&gt;\n\n\n\n\n\n\n\n\nTip\n\n\n\nTo check that transformations on images were applied correctly, it can be useful to inspect the images, e.g., using torchvision::tensor_image_browse().\n\n\nNext, we create the factor vector that contains the labels. For that, we use the data stored in the original torchvision::cifar10_dataset() object.\n\nlabels &lt;- factor(cifar10_original$y, labels = cifar10_original$classes[1:10])\nstr(labels)\n\n Factor w/ 10 levels \"airplane\",\"automobile\",..: 7 10 10 5 2 2 3 8 9 4 ...\n\n\nNext, we create a data.table that contains the images and labels.\n\ncifar10_dt &lt;- data.table(image = cifar10_lt, label = labels)\nhead(cifar10_dt)\n\n             image      label\n     &lt;lazy_tensor&gt;     &lt;fctr&gt;\n1: &lt;tnsr[3x32x32]&gt;       frog\n2: &lt;tnsr[3x32x32]&gt;      truck\n3: &lt;tnsr[3x32x32]&gt;      truck\n4: &lt;tnsr[3x32x32]&gt;       deer\n5: &lt;tnsr[3x32x32]&gt; automobile\n6: &lt;tnsr[3x32x32]&gt; automobile\n\n\nThis table can now be converted to an mlr3::Task using the as_task_&lt;type&gt; converters.\n\ntsk_cifar &lt;- as_task_classif(cifar10_dt, id = \"cifar10\", target = \"label\")\ntsk_cifar\n\n&lt;TaskClassif:cifar10&gt; (50000 x 2)\n* Target: label\n* Properties: multiclass\n* Features (1):\n  - lt (1): image\n\n\nWe will now try to train a simple multi-layer perceptron – the one we have defined above – on the images. One problem that we have is that the images are of shape 32x32x3, but the nn_linear layer expects a flat input of size 3072 (32 * 32 * 3).\nThis is where the lazy_tensor datatype comes in handy. We can use the PipeOps to transform the data before it is loaded. Here, the -1 in the shape c(-1, 3072) indicates that the first dimension (batch dimension) can be of any size.\n\nreshaper &lt;- po(\"trafo_reshape\", shape = c(-1, 3072))\ntsk_cifar_flat &lt;- reshaper$train(list(tsk_cifar))[[1L]]\ntsk_cifar_flat$head()\n\n        label         image\n       &lt;fctr&gt; &lt;lazy_tensor&gt;\n1:       frog  &lt;tnsr[3072]&gt;\n2:      truck  &lt;tnsr[3072]&gt;\n3:      truck  &lt;tnsr[3072]&gt;\n4:       deer  &lt;tnsr[3072]&gt;\n5: automobile  &lt;tnsr[3072]&gt;\n6: automobile  &lt;tnsr[3072]&gt;\n\n\nNote that this transformation is not applied eagerly, but only when the data is actually loaded.\n\n\n\n\n\n\nNote\n\n\n\nIn this case, as all the images are stored in memory, we could have also applied the transformation directly to the array representing the images, but decided not to do this for demonstration purposes.\n\n\nWe can now use almost the same graph as before on the flattened task. We only need to exchange the ingress, as the new task has as data a lazy_tensor instead of numeric vectors.\n\ngraph &lt;- po(\"torch_ingress_ltnsr\") %&gt;&gt;% architecture %&gt;&gt;%\n  po(\"torch_loss\", loss = t_loss(\"cross_entropy\")) %&gt;&gt;%\n  po(\"torch_optimizer\", optimizer = t_opt(\"adam\")) %&gt;&gt;%\n  po(\"torch_model_classif\", epochs = 10, batch_size = 256)\nglrn &lt;- as_learner(graph)\n\n\nglrn$train(tsk_cifar_flat)\n\nAlternatively, we can integrate the flattening step into the graph from which the GraphLearner was created.\n\ngraph_with_flattening &lt;- reshaper %&gt;&gt;% graph\nglrn_with_flattening &lt;- as_learner(graph_with_flattening)\n\nThis learner can now be applied directly to the (unflattened) task.\n\nglrn_with_flattening$param_set$set_values(torch_model_classif.epochs = 0)\n\n\nglrn_with_flattening$train(tsk_cifar)\n\n\n\nSaving an mlr3torch Learner\nWe have seen earlier that torch tensors cannot simply be saved using saveRDS(). The same also applies to the mlr3torch learners. To save an mlr3torch learner, we need to call the $marshal() method first.\n\npth &lt;- tempfile(fileext = \".rds\")\nglrn_with_flattening$marshal()\nsaveRDS(glrn_with_flattening, pth)\n\nAfterward, we can load the learner again using readRDS() and the $unmarshal() method.\n\nglrn_with_flattening &lt;- readRDS(pth)\nglrn_with_flattening$unmarshal()"
  },
  {
    "objectID": "notebooks/6-training-efficiency-exercise-task.html",
    "href": "notebooks/6-training-efficiency-exercise-task.html",
    "title": "Training Efficiency",
    "section": "",
    "text": "Question 1: Validation\nIn this exercise, we will once again train a simple multi-layer perceptron on the Indian Liver Patient Dataset (ILPD). Create a learner that:\n\nUses 2 hidden layers with 100 neurons each.\nUtilizes a batch size of 128.\nTrains for 200 epochs.\nEmploys a validation set comprising 30% of the data.\nTracks the training and validation log-loss during training.\nUtilizes trace-jitting to speed up the training process.\nEmploys the history callback to record the training and validation log-loss during training.\n\nAfterward, plot the validation log-loss, which is accessible via learner$model$callbacks$history.\nBelow, we create the task and remove the gender feature for simplicity.\n\nlibrary(mlr3verse)\n\nLoading required package: mlr3\n\nlibrary(mlr3torch)\n\nLoading required package: mlr3pipelines\n\n\nLoading required package: torch\n\nilpd_num &lt;- tsk(\"ilpd\")\nilpd_num$select(setdiff(ilpd_num$feature_names, \"gender\"))\nilpd_num\n\n&lt;TaskClassif:ilpd&gt; (583 x 10): Indian Liver Patient Data\n* Target: diseased\n* Properties: twoclass\n* Features (9):\n  - dbl (5): albumin, albumin_globulin_ratio, direct_bilirubin, total_bilirubin, total_protein\n  - int (4): age, alanine_transaminase, alkaline_phosphatase, aspartate_transaminase\n\n\n\n\nHint\n\n\nTo specify the validation set, use the validate field, which can either be set during construction or by calling $configure().\nTrace-jitting can be enabled via the jit_trace parameter.\nThe history callback can be constructed via t_clbk(\"history\") and needs to be passed during the construction of the learner.\nThe validation and measures can be specified via measures_valid and take a measure object that is constructed via msr().\n\n\nQuestion 2: Early Stopping Enable early stopping to prevent overfitting and re-train the learner (using a patience of 10). Print the final validation performance of the learner and the early stopped results. You can consult the documentation of LearnerTorch on how to access these two results (section Active Bindings).\n\n\nHint\n\nYou can enable early stopping by setting the patience parameter.\n\nQuestion 3: Early Stopping and Dropout Tuning\nWhile early stopping in itself is already useful, mlr3torch also allows you to simultaneously tune the number of epochs using early stopping while tuning other hyperparameters via traditional hyperparameter tuning from mlr3tuning.\nOne thing we have not covered so far is that the MLP learner we have used so far also uses a dropout layer. The dropout probability can be configured via the p parameter.\nYour task is to tune the dropout probability p in the range \\([0, 1]\\) and the epochs using early stopping (using the configuration from the previous exercise).\nTo adapt this to work with early stopping, you need to set the:\n\nepochs to to_tune(upper = &lt;value&gt;, internal = TRUE): This tells the Tuner that the learner will tune the number of epochs itself.\n$validate field of the \"test\" so the same data is used for tuning and validation.\nTuning measure to msr(\"internal_valid_score\", minimize = TRUE). We set minimize to TRUE because we have used the log-loss as a validation measure.\n\nApart from this, the tuning works just like in tutorial 5. Use 3-fold cross-validation for the tuning and evaluate 10 configurations.\nRun the tuning and print the optimal configuration."
  },
  {
    "objectID": "notebooks/6-training-efficiency.html",
    "href": "notebooks/6-training-efficiency.html",
    "title": "Training Efficiency",
    "section": "",
    "text": "Methods for increasing training efficiency can be roughly split into:"
  },
  {
    "objectID": "notebooks/6-training-efficiency.html#parallel-processing",
    "href": "notebooks/6-training-efficiency.html#parallel-processing",
    "title": "Training Efficiency",
    "section": "Parallel Processing",
    "text": "Parallel Processing\n\nGraphical Processing Unit (GPU)\nUsing a GPU is crucial when training relatively large neural networks because GPUs are specifically designed to handle the parallel processing required for complex computations. To use a GPU in mlr3torch, we can set the device parameter to “cuda”. By default, it is set to “auto”, which will use a GPU if it is available and otherwise fall back to the CPU.\n\n\n\n\n\n\nTip\n\n\n\nTo check if a GPU is available, we can use the torch::cuda_is_available() function.\n\nlibrary(torch)\ncuda_is_available()\n\n[1] FALSE\n\n\nIf you have an M1 Mac (or later), you can also use the available graphics card by setting the device parameter to \"mps\". You can check this by running:\n\nbackends_mps_is_available()\n\n[1] TRUE\n\n\n\n\nTo demonstrate the speed improvements obtained by using a GPU, we conduct a large matrix operation on a GPU and a CPU. We start by randomly sampling a matrix of size 1000x1000.\n\nx_cpu = torch_randn(1000, 1000, device = \"cpu\")\n\nBelow, we perform a matrix multiplication on the CPU and the GPU and compare the timings.\n\n# this will only run if a GPU is available\nx_cuda = x_cpu$cuda()\n\nbench::mark(\n  cpu = x_cpu$matmul(x_cpu),\n  cuda = x_cuda$matmul(x_cuda)\n)\n\n\n\nCPU Threads\nTraining large networks on a CPU is not a recommended approach, but it can be useful for smaller networks or when you don’t have a GPU. You can still use multiple threads to speed up the execution of operations. Note that the code below will not run on macOS, as it is not possible to set the number of threads on macOS.\n\n# this will be skipped on macOS\nbench::mark(\n  {torch_set_num_threads(1L); x_cpu$matmul(x_cpu)},\n  {torch_set_num_threads(16L); x_cpu$matmul(x_cpu)}\n)\n\ntorch also allows for interop-parallelization, but this is more advanced and code needs to be written in a specific way.\n\n\n\n\n\n\nQuiz: Number of Threads\n\n\n\nQuestion 1: On a CPU with 4 cores, does it make sense to set the number of threads to values greater than 4? Explain your answer.\n\n\nClick for answer\n\nOn a CPU with 4 cores, at most 4 threads can run in parallel. Using more threads than the number of cores will not speed up the execution of operations.\n\nQuestion 2: On a CPU with 64 cores, is it always the case that using 64 threads is better than using 32 threads?\n\n\nClick for answer\n\nNot necessarily. Using more threads will mean that:\n\nThe threads need to communicate and synchronize, which increases the runtime.\nMore resources are used for the computation, which decreases the runtime.\n\nThe optimal number of threads is a trade-off between these two effects."
  },
  {
    "objectID": "notebooks/6-training-efficiency.html#efficient-data-loading",
    "href": "notebooks/6-training-efficiency.html#efficient-data-loading",
    "title": "Training Efficiency",
    "section": "Efficient Data Loading",
    "text": "Efficient Data Loading\nBesides speeding up the computation of operations in the forward and backward pass, another possible bottleneck is the loading of data. There are various ways to improve data loading speed:\n\nImprove the implementation of the dataset class\nParallelize the data loading process\nMove data to the GPU\n\nThese approaches will now be discussed.\n\nEfficient Dataset Implementation\nWhen implementing a dataset, we need to define:\n\nHow we store and load the data\nWhether implementing loading of a batch is beneficial\n\n\n\n\n\n\n\nQuiz: Data Loading\n\n\n\nThe tiny imagenet dataset is a dataset of 100,000 images of size 64x64x3. It is a subset of the famous imagenet dataset. Below, we show some examples from the dataset:\n\nWe will now consider different ways to write a torch::dataset implementation for this data. Assume we have some image paths stored in a character vector as well as in an array where they are already loaded into memory.\n\nstr(image_paths)\n\n chr [1:100] \"/Users/sebi/Library/Caches/org.R-project.R/R/mlr3torch/datasets/tiny_imagenet/raw/tiny-imagenet-200/train/n0144\"| __truncated__ ...\n\nstr(image_array)\n\n num [1:100, 1:3, 1:64, 1:64] 1 0.0784 0.4706 0.5608 0.5647 ...\n\n\nAn individual image can, for example, be loaded using the torchvision::base_loader() function:\n\nlibrary(torchvision)\nstr(base_loader(image_paths[1]))\n\n num [1:64, 1:64, 1:3] 1 1 1 1 1 ...\n\n\nQuestion 1: Reading From Disk or RAM\nWhich of the following is the faster way to load the images? Explain why.\n\nLoading the images from disk:\n\nds_disk = dataset(\"image_paths\",\n  initialize = function(image_paths) {\n    self$image_paths = image_paths\n  },\n  .getitem = function(i) {\n    torch_tensor(torchvision::base_loader(self$image_paths[i]))\n  },\n  .length = function() {\n    length(self$image_paths)\n  }\n)(image_paths)\n\nLoading the images from an array:\n\nds_ram = dataset(\"image_array\",\n  initialize = function(image_array) {\n    self$image_array = image_array\n  },\n  .getbatch = function(i) {\n    torch_tensor(self$image_array[i, , , ])\n  },\n  .length = function() {\n    nrow(self$image_array)\n  }\n)(image_array)\n\n\n\n\nClick for answer\n\nGenerally, loading images from RAM is significantly faster than loading them from disk. Although the benchmark presented below may seem somewhat ‘unfair’ since ds_ram has already loaded the images into memory, this difference is evident in practice. When iterating over the dataset for multiple epochs, the first method will need to reload the images from disk for each epoch, while the second method only requires a single loading of the images into memory.\n\niter = function(ds, ..., epochs = 1) {\n  dl = torch::dataloader(ds, batch_size = 16, ...)\n  for (epoch in seq_len(epochs)) {\n    coro::loop(for(batch in dl) {\n      batch\n    })\n  }\n}\nbench::mark(\n  disk = iter(ds_disk),\n  ram = iter(ds_ram),\n  check = FALSE\n)\n\n# A tibble: 2 × 6\n  expression      min   median `itr/sec` mem_alloc `gc/sec`\n  &lt;bch:expr&gt; &lt;bch:tm&gt; &lt;bch:tm&gt;     &lt;dbl&gt; &lt;bch:byt&gt;    &lt;dbl&gt;\n1 disk        19.81ms  22.13ms      44.0      14MB     13.8\n2 ram          8.75ms   9.75ms      97.1     9.4MB     22.2\n\n\n\nQuestion 2: (Don’t) Copy that\nConsider now the next dataset implementation:\n\nds_tensor = dataset(\"tensor\",\n  initialize = function(image_array) {\n    self$tensor = torch_tensor(image_array)\n  },\n  .getitem = function(i) {\n    self$tensor[i, ..]\n  },\n  .length = function() {\n    nrow(self$tensor)\n  }\n)(image_array)\n\nDo you think this implementation is faster or slower than the ds_ram implementation? Explain why.\n\n\nClick for answer\n\nThis implementation is faster than the ds_ram implementation. This is because the ds_tensor implementation copies the R array to a torch tensor only once, whereas the ds_ram implementation copies the R array to a torch tensor for each item.\n\nbench::mark(\n  tensor = iter(ds_tensor),\n  array = iter(ds_ram),\n  check = FALSE\n)\n\n# A tibble: 2 × 6\n  expression      min   median `itr/sec` mem_alloc `gc/sec`\n  &lt;bch:expr&gt; &lt;bch:tm&gt; &lt;bch:tm&gt;     &lt;dbl&gt; &lt;bch:byt&gt;    &lt;dbl&gt;\n1 tensor       4.45ms    5.1ms      195.   96.08KB     6.71\n2 array        8.42ms   9.75ms      101.    9.38MB    25.9 \n\n\n\nQuestion 3: $.getbatch() vs $.getitem()\nWhich implementation is faster? Explain why.\n\nds_tensor_batch = dataset(\"tensor_batch\",\n  initialize = function(image_array) {\n    self$tensor = torch_tensor(image_array)\n  },\n  .getbatch = function(i) {\n    self$tensor[i, ..]\n  },\n  .length = function() {\n    nrow(self$tensor)\n  }\n)(image_array)\n\n\n\nClick for answer\n\nThe $.getbatch() implementation is faster than the $.getitem() implementation. This is because when using the $.getitem() method, the batch for indices ids is obtained by calling $.getitem(id) for each index in ids and then stacking them together, which requires a new tensor allocation. Slicing the tensor, however, avoids this allocation when shuffle = TRUE (which is also the default).\n\nbench::mark(\n  getbatch = iter(ds_tensor_batch),\n  getitem = iter(ds_tensor),\n  check = FALSE\n)\n\n# A tibble: 2 × 6\n  expression      min   median `itr/sec` mem_alloc `gc/sec`\n  &lt;bch:expr&gt; &lt;bch:tm&gt; &lt;bch:tm&gt;     &lt;dbl&gt; &lt;bch:byt&gt;    &lt;dbl&gt;\n1 getbatch     1.66ms   2.01ms      473.    3.83KB     4.44\n2 getitem      4.71ms   5.14ms      192.   54.69KB     7.19\n\n\n\n\n\n\n\nParallel Data Loading\nIn Deep Learning, datasets can be very large, and it might therefore be the case that the data is simply too large to fit into memory. In this case, we can use parallel data loading to speed up the data loading process. Instead of loading the data sequentially in the main process, other R processes will be started that execute the data loading. For example, if we set num_workers = 4L, 4 R processes will be started that load the data, while the main process is free to train the model. These processes then send the batches to the main process. The image below visualizes this process:\n\nCreating such a parallel dataloader is as easy as setting the num_workers parameter to a value greater than 0.\n\n\n\n\n\n\nNote\n\n\n\nNote that there is some communication overhead that results from sending the batches from the worker to the main process. This will hopefully be reduced in the future, but is currently there. For this reason, parallel data loading is therefore – currently – only beneficial when it is slow, e.g., because of loading the data from disk or because of expensive preprocessing.\n\n\n\n\nMoving Data to the GPU\nOne thing we have ignored so far is that when training using a GPU, the data needs to be moved to the GPU. This is because a GPU has its own memory (VRAM), and the data needs to be moved to this memory before it can be used for training. The moving of the data to the GPU cannot be done on the processes that are loading the data but must be done in the main process, i.e., after the batch was received from (possibly parallelized) dataloader. One way to speed up the data loading process is to pin the memory of the data to the GPU. Before a tensor can be moved from RAM to VRAM, it needs to be in so-called page-locked memory, which can be done using the pin_memory parameter.\n\n\niter_cuda = function(ds, pin_memory = TRUE) {\n  dl = torch::dataloader(ds, batch_size = 16, pin_memory = pin_memory)\n  coro::loop(for(batch in dl) {\n    batch$cuda()\n  })\n}\n\nbench::mark(\n  not_pinned = iter_cuda(ds_disk, pin_memory = FALSE),\n  pinned = iter_cuda(ds_disk, pin_memory = TRUE)\n)\n\n\n\n\n\n\n\nNote\n\n\n\nIn order to use parallel data loading or memory pinning with mlr3torch, these parameters can simply be specified in the learner:\n\nlrn(\"classif.mlp\", num_workers = 8L, pin_memory = TRUE, device = \"cuda\")\n\n&lt;LearnerTorchMLP[classif]:classif.mlp&gt;: My Little Powny\n* Model: -\n* Parameters: device=cuda, num_threads=1, num_interop_threads=1, seed=random, jit_trace=FALSE, eval_freq=1,\n  measures_train=&lt;list&gt;, measures_valid=&lt;list&gt;, patience=0, min_delta=0, num_workers=8, pin_memory=TRUE,\n  neurons=integer(0), p=0.5, activation=&lt;nn_relu&gt;, activation_args=&lt;list&gt;\n* Validate: NULL\n* Packages: mlr3, mlr3torch, torch\n* Predict Types:  [response], prob\n* Feature Types: integer, numeric, lazy_tensor\n* Properties: internal_tuning, marshal, multiclass, twoclass, validation\n* Optimizer: adam\n* Loss: cross_entropy\n* Callbacks: -"
  },
  {
    "objectID": "notebooks/6-training-efficiency.html#jit-compilation-ignite-optimizers",
    "href": "notebooks/6-training-efficiency.html#jit-compilation-ignite-optimizers",
    "title": "Training Efficiency",
    "section": "JIT Compilation & Ignite Optimizers",
    "text": "JIT Compilation & Ignite Optimizers\nSome special care needs to be taken when using torch (or mlr3torch) in order to get good performance. In the future, this will hopefully not be necessary anymore, but is currently required.\n\n‘Ignite’ Optimizers\nIn torch, different versions of optimizers exist:\n\noptim_adamw\n\n&lt;optim_adamw&gt; object generator\n  Inherits from: &lt;inherit&gt;\n  Public:\n    initialize: function (params, lr = 0.001, betas = c(0.9, 0.999), eps = 1e-08, \n    loop_fun: function (group, param, g, p) \n    step: function (closure = NULL) \n    clone: function (deep = FALSE) \n  Parent env: &lt;environment: 0x130e5aee0&gt;\n  Locked objects: FALSE\n  Locked class: FALSE\n  Portable: TRUE\n\noptim_ignite_adamw\n\n&lt;optim_ignite_adamw&gt; object generator\n&lt;optim_ignite&gt; object generator\n  Inherits from: &lt;inherit&gt;\n  Public:\n    initialize: function (params, lr = 0.001, betas = c(0.9, 0.999), eps = 1e-08, \n    clone: function (deep = FALSE) \n  Private:\n    .config_names: lr betas eps weight_decay amsgrad\n    .state_names: exp_avg exp_avg_sq max_exp_avg_sq step\n    .optim: function (params, ...) \n    .get_states: function (opt) \n    .set_states: function (opt, params, states) \n    .add_param_group: function (opt, params, lr, betas, eps, weight_decay, amsgrad) \n    .assert_params: function (lr, betas, eps, weight_decay, amsgrad) \n    .set_param_group_options: function (opt, list) \n    .zero_grad: function (opt) \n    .get_param_groups: function (ptr) \n  Parent env: &lt;environment: 0x1171a0710&gt;\n  Locked objects: FALSE\n  Locked class: FALSE\n  Portable: TRUE\n\n\nThe ‘ignite’ indicates that the optimizer is a version that is optimized for performance. Not for all optimizers does an ignite version exist, but for the most common ones, it does.\nBelow, we compare the performance of the default optimizer and the ignite optimizer and see that the latter is considerably faster.\n\nadamw = as_torch_optimizer(torch::optim_adamw)\nignite_adamw = as_torch_optimizer(torch::optim_ignite_adamw)\n\nlearner = lrn(\"classif.mlp\", epochs = 10, neurons = c(100, 100), batch_size = 32, optimizer = adamw)\n\nlearner_ignite = learner$clone(deep = TRUE)\nlearner_ignite$configure(\n  optimizer = ignite_adamw\n)\ntask_sonar = tsk(\"sonar\")\n\nbench::mark(\n  learner$train(task_sonar),\n  learner_ignite$train(task_sonar),\n  check = FALSE\n)\n\nWarning: Some expressions had a GC in every iteration; so filtering is disabled.\n\n\n# A tibble: 2 × 6\n  expression                            min   median `itr/sec` mem_alloc `gc/sec`\n  &lt;bch:expr&gt;                       &lt;bch:tm&gt; &lt;bch:tm&gt;     &lt;dbl&gt; &lt;bch:byt&gt;    &lt;dbl&gt;\n1 learner$train(task_sonar)           625ms    625ms      1.60    15.7MB     4.80\n2 learner_ignite$train(task_sonar)    228ms    260ms      3.68    11.2MB     4.91\n\n\n\n\nJIT Compilation\nJIT (Just-In-Time) compilation is a runtime optimization technique that compiles code into machine code during execution rather than beforehand. This has different advantages:\n\nBy JIT-compiling a model, some operations can be optimized for performance.\nA JIT-compiled model can be saved and executed without an R dependency for deployment (only LibTorch is required), e.g., in a C++ application.\nRunning a JIT-compiled model in R is faster because the whole network is executed in C++ instead of R.\n\nIn torch, this can either be done using TorchScript or by tracing a model. We will briefly discuss both approaches, but for more information, see the torch documentation.\n\nTorchScript\nTorchScript is a subset of Python – i.e., its own programming language – that can be used to define compiled functions. In R, this is available via the jit_compile function.\n\nf = jit_compile(\"\ndef f(x, w, bias):\n  return x @ w + bias\n\")$f\n\nx = torch_randn(10, 10)\nw = torch_randn(10, 1)\nbias = torch_randn(1)\n\nout = f(x, w, bias)\nstr(out)\n\nFloat [1:10, 1:1]\n\n\nBesides syntax, there are some important differences between TorchScript and R:\n\nIn TorchScript, indexing tensors is 0-based, and\nTorchScript is statically typed, so you need to specify the types of the arguments, unless they are tensors, which is the default.\n\nBelow, we define a function that takes a list of tensors and calculates their sum.\n\nsum_jit = jit_compile(\"\ndef sum_jit(xs: List[Tensor]):\n  output = torch.zeros_like(xs[0])\n  for x in xs:\n    output = output + x\n  return output\n\")$sum_jit\n\nsum_jit(list(torch_randn(1), torch_randn(1)))\n\ntorch_tensor\n-0.7121\n[ CPUFloatType{1} ]\n\n\n\n\nTracing\nThe alternative to writing TorchScript is to write your module in R and to use jit_trace to compile it.\n\nf2 = function(x, w, bias) {\n  x$matmul(w) + bias\n}\n# need to provide some example input\n# arguments are passed by position\nf2 = jit_trace(f2, torch_randn(10, 10), torch_randn(10, 100), torch_randn(100))\nout2 = f2(x, w, bias)\ntorch_equal(out, out2)\n\n[1] TRUE\n\n\nAn advantage of trace-compilation is that it even allows you to JIT-compile modules, which is currently not possible with jit_compile.\n\nnet = nn_sequential(\n  nn_linear(10, 100),\n  nn_relu(),\n  nn_linear(100, 10)\n)\nnet_jit = jit_trace(net, torch_randn(10, 10))\n\ntorch_equal(net(x), net_jit(x))\n\n[1] TRUE\n\n\nTrace-compilation is restrictive because it only records operations applied to torch tensors and is unaware of R control flow, so you need to be careful when using it. Furthermore, it only accepts torch tensors as arguments. Unless you have dynamic inputs and outputs or modify the configuration of the module, trace-compilation should usually work. You can also check this by running the original and trace-jitted module on some example inputs and see if they return the same result.\n\n\n\n\n\n\nNote\n\n\n\nA trace-jitted module does respect the mode of the network, i.e., whether it is training or evaluating.\n\n\nIn mlr3torch, trace compilation is also available and can be enabled by setting jit_trace = TRUE in the learner.\n\nlearner = lrn(\"classif.mlp\", jit_trace = TRUE)\n\nYou can also combine TorchScript with tracing:\n\nnet_both = nn_module(\n  initialize = function() {\n    self$linear = nn_linear(1, 1)\n  },\n  forward = function(x) {\n    self$linear(sum_jit(x))\n  }\n)()\n\nnet_both(list(torch_randn(1), torch_randn(1)))\n\ntorch_tensor\n 1.0027\n[ CPUFloatType{1} ][ grad_fn = &lt;ViewBackward0&gt; ]\n\nnet_both(list(torch_randn(1)))\n\ntorch_tensor\n0.01 *\n 8.5286\n[ CPUFloatType{1} ][ grad_fn = &lt;ViewBackward0&gt; ]\n\n\n\n\n\n\n\n\nQuiz: Just In Time\n\n\n\nQuestion 1: Consider the trace-jitted function below. Can you predict the output of the last two lines? Can you explain why this happens?\n\nf = function(a, b, multiply) {\n  if (multiply$item()) {\n    a * b\n  } else {\n    a + b\n  }\n}\nfjit = jit_trace(f, torch_tensor(1), torch_tensor(2), torch_tensor(TRUE))\n\nfjit(torch_tensor(2), torch_tensor(3), torch_tensor(TRUE))\n\ntorch_tensor\n 6\n[ CPUFloatType{1} ]\n\nfjit(torch_tensor(2), torch_tensor(3), torch_tensor(FALSE))\n\ntorch_tensor\n 6\n[ CPUFloatType{1} ]\n\n\nQuestion 2: Answer the same question for the following function:\n\nf = function(a, b, multiply) {\n  torch_where(multiply, a * b, a + b)\n}\nfjit = jit_trace(f, torch_tensor(1), torch_tensor(2), torch_tensor(TRUE))\n\nfjit(torch_tensor(2), torch_tensor(3), torch_tensor(TRUE))\n\ntorch_tensor\n 6\n[ CPUFloatType{1} ]\n\nfjit(torch_tensor(2), torch_tensor(3), torch_tensor(FALSE))\n\ntorch_tensor\n 5\n[ CPUFloatType{1} ]\n\n\n\n\n\n\n\nMixed Precision Training\nAnother way to speed up the training process is to use mixed precision training. This technique involves training the model using both 16-bit and 32-bit floating point numbers. This allows reducing the memory footprint of the model and speeding up the training process.\nWe won’t cover this here, but refer to the torch documentation that explains how to do this."
  },
  {
    "objectID": "notebooks/6-training-efficiency.html#methodological-approaches",
    "href": "notebooks/6-training-efficiency.html#methodological-approaches",
    "title": "Training Efficiency",
    "section": "Methodological Approaches",
    "text": "Methodological Approaches\n\nValidation and Early Stopping\nFor more details on this topic, see the corresponding chapter in the mlr3 book.\nAs we have already seen in one of the previous notebooks, in deep learning, some part of the data is often used for validation purposes. This allows monitoring the performance of the model on unseen data.\nIn mlr3torch, we can track the performance of the model on a validation set by specifying:\n\nvalidate, which is the ratio of the data that is used for validation\nmeasures_valid, which is a list of measures to use for validation\neval_freq, which is the frequency at which the validation is performed\ncallbacks, which is a list of callbacks to use during training, in this case, we use the history callback, which records the performance of the model on the validation set at regular intervals, enabling us to monitor and analyze the model’s performance over time.\n\n\n\n\n\n\n\nTip\n\n\n\nWhile mlr3torch comes with predefined callbacks, it is also possible to define custom callbacks that modify the training process.\n\n\n\ntask = tsk(\"sonar\")\n\nmlp_learner = lrn(\"classif.mlp\",\n  neurons = c(50, 50), batch_size = 256, epochs = 400,\n  optimizer = t_opt(\"adam\", lr = 0.003),\n  predict_type = \"prob\", jit_trace = TRUE,\n  # Validation / Performance Monitoring\n  validate = 0.3, # how much data to use for validation\n  measures_valid = msr(\"classif.logloss\"), # how to evaluate train performance\n  measures_train = msr(\"classif.logloss\"), # how to evaluate validation performance\n  callbacks = t_clbk(\"history\"), # history callbacks save train and validation performance\n  eval_freq = 10 # after how many training epochs to perform validation\n)\nmlp_learner$train(task)\nhistory = mlp_learner$model$callbacks$history\nstr(history)\n\nClasses 'data.table' and 'data.frame':  40 obs. of  3 variables:\n $ epoch                : num  10 20 30 40 50 60 70 80 90 100 ...\n $ train.classif.logloss: num  0.678 0.643 0.569 0.515 0.478 ...\n $ valid.classif.logloss: num  0.667 0.618 0.536 0.469 0.436 ...\n - attr(*, \".internal.selfref\")=&lt;externalptr&gt; \n - attr(*, \"sorted\")= chr \"epoch\"\n\nhead(history)\n\nKey: &lt;epoch&gt;\n   epoch train.classif.logloss valid.classif.logloss\n   &lt;num&gt;                 &lt;num&gt;                 &lt;num&gt;\n1:    10             0.6775741             0.6665855\n2:    20             0.6430574             0.6176948\n3:    30             0.5685190             0.5364953\n4:    40             0.5151559             0.4694589\n5:    50             0.4780497             0.4363074\n6:    60             0.3861667             0.4153698\n\n\nBelow we plot the training and validation for the different epochs:\n\n\n\n\n\n\n\n\n\nInstead of only monitoring the validation loss (and watching it get worse and worse), we can also stop the training process dynamically when the validation loss begins to increase. This regularization technique is called early stopping, and it prevents overfitting during the training of iteratively trained machine learning models. It involves monitoring the validation loss during training and stopping the training process when the validation loss begins to increase, indicating that the model is starting to overfit the training data.\nThe key configuration option for early stopping is the patience parameter, which defines the number of epochs to wait after the last improvement in validation loss before stopping the training. For example, if patience is set to 10, the training will continue for 10 additional epochs after the last observed improvement in validation loss. If no improvement is seen during this period, training will be halted.\nAdvantages of early stopping include:\n\nPrevention of Overfitting: By stopping training when the model starts to overfit, we can achieve better generalization on unseen data.\nResource Efficiency: It saves computational resources by avoiding unnecessary training epochs once the model performance has plateaued.\n\nNow, let’s train the learner again using early stopping with a patience of 10 epochs:\n\nmlp_learner$param_set$set_values(\n  patience = 5\n)\nmlp_learner$train(task)\nmlp_learner$internal_tuned_values$epochs\n\n[1] 160\n\n\nBeyond only tuning the number of epochs, mlr3’s internal tuning mechanism also allows tuning the number of epochs internally while using an offline tuning method to optimize other hyperparameters. To use this, we can set the parameters we want to tune TuneTokens:\n\nlibrary(mlr3tuning)\nmlp_learner$param_set$set_values(\n  epochs = to_tune(upper = 100, internal = TRUE),\n  opt.lr = to_tune(lower = 1e-4, upper = 1e-1, logscale = TRUE)\n)\n\nWe could now pass this learner to a tuner, where the tuner would only optimize the learning rate, while the learner optimizes the epochs internally."
  },
  {
    "objectID": "notebooks/6-training-efficiency.html#architecture-design",
    "href": "notebooks/6-training-efficiency.html#architecture-design",
    "title": "Training Efficiency",
    "section": "Architecture Design",
    "text": "Architecture Design\nAnother essential aspect of training neural networks efficiently and effectively is the design of the network architecture, which can be a challenging task. However, for many tasks, there are well-known architectures that perform well and can be used as a starting point. Unless there is a specific reason to design a new architecture, it is recommended to use such an architecture.\n\n\n\n\n\n\nNote\n\n\n\nBecause the Python deep learning ecosystem is so large, many more architectures are implemented in Python than in R. One way to use them in R is to simply translate the PyTorch code to (R-)torch. While PyTorch and (R-)torch are quite similar, there are some differences, e.g., 1-based and 0-based indexing. The torch website contains a brief tutorial on how to do this.\n\n\nNonetheless, we will cover important techniques that can be used to speed up the training process, namely batch normalization and dropout.\n\nBatch Normalization\nBatch Normalization is an important technique in deep learning that contributed significantly to speeding up the training process.\nThe formula for batch normalization (during training) is given by:\n\\[\n\\hat{x} = \\frac{x - \\mu_B}{\\sqrt{\\sigma_B^2 + \\epsilon}}\n\\]\nwhere:\n\n\\(\\hat{x}\\) is the normalized output,\n\\(x\\) is the input,\n\\(\\mu_B\\) is the mean of the batch,\n\\(\\sigma_B^2\\) is the variance of the batch,\n\\(\\epsilon\\) is a small constant added for numerical stability.\n\nDuring inference, the module uses the running mean and variance of the training data to normalize the input.\nIn torch, different versions of batch normalization exist for different dimensions of the input tensor. Below, we illustrate the batch normalization module using a 1D input tensor (the batch dimension does not count here)\n\nx = torch_randn(10, 5)\nbn = nn_batch_norm1d(num_features = 5)\nbn(x)\n\ntorch_tensor\n 1.4613 -1.3934 -0.2146  1.0406  0.1413\n-0.9634 -0.3388  1.7441  0.7744  2.1476\n-2.0328  0.5667 -2.0592  0.4071 -0.0529\n 0.6778  0.3264  0.2637 -0.2301 -0.0409\n-0.9243  0.1298 -0.6447 -1.5477 -2.1935\n 0.8150 -0.1962  0.7988 -1.5426  0.1137\n-0.2350 -2.0121 -0.1847  1.1725  0.0143\n 0.8381  0.6141  0.9971  1.0148 -0.5667\n 0.2166  0.7147 -0.7208 -0.1408 -0.0285\n 0.1467  1.5887  0.0203 -0.9482  0.4657\n[ CPUFloatType{10,5} ][ grad_fn = &lt;NativeBatchNormBackward0&gt; ]\n\n\n\n\n\n\n\n\nQuiz: Batch Normalization\n\n\n\nQuestion 1: Earlier we have learned that nn_modules have buffers and parameters, where the latter are learned with gradient descent. Do you think the mean and variance are parameters or buffers?\n\n\nClick for answer\n\nThey are both buffers as they only store the variance and running mean of all training samples seen, i.e., they are not updated using gradient information.\n\nQuestion 2: Training vs. Evaluation Mode: While many nn_modules behave the same way irrespective of their mode, batch normalization is an example of a module that behaves differently during training and evaluation, i.e., during training, the module uses the mean and variance of the current batch, while during evaluation, it uses the running mean and variance of all training samples seen.\n\nbn(x[1:10, ])\n\ntorch_tensor\n 1.4613 -1.3934 -0.2146  1.0406  0.1413\n-0.9634 -0.3388  1.7441  0.7744  2.1476\n-2.0328  0.5667 -2.0592  0.4071 -0.0529\n 0.6778  0.3264  0.2637 -0.2301 -0.0409\n-0.9243  0.1298 -0.6447 -1.5477 -2.1935\n 0.8150 -0.1962  0.7988 -1.5426  0.1137\n-0.2350 -2.0121 -0.1847  1.1725  0.0143\n 0.8381  0.6141  0.9971  1.0148 -0.5667\n 0.2166  0.7147 -0.7208 -0.1408 -0.0285\n 0.1467  1.5887  0.0203 -0.9482  0.4657\n[ CPUFloatType{10,5} ][ grad_fn = &lt;NativeBatchNormBackward0&gt; ]\n\n\nWhich of the following statements is true and why?\n\nbn$eval()\nequal1 = torch_equal(\n  torch_cat(list(bn(x[1:2, ]), bn(x[3:4, ]))),\n  bn(x[1:4, ])\n)\nbn$train()\nequal2 = torch_equal(\n  torch_cat(list(bn(x[1:2, ]), bn(x[3:4, ]))),\n  bn(x[1:4, ])\n)\n\n\n\nClick for answer\n\n\nc(equal1, equal2)\n\n[1]  TRUE FALSE\n\n\nThe first statement is true because, in evaluation mode, the module uses the running mean and variance of all training samples seen. The second statement is false because the first tensor uses different means and variances for rows 1-2 and 3-4, while the second tensor uses the same mean and variance for all rows.\n\n\n\nTo illustrate its effectiveness, we will define a simple CNN, with and without batch normalization, train it on CIFAR-10, and compare their performance.\nTo build the neural networks, we will use mlr3torch, which allows building architectures from PipeOps. This makes the creation of network architectures easier, as we, e.g., don’t have to specify auxiliary parameters (such as the input dimension of a linear layer). Recall that the po(\"torch_ingress_ltnsr\") is a special PipeOp that marks the input of the neural network. Note that po(\"nn_relu_1\") is equivalent to po(\"nn_relu\", id = \"nn_relu_1\"). We need to specify unique ID parameters as this is required in mlr3pipelines.\n\ncnn_bn = po(\"torch_ingress_ltnsr\") %&gt;&gt;%\n  po(\"nn_conv2d_1\", out_channels = 32, kernel_size = 3, stride = 1, padding = 1) %&gt;&gt;%\n  po(\"nn_batch_norm2d_1\") %&gt;&gt;%\n  po(\"nn_relu_1\") %&gt;&gt;%\n  po(\"nn_max_pool2d_1\", kernel_size = 2, stride = 2) %&gt;&gt;%\n  po(\"nn_conv2d_2\", out_channels = 64, kernel_size = 3, stride = 1, padding = 1) %&gt;&gt;%\n  po(\"nn_batch_norm2d_2\") %&gt;&gt;%\n  po(\"nn_relu_2\") %&gt;&gt;%\n  po(\"nn_max_pool2d_2\", kernel_size = 2, stride = 2)\n\ncnn = po(\"torch_ingress_ltnsr\") %&gt;&gt;%\n  po(\"nn_conv2d_1\", out_channels = 32, kernel_size = 3, stride = 1, padding = 1) %&gt;&gt;%\n  po(\"nn_relu_1\") %&gt;&gt;%\n  po(\"nn_max_pool2d_1\", kernel_size = 2, stride = 2) %&gt;&gt;%\n  po(\"nn_conv2d\", out_channels = 64, kernel_size = 3, stride = 1, padding = 1) %&gt;&gt;%\n  po(\"nn_relu_2\") %&gt;&gt;%\n  po(\"nn_max_pool2d_2\", kernel_size = 2, stride = 2)\n\nhead = po(\"nn_flatten\") %&gt;&gt;%\n  po(\"nn_linear\", out_features = 128) %&gt;&gt;%\n  po(\"nn_relu\") %&gt;&gt;%\n  po(\"nn_head\")\n\nmodel = po(\"torch_optimizer\", optimizer = t_opt(\"adam\", lr = 0.003)) %&gt;&gt;%\n  po(\"torch_model_classif\",\n    epochs = 100,\n    batch_size = 256,\n    predict_type = \"prob\",\n    device = \"cuda\"\n  )\n\nWe evaluate the two models on the CIFAR-10 image classification task that we have introduced earlier. There, the goal is to classify images into 10 different classes.\n\nnet_bn = as_learner(cnn_bn %&gt;&gt;% head %&gt;&gt;% model)\nnet_bn$id = \"net_bn\"\nnet = as_learner(cnn %&gt;&gt;% head %&gt;&gt;% model)\nnet$id = \"net\"\n\ncifar10 = tsk(\"cifar10\")\nresampling = rsmp(\"holdout\")$instantiate(cifar10)\n\ndesign = benchmark_grid(\n  task = cifar10,\n  learner = list(net_bn, net),\n  resampling = resampling\n)\ndesign\n\n      task learner resampling\n    &lt;char&gt;  &lt;char&gt;     &lt;char&gt;\n1: cifar10  net_bn    holdout\n2: cifar10     net    holdout\n\n\n\nbmr = benchmark(design)\nbmr$aggregate()"
  },
  {
    "objectID": "notebooks/6-training-efficiency.html#dropout",
    "href": "notebooks/6-training-efficiency.html#dropout",
    "title": "Training Efficiency",
    "section": "Dropout",
    "text": "Dropout\nDropout is a regularization technique used to prevent overfitting in neural networks by randomly setting a fraction of input units to zero during training. This encourages the network to learn more robust features that are not reliant on specific neurons, thereby improving its generalization capabilities. During each training iteration, dropout randomly “drops” a subset of neurons by setting their activations to zero with a specified probability (commonly between 20% to 50%). This forces the network to distribute the learned representations more evenly across neurons, reducing the reliance on any single neuron and mitigating overfitting. Dropout is more commonly used in the context of fully connected layers.\n\n\n\n\n\nSource: https://medium.com/konvergen/understanding-dropout-ddb60c9f98aa\nJust like batch normalization, it also has different behavior during training and evaluation.\n\ndropout = nn_dropout(p = 0.5)\ndropout(x)\n\ntorch_tensor\n 0.0000 -3.9488  0.0093  0.0000  0.7024\n-0.0000 -1.4141  0.0000  2.9566  5.7694\n-4.4366  0.7622 -0.0000  2.1163  0.2118\n 0.0000  0.0000  0.0000  0.6584  0.2422\n-0.0000 -0.0000 -0.9663 -0.0000 -5.1942\n 0.0000 -1.0714  2.3080 -0.0000  0.6326\n-1.1987 -5.4360  0.0000  3.8675  0.0000\n 0.0000  0.8761  2.7579  3.5069 -0.0000\n-0.3855  1.1178 -0.0000  0.8627  0.0000\n-0.0000  0.0000  0.0000 -0.0000  1.5217\n[ CPUFloatType{10,5} ]\n\ndropout$eval()\ndropout(x)\n\ntorch_tensor\n 0.9281 -1.9744  0.0046  1.7829  0.3512\n-1.2553 -0.7071  2.2261  1.4783  2.8847\n-2.2183  0.3811 -2.0875  1.0582  0.1059\n 0.2226  0.0924  0.5471  0.3292  0.1211\n-1.2201 -0.1440 -0.4831 -1.1782 -2.5971\n 0.3462 -0.5357  1.1540 -1.1725  0.3163\n-0.5994 -2.7180  0.0385  1.9338  0.1908\n 0.3669  0.4380  1.3789  1.7534 -0.5429\n-0.1927  0.5589 -0.5695  0.4313  0.1367\n-0.2556  1.6093  0.2711 -0.4924  0.7609\n[ CPUFloatType{10,5} ]\n\n\nTo look at the effects, we will create a second classification head with dropout and then define new learners\n\nhead_dropout = po(\"nn_flatten\") %&gt;&gt;%\n  po(\"nn_linear\", out_features = 128) %&gt;&gt;%\n  po(\"nn_relu\") %&gt;&gt;%\n  po(\"nn_dropout\", p = 0.5) %&gt;&gt;%\n  po(\"nn_head\")\n\nnet_bn_dropout = as_learner(cnn_bn %&gt;&gt;% head_dropout %&gt;&gt;% model)\nnet_bn_dropout$id = \"net_bn_dropout\"\nnet_dropout = as_learner(cnn %&gt;&gt;% head_dropout %&gt;&gt;% model)\nnet_dropout$id = \"net_dropout\"\n\ndesign2 = benchmark_grid(\n  task = cifar10,\n  learner = list(net_bn_dropout, net_dropout),\n  resampling = resampling\n)\n\nNext, we run the second benchmark experiment and afterwards combine the results with the first benchmark experiment.\n\nbmr2 = benchmark(design2)\nbmr = c(bmr, bmr2)\nautoplot(bmr)\n\n\n\n\n\n\n\nQuiz: Dropout\n\n\n\nQuestion 1: Worse Training Loss: You are training a neural network with and without dropout. The training loss is higher with dropout, is this a bug?\n\n\nClick for answer\n\nNot necessarily, as dropout is a regularization technique that prevents overfitting. It’s goal is to reduce the generalization performance of the model."
  },
  {
    "objectID": "notebooks/6-training-efficiency.html#transfer-learning",
    "href": "notebooks/6-training-efficiency.html#transfer-learning",
    "title": "Training Efficiency",
    "section": "Transfer Learning",
    "text": "Transfer Learning\nTransfer learning is a powerful technique in machine learning where a pre-trained model developed for a specific task is reused as the starting point for a model on a second, related task. Instead of training a model from scratch, which can be time-consuming and computationally expensive, transfer learning leverages the knowledge gained from a previously learned task to improve learning efficiency and performance on a new task.\nThe advantages of transfer learning are:\n\nReduced Training Time: Leveraging a pre-trained model can significantly decrease the time required to train a new model, as the foundational feature extraction layers are already optimized.\nImproved Performance: Transfer learning can enhance model performance, especially when the new task has limited training data. The pre-trained model’s knowledge helps in achieving better generalization.\nResource Efficiency: Utilizing pre-trained models reduces the computational resources needed, making it feasible to develop sophisticated models without extensive hardware.\n\nWhen the model is then trained on a new task, only the last layer is replaced with a new output layer to adjust for the new task.\nThis is visualized below:\n\nSource: https://en.wikipedia.org/wiki/Transfer_learning\nmlr3torch connects various pretrained image networks that are available in the torchvision package. The ResNet-18 model is a popular pre-trained model that was pretrained on ImageNet. We can use the pretrained weights by setting the pretrained parameter to TRUE.\n\nresnet = lrn(\"classif.resnet18\",\n  pretrained = TRUE,\n  epochs = 2,\n  batch_size = 256,\n  validate = 0.3,\n  measures_valid = msr(\"classif.logloss\"),\n  device = \"cuda\",\n  predict_type = \"prob\",\n  id = \"pretrained\"\n)\nresnet_no_pretrain = resnet$clone(deep = TRUE)\nresnet_no_pretrain$param_set$set_values(\n  pretrained = FALSE\n)\nresnet_no_pretrain$id = \"not_pretrained\"\n\ngrid = benchmark_grid(\n  task = tsk(\"cifar10\"),\n  learner = list(resnet, resnet_no_pretrain),\n  resampling = rsmp(\"insample\")\n)\n\nbmr = benchmark(grid, store_models = TRUE)\nbmr$aggregate()\n\nWhen fine-tuning a pretrained model like ResNet-18, it’s common to observe instabilities in gradients, which can manifest as fluctuating validation performance. This can e.g. be because the learning rate is too high (compared to the learning rate that was used during pretraining).\nTo address this, one can:\n\nUse a smaller learning rate for the pretrained layers than for the new output head.\nFreeze the pretrained layers (for some epochs) and only train the new output head.\n\nIn mlr3torch this can be achieved via the callback mechanism. For the unfreezing, there even exists a predefined callback t_clbk(\"unfreeze\"). To create a custom callback, the torch_callback() function can be used. A tutorial on this can be found on the mlr3torch package website.\n\n\n\n\n\n\nIn-Context Learning\n\n\n\nLarge foundation models (such as GPT-4) even allow to perform tasks on which they were not pretrained on without any finetuning. This is referred to as in-context learning or zero-shot learning. There, the task is fed into the model during inference: “Hey ChatGPT, is What is the sentiment of this sentence. Return -1 for sad, 0 for neutral, 1 for happy: ”"
  },
  {
    "objectID": "notebooks/6-training-efficiency.html#data-augmentation",
    "href": "notebooks/6-training-efficiency.html#data-augmentation",
    "title": "Training Efficiency",
    "section": "Data Augmentation",
    "text": "Data Augmentation\nData augmentation is a technique used to increase the diversity and quantity of training data without actually collecting new data. By applying various transformations to the existing dataset, data augmentation helps improve the generalization capabilities of machine learning models, reduce overfitting, and enhance model robustness. This is especially crucial when you have limited data.\nData augmentation for images can consist of rotation, flipping, translating, grey scaling, etc. Which data augmentation is admissible, depends on the task:\n\nIf the modeling task is to predict whether there is a mark in the top right corner of an image, vertical or horizontal flipping is not admissible.\nIf the goal is to predict whether there is a mark somewhere in the image, it would be admissible.\n\nIn other words, the data augmentation must be compatible with the invariances of the task.\nIn mlr3torch, data augmentation is available via PipeOps of the form po(\"augment_\"). Currently, only augemntation operators from the torchvision package are available, but you can also add your own.\n\naugment = po(\"augment_random_resized_crop\") %&gt;&gt;%\n  po(\"augment_random_horizontal_flip\") %&gt;&gt;%\n  po(\"augment_random_vertical_flip\")\n\nWe can just create a new GraphLearner that includes the augemntation steps as well as the learner from above:\n\nresnet_augmented = as_learner(augment %&gt;&gt;% resnet)\nresnet_augmented$id = \"resnet_augmented\"\nresnet_augmented$train(task = cifar10)\n\n\n\n\n\n\n\nQuiz: Data Augmentation\n\n\n\nQuestion 1: Do you think data augmentation should be applied to the validation set?\n\n\nClick for answer\n\nNo, as the purpose of data augmentation is not to improve an individual prediction, it will not be applied during test time and hence also not to the validation set. Looking at the performance of augmented validation data is, however, also not a mistake."
  },
  {
    "objectID": "notebooks/7-usecase.html",
    "href": "notebooks/7-usecase.html",
    "title": "Practical Use Case",
    "section": "",
    "text": "Brief recap of what is important to get good and fast performance in mlr3torch:\n\nUse jit-compilation to speed up the training"
  }
]